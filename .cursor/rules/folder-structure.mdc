## Code Location
- **ALL code must be stored in**: `romance-novel-nlp-research/src/`
- **NEVER create new folders** like `scripts/`, `tools/`, etc.
- **Use existing structure**: `src/data_quality/`, `src/csv_building/`, etc.

## Data Storage Rules

### Raw Data (Read-only)
- **Location**: `romance-novel-nlp-research/data/raw/`
- **Purpose**: Original Goodreads JSON files
- **Rules**: Never modify, read-only access
- **Files**: `goodreads_*.json.gz`

### Intermediate Data (Scratch)
- **Location**: `romance-novel-nlp-research/data/intermediate/`
- **Purpose**: Temporary processing outputs
- **Rules**: Can be deleted, regenerated during processing

### Processed Data (Final)
- **Location**: `romance-novel-nlp-research/data/processed/`
- **Purpose**: Final cleaned datasets for analysis
- **Rules**: Keep, versioned outputs
- **Files**: `cleaned_romance_novels_step*.pkl/csv` (pipeline outputs)

### Logs
- **Location**: `romance-novel-nlp-research/organized_outputs/logs/`
- **Purpose**: All pipeline execution logs, validation reports, error logs
- **Files**: `*_20250907_*.log` (timestamped execution logs)

### Outputs
- **Location**: `romance-novel-nlp-research/organized_outputs/`
- **Purpose**: All organized outputs, analysis results, reports, visualizations
- **Structure**: 
  - `datasets/` - All dataset versions (step-by-step + specialized)
  - `logs/` - All pipeline execution logs
  - `reports/` - All analysis reports (JSON + Markdown)
  - `visualizations/` - All plots and charts

### Configuration
- **Location**: `romance-novel-nlp-research/config/` (if needed)
- **Purpose**: YAML configuration files
- **Note**: Currently not used in active project

### Documentation
- **Location**: `romance-novel-nlp-research/docs/` (if needed)
- **Purpose**: Project documentation, protocols, guides
- **Note**: Currently not used in active project

### Tests
- **Location**: `romance-novel-nlp-research/tests/` (if needed)
- **Purpose**: Unit tests, integration tests
- **Note**: Currently not used in active project

## Code Reuse Rules

### Pipeline Execution
- **NEVER write new pipeline code from scratch**
- **ALWAYS modify existing code** in `src/data_quality/` or `src/csv_building/`
- **Use parameters** to control behavior (e.g., `sample_size`)
- **Same script** should work for both full dataset and subsets

### Example: Pipeline Runner
```python
# ✅ CORRECT: Modify existing pipeline
pipeline = DataTypeOptimizer()
pipeline.run_complete_optimization()  # For full dataset

# ❌ WRONG: Create new pipeline code
# Don't create new files like scripts/run_pipeline_5000_sample.py
```

## File Naming Conventions

### Data Files
- **Pipeline Outputs**: `romance_novels_step[step]_[timestamp].[format]`
- **Optimized Data**: `cleaned_romance_novels_step5_optimized_[timestamp].[format]`
- **Reports**: `*_report_step[step]_[timestamp].json`
- **Quality Reports**: `*_quality_report_[timestamp].txt`

### Pipeline Step Files
- **Step Implementations**: `step[number]_[description].py`
- **Main Runner**: `pipeline_runner.py`
- **Individual Runners**: `run_[description].py`

### Archive Files
- **Cleanup Archives**: `cleanup_YYYYMMDD/`
- **Archive Summaries**: `CLEANUP_SUMMARY.md`
- **Archive Documentation**: `ARCHIVE_SUMMARY.md`

### Output Organization
- **Step Outputs**: `src/data_quality/outputs/[step_name]/`
- **Timestamped Files**: `*_[YYYYMMDD_HHMMSS].[format]`
- **Quality Reports**: `*_quality_report_[timestamp].md`

## Academic Compliance

### Data Handling
- **Raw inputs**: Read-only in `data/raw/`
- **Interim outputs**: `data/intermediate/` (can be deleted)
- **Final outputs**: `data/processed/` (keep for research)
- **Logs**: `logs/` (for reproducibility)
- **Reports**: `outputs/` (for documentation)

## Current Project Structure

```
romance-novel-nlp-research/
├── .cursor/                          # Cursor IDE configuration
│   └── rules/                        # Project-specific rules
├── archive/                          # Archived development history
│   ├── cleanup_20250105/             # January 5, 2025 cleanup
│   ├── cleanup_20250106/             # January 6, 2025 cleanup
│   ├── cleanup_20250907/             # September 7, 2025 comprehensive cleanup
│   │   ├── CLEANUP_SUMMARY.md        # Comprehensive cleanup documentation
│   │   ├── duplicate_outputs/        # Files now in organized_outputs/
│   │   ├── old_datasets/             # Outdated dataset versions
│   │   ├── old_logs/                 # Old log files
│   │   ├── old_reports/              # Outdated reports
│   │   └── scratch_files/            # (Empty - no scratch files found)
│   ├── data_quality_archive_20250902/  # September 2025 data quality archive
│   ├── data_quality_artifacts/       # Legacy data quality files
│   ├── pipeline_outputs_20250902/    # Archived pipeline outputs
│   ├── processed_data_20250904/      # Archived processed data
│   ├── unused_code/                  # Previous implementations
│   ├── ARCHIVE_SUMMARY.md            # Main archive documentation
│   └── DATA_QUALITY_ARCHIVE_SUMMARY.md # Data quality archive summary
├── data/                             # Data storage
│   ├── raw/                          # Original Goodreads JSON files (read-only)
│   │   ├── goodreads_book_authors.json.gz
│   │   ├── goodreads_book_genres_initial.json.gz
│   │   ├── goodreads_book_series.json.gz
│   │   ├── goodreads_book_works.json.gz
│   │   ├── goodreads_books_romance.json.gz
│   │   ├── goodreads_interactions_romance.json.gz
│   │   ├── goodreads_reviews_dedup.json.gz
│   │   ├── goodreads_reviews_romance.json.gz
│   │   ├── goodreads_reviews_spoiler.json.gz
│   │   └── README.md
│   ├── intermediate/                 # Temporary processing outputs
│   └── processed/                    # Current datasets (3 files)
│       ├── final_books_2000_2020_en_enhanced_20250907_013708.csv
│       ├── romance_novels_text_preprocessed_20250907_015606.csv
│       └── text_preprocessing_report_20250907_015613.json
├── organized_outputs/                # All organized outputs
│   ├── datasets/                     # All dataset versions
│   │   ├── step_by_step/             # 7 datasets showing pipeline progression
│   │   └── specialized_versions/     # 3 final dataset versions
│   ├── logs/                         # All pipeline execution logs
│   ├── reports/                      # All analysis reports
│   │   ├── json/                     # 15 JSON reports with statistics
│   │   └── markdown/                 # 5 Markdown analysis reports
│   ├── visualizations/               # 17 publication-ready plots
│   └── README.md                     # Complete output documentation
├── src/                              # Source code (ALL code must be here)
│   ├── data_quality/                 # Complete 6-step data quality pipeline
│   │   ├── __init__.py               # Module exports
│   │   ├── pipeline_runner.py        # Main pipeline runner (executes all 6 steps)
│   │   ├── step1_missing_values_cleaning.py      # Step 1: Missing Values Cleaning
│   │   ├── step2_duplicate_detection.py          # Step 2: Duplicate Detection
│   │   ├── step3_data_type_validation.py         # Step 3: Data Type Validation
│   │   ├── step4_outlier_detection.py            # Step 4: Outlier Detection
│   │   ├── step4_outlier_treatment.py            # Step 4: Outlier Treatment
│   │   ├── step5_data_type_optimization.py       # Step 5: Data Type Optimization
│   │   └── step6_final_quality_validation.py     # Step 6: Final Quality Validation
│   ├── csv_building/                 # CSV building and data integration
│   │   ├── __init__.py
│   │   ├── final_csv_builder.py      # Main CSV builder implementation
│   │   └── run_builder.py            # Builder runner script
│   ├── nlp_preprocessing/            # NLP text preprocessing
│   │   ├── __init__.py
│   │   ├── text_preprocessor.py      # Main text preprocessor
│   │   ├── run_preprocessor.py       # Preprocessor runner
│   │   └── test_preprocessor.py      # Preprocessor tests
│   ├── eda_analysis/                 # Exploratory data analysis
│   │   ├── __init__.py
│   │   ├── improved_eda_final.py     # Main EDA analysis
│   │   ├── comparative_distribution_visualization.py
│   │   └── outputs/                  # EDA outputs
│   │       └── complete_pipeline_analysis/       # Final analysis results
│   └── __init__.py
├── .cursorrules                      # Cursor rules configuration
├── .cursorignore                     # Cursor ignore patterns
├── .gitignore                        # Git ignore patterns
├── LICENSE                           # MIT License
├── README.md                         # Project documentation
└── requirements.txt                  # Python dependencies
```

## Pipeline Architecture

### Complete 6-Step Pipeline Implementation
The project implements a **complete 6-step data cleaning pipeline** with all steps currently active:

1. **Step 1**: Missing Values Cleaning
   - **Implementation**: `step1_missing_values_cleaning.py`
   - **Outputs**: `src/data_quality/outputs/missing_values_cleaning/`
   - **Purpose**: Handle and document missing value patterns

2. **Step 2**: Duplicate Detection
   - **Implementation**: `step2_duplicate_detection.py`
   - **Outputs**: `src/data_quality/outputs/duplicate_detection/`
   - **Purpose**: Identify and resolve duplicate records

3. **Step 3**: Data Type Validation
   - **Implementation**: `step3_data_type_validation.py`
   - **Outputs**: `src/data_quality/outputs/data_type_validation/`
   - **Purpose**: Validate and standardize data types

4. **Step 4**: Outlier Detection & Treatment
   - **Detection**: `step4_outlier_detection.py`
   - **Treatment**: `step4_outlier_treatment.py`
   - **Outputs**: `src/data_quality/outputs/outlier_detection/`
   - **Purpose**: Identify and treat statistical outliers

5. **Step 5**: Data Type Optimization & Persistence
   - **Implementation**: `step5_data_type_optimization.py`
   - **Outputs**: `src/data_quality/outputs/data_type_optimization/`
   - **Purpose**: Optimize memory usage and create persistent formats

6. **Step 6**: Final Quality Validation & Certification
   - **Implementation**: `step6_final_quality_validation.py`
   - **Outputs**: `src/data_quality/outputs/final_quality_validation/`
   - **Purpose**: Final quality assessment and certification

### Pipeline Execution
- **Main Runner**: `src/data_quality/pipeline_runner.py` (Executes all 6 steps)
- **Individual Steps**: Each step can be run independently
- **Output Organization**: All outputs organized by step in `src/data_quality/outputs/`
- **Quality Reports**: Comprehensive reporting for each step

### Module Structure
- **`src/data_quality/`**: Complete 6-step pipeline implementation
- **`src/csv_building/`**: Data integration and CSV building
- **Archived**: Legacy implementations and development artifacts (January 2025 cleanup)

## Enforcement

### When Creating New Files
1. **Check existing structure** first
2. **Use appropriate subfolder** in `src/`
3. **Follow naming conventions**
4. **Respect academic constraints**
5. **Reuse existing code** when possible

### When Modifying Code
1. **Modify existing files** in `src/`
2. **Add parameters** for flexibility
3. **Maintain backward compatibility**
4. **Update documentation** if needed

### Archive Management
1. **Archive outdated files** rather than deleting
2. **Document what was archived** and why
3. **Maintain recovery instructions**
4. **Keep archive structure organized**

### Recent Cleanup Activities (September 2025)
The repository has undergone comprehensive cleanup and organization to maintain a clean, focused codebase:

**September 7, 2025 Comprehensive Cleanup**:
- **Repository Organization**: All outputs centralized in `organized_outputs/` directory
- **Duplicate Files**: Archived 50+ duplicate files now superseded by organized structure
- **Old Dataset Versions**: Archived 5 outdated dataset versions, keeping only current ones
- **Old Logs**: Archived 3 old log files superseded by organized logs
- **Old Reports**: Archived 10+ outdated reports superseded by organized reports
- **Data Quality Outputs**: Archived old pipeline outputs from `src/data_quality/outputs/`

**Previous Cleanup Activities (January 2025)**:
- **Python Cache Files**: Removed `__pycache__` directories from `src/data_quality/` and `src/csv_building/`
- **Empty Log Files**: Archived 5 empty log files (0 bytes) that were never used
- **Duplicate Outputs**: Archived earlier versions of quality reports and unprocessed batch files
- **Development Tools**: Archived 3 diagnostic scripts used during development
- **Alternative Runners**: Archived 4 redundant runner scripts that duplicated core functionality

**Current Repository State**:
- **Clean Source Code**: No cache files or redundant scripts in active directories
- **Organized Outputs**: All outputs centralized in `organized_outputs/` with clear structure
- **Current Datasets**: Only 3 current dataset files in `data/processed/`
- **Preserved History**: All development artifacts safely archived
- **Ready for Research**: Clean, organized codebase prepared for advanced NLP analysis

This rule must be applied to ALL tasks and code changes.
