## Code Location
- **ALL code must be stored in**: `romance-novel-nlp-research/src/`
- **NEVER create new folders** like `scripts/`, `tools/`, etc.
- **Use existing structure**: `src/data_quality/`, `src/csv_building/`, etc.

## Data Storage Rules

### Raw Data (Read-only)
- **Location**: `romance-novel-nlp-research/data/raw/`
- **Purpose**: Original Goodreads JSON files
- **Rules**: Never modify, read-only access
- **Files**: `goodreads_*.json.gz`

### Intermediate Data (Scratch)
- **Location**: `romance-novel-nlp-research/data/intermediate/`
- **Purpose**: Temporary processing outputs
- **Rules**: Can be deleted, regenerated during processing

### Processed Data (Final)
- **Location**: `romance-novel-nlp-research/data/processed/`
- **Purpose**: Final cleaned datasets for analysis
- **Rules**: Keep, versioned outputs
- **Files**: `cleaned_romance_novels_step*.pkl/csv` (pipeline outputs)

### Logs
- **Location**: `romance-novel-nlp-research/logs/`
- **Purpose**: Execution logs, validation reports, error logs
- **Files**: `*_20250902_*.log` (timestamped execution logs)

### Outputs
- **Location**: `romance-novel-nlp-research/outputs/`
- **Purpose**: Pipeline step outputs, analysis results, reports
- **Structure**: Organized by pipeline step (step5, step6)

### Configuration
- **Location**: `romance-novel-nlp-research/config/` (if needed)
- **Purpose**: YAML configuration files
- **Note**: Currently not used in active project

### Documentation
- **Location**: `romance-novel-nlp-research/docs/` (if needed)
- **Purpose**: Project documentation, protocols, guides
- **Note**: Currently not used in active project

### Tests
- **Location**: `romance-novel-nlp-research/tests/` (if needed)
- **Purpose**: Unit tests, integration tests
- **Note**: Currently not used in active project

## Code Reuse Rules

### Pipeline Execution
- **NEVER write new pipeline code from scratch**
- **ALWAYS modify existing code** in `src/data_quality/` or `src/csv_building/`
- **Use parameters** to control behavior (e.g., `sample_size`)
- **Same script** should work for both full dataset and subsets

### Example: Pipeline Runner
```python
# ✅ CORRECT: Modify existing pipeline
pipeline = DataTypeOptimizer()
pipeline.run_complete_optimization()  # For full dataset

# ❌ WRONG: Create new pipeline code
# Don't create new files like scripts/run_pipeline_5000_sample.py
```

## File Naming Conventions

### Data Files
- **Pipeline Outputs**: `romance_novels_step[step]_[timestamp].[format]`
- **Optimized Data**: `cleaned_romance_novels_step5_optimized_[timestamp].[format]`
- **Reports**: `*_report_step[step]_[timestamp].json`
- **Quality Reports**: `*_quality_report_[timestamp].txt`

### Pipeline Step Files
- **Step Implementations**: `step[number]_[description].py`
- **Main Runner**: `pipeline_runner.py`
- **Individual Runners**: `run_[description].py`

### Archive Files
- **Cleanup Archives**: `cleanup_YYYYMMDD/`
- **Archive Summaries**: `CLEANUP_SUMMARY.md`
- **Archive Documentation**: `ARCHIVE_SUMMARY.md`

### Output Organization
- **Step Outputs**: `src/data_quality/outputs/[step_name]/`
- **Timestamped Files**: `*_[YYYYMMDD_HHMMSS].[format]`
- **Quality Reports**: `*_quality_report_[timestamp].md`

## Academic Compliance

### Data Handling
- **Raw inputs**: Read-only in `data/raw/`
- **Interim outputs**: `data/intermediate/` (can be deleted)
- **Final outputs**: `data/processed/` (keep for research)
- **Logs**: `logs/` (for reproducibility)
- **Reports**: `outputs/` (for documentation)

## Current Project Structure

```
romance-novel-nlp-research/
├── .cursor/                          # Cursor IDE configuration
│   └── rules/                        # Project-specific rules
├── archive/                          # Archived development history
│   ├── cleanup_20250105/             # January 5, 2025 cleanup
│   │   ├── CLEANUP_SUMMARY.md        # Cleanup documentation
│   │   ├── csv_building_archive/     # Archived CSV building files
│   │   ├── duplicate_outputs/        # Duplicate output files
│   │   ├── old_logs/                 # Outdated log files
│   │   └── old_reports/              # Superseded reports
│   ├── cleanup_20250106/             # January 6, 2025 cleanup
│   │   ├── CLEANUP_SUMMARY.md        # Latest cleanup documentation
│   │   ├── data_quality_pycache/     # Python cache files
│   │   ├── csv_building_pycache/     # Python cache files
│   │   ├── empty_logs/               # Empty log files
│   │   ├── src_development_tools/    # Development diagnostic tools
│   │   ├── src_alternative_runners/  # Redundant runner scripts
│   │   └── duplicate_outputs/        # Earlier output versions
│   ├── data_quality_archive_20250902/  # September 2025 data quality archive
│   │   ├── legacy_analysis/           # Replaced analysis implementations
│   │   ├── development_artifacts/     # Development versions and tests
│   │   ├── ARCHIVE_SUMMARY.md         # Archive documentation
│   │   └── CLEANUP_SUMMARY.md         # Cleanup operation summary
│   ├── pipeline_outputs_20250902/     # Archived pipeline outputs
│   │   ├── step1_3_processed/         # Step 1-3 intermediate datasets
│   │   ├── step1_3_outputs/           # Step 1-3 summary reports
│   │   ├── step4_outputs/             # Step 4 outlier detection outputs
│   │   ├── intermediate_data/          # Title cleaning and validation
│   │   ├── ARCHIVE_SUMMARY.md         # Pipeline archive documentation
│   │   └── CLEANUP_SUMMARY.md         # Pipeline cleanup summary
│   ├── processed_data_20250904/       # Archived processed data
│   │   ├── ARCHIVE_SUMMARY.md         # Processed data documentation
│   │   └── *.csv                      # Archived CSV files
│   ├── data_quality_artifacts/        # Legacy data quality files
│   ├── unused_code/                   # Previous implementations
│   ├── ARCHIVE_SUMMARY.md             # Main archive documentation
│   └── DATA_QUALITY_ARCHIVE_SUMMARY.md # Data quality archive summary
├── data/                             # Data storage
│   ├── raw/                          # Original Goodreads JSON files (read-only)
│   │   ├── goodreads_book_authors.json.gz
│   │   ├── goodreads_book_genres_initial.json.gz
│   │   ├── goodreads_book_series.json.gz
│   │   ├── goodreads_book_works.json.gz
│   │   ├── goodreads_books_romance.json.gz
│   │   ├── goodreads_interactions_romance.json.gz
│   │   ├── goodreads_reviews_dedup.json.gz
│   │   ├── goodreads_reviews_romance.json.gz
│   │   ├── goodreads_reviews_spoiler.json.gz
│   │   └── README.md
│   ├── intermediate/                 # Temporary processing outputs
│   └── processed/                    # Final cleaned datasets
│       └── README.md                 # Pipeline documentation
├── outputs/                          # Current pipeline outputs (currently empty)
│   └── (ready for future outputs)    # Pipeline outputs now in src/data_quality/outputs/
├── src/                              # Source code (ALL code must be here)
│   ├── data_quality/                 # Complete 6-step data quality pipeline
│   │   ├── __init__.py               # Module exports
│   │   ├── pipeline_runner.py        # Main pipeline runner (executes all 6 steps)
│   │   ├── step1_missing_values_cleaning.py      # Step 1: Missing Values Cleaning
│   │   ├── step2_duplicate_detection.py          # Step 2: Duplicate Detection
│   │   ├── step3_data_type_validation.py         # Step 3: Data Type Validation
│   │   ├── step4_outlier_detection.py            # Step 4: Outlier Detection
│   │   ├── step4_outlier_treatment.py            # Step 4: Outlier Treatment
│   │   ├── step5_data_type_optimization.py       # Step 5: Data Type Optimization
│   │   ├── step6_final_quality_validation.py     # Step 6: Final Quality Validation
│   │   └── outputs/                              # Pipeline execution outputs
│   │       ├── missing_values_cleaning/          # Step 1 outputs
│   │       ├── duplicate_detection/              # Step 2 outputs
│   │       ├── data_type_validation/             # Step 3 outputs
│   │       ├── outlier_detection/                # Step 4 outputs
│   │       ├── data_type_optimization/           # Step 5 outputs
│   │       └── final_quality_validation/         # Step 6 outputs
│   ├── csv_building/                 # CSV building and data integration
│   │   ├── __init__.py
│   │   ├── final_csv_builder.py      # Main CSV builder implementation
│   │   └── run_builder.py            # Builder runner script
│   └── __init__.py
├── logs/                             # Execution logs (currently empty)
│   └── (ready for future logs)       # Logs archived in cleanup_20250105/old_logs/
├── notebooks/                        # Jupyter notebooks (if needed)
├── docs/                             # Project documentation (if needed)
├── .cursorrules                      # Cursor rules configuration
├── .cursorignore                     # Cursor ignore patterns
├── .gitignore                        # Git ignore patterns
├── LICENSE                           # MIT License
├── README.md                         # Project documentation
└── requirements.txt                  # Python dependencies
```

## Pipeline Architecture

### Complete 6-Step Pipeline Implementation
The project implements a **complete 6-step data cleaning pipeline** with all steps currently active:

1. **Step 1**: Missing Values Cleaning
   - **Implementation**: `step1_missing_values_cleaning.py`
   - **Outputs**: `src/data_quality/outputs/missing_values_cleaning/`
   - **Purpose**: Handle and document missing value patterns

2. **Step 2**: Duplicate Detection
   - **Implementation**: `step2_duplicate_detection.py`
   - **Outputs**: `src/data_quality/outputs/duplicate_detection/`
   - **Purpose**: Identify and resolve duplicate records

3. **Step 3**: Data Type Validation
   - **Implementation**: `step3_data_type_validation.py`
   - **Outputs**: `src/data_quality/outputs/data_type_validation/`
   - **Purpose**: Validate and standardize data types

4. **Step 4**: Outlier Detection & Treatment
   - **Detection**: `step4_outlier_detection.py`
   - **Treatment**: `step4_outlier_treatment.py`
   - **Outputs**: `src/data_quality/outputs/outlier_detection/`
   - **Purpose**: Identify and treat statistical outliers

5. **Step 5**: Data Type Optimization & Persistence
   - **Implementation**: `step5_data_type_optimization.py`
   - **Outputs**: `src/data_quality/outputs/data_type_optimization/`
   - **Purpose**: Optimize memory usage and create persistent formats

6. **Step 6**: Final Quality Validation & Certification
   - **Implementation**: `step6_final_quality_validation.py`
   - **Outputs**: `src/data_quality/outputs/final_quality_validation/`
   - **Purpose**: Final quality assessment and certification

### Pipeline Execution
- **Main Runner**: `src/data_quality/pipeline_runner.py` (Executes all 6 steps)
- **Individual Steps**: Each step can be run independently
- **Output Organization**: All outputs organized by step in `src/data_quality/outputs/`
- **Quality Reports**: Comprehensive reporting for each step

### Module Structure
- **`src/data_quality/`**: Complete 6-step pipeline implementation
- **`src/csv_building/`**: Data integration and CSV building
- **Archived**: Legacy implementations and development artifacts (January 2025 cleanup)

## Enforcement

### When Creating New Files
1. **Check existing structure** first
2. **Use appropriate subfolder** in `src/`
3. **Follow naming conventions**
4. **Respect academic constraints**
5. **Reuse existing code** when possible

### When Modifying Code
1. **Modify existing files** in `src/`
2. **Add parameters** for flexibility
3. **Maintain backward compatibility**
4. **Update documentation** if needed

### Archive Management
1. **Archive outdated files** rather than deleting
2. **Document what was archived** and why
3. **Maintain recovery instructions**
4. **Keep archive structure organized**

### Recent Cleanup Activities (January 2025)
The repository has undergone comprehensive cleanup to maintain a clean, focused codebase:

**January 6, 2025 Cleanup**:
- **Python Cache Files**: Removed `__pycache__` directories from `src/data_quality/` and `src/csv_building/`
- **Empty Log Files**: Archived 5 empty log files (0 bytes) that were never used
- **Duplicate Outputs**: Archived earlier versions of quality reports and unprocessed batch files
- **Development Tools**: Archived 3 diagnostic scripts used during development
- **Alternative Runners**: Archived 4 redundant runner scripts that duplicated core functionality

**January 5, 2025 Cleanup**:
- **Outdated Logs**: Archived 20+ log files from September 2025 (outdated)
- **Duplicate Outputs**: Archived 8 duplicate output files with older timestamps
- **Old Reports**: Archived 7 superseded analysis reports and summaries

**Current Repository State**:
- **Clean Source Code**: No cache files or redundant scripts in active directories
- **Focused Structure**: Only core functionality remains in `src/` directory
- **Preserved History**: All development artifacts safely archived
- **Ready for Phase 4**: Clean codebase prepared for NLP analysis implementation

This rule must be applied to ALL tasks and code changes.
