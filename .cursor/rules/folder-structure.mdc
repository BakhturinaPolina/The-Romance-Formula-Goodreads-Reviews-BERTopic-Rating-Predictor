## Code Location
- **ALL code must be stored in**: `romance-novel-nlp-research/src/`
- **NEVER create new folders** like `scripts/`, `tools/`, etc.
- **Use existing structure**: `src/data_quality/`, `src/csv_building/`, etc.

## Data Storage Rules

### Raw Data (Read-only)
- **Location**: `romance-novel-nlp-research/data/raw/`
- **Purpose**: Original Goodreads JSON files
- **Rules**: Never modify, read-only access
- **Files**: `goodreads_*.json.gz`

### Intermediate Data (Scratch)
- **Location**: `romance-novel-nlp-research/data/intermediate/`
- **Purpose**: Temporary processing outputs
- **Rules**: Can be deleted, regenerated during processing

### Processed Data (Final)
- **Location**: `romance-novel-nlp-research/data/processed/`
- **Purpose**: Final cleaned datasets for analysis
- **Rules**: Keep, versioned outputs
- **Files**: `cleaned_romance_novels_step*.pkl/csv` (pipeline outputs)

### Logs
- **Location**: `romance-novel-nlp-research/logs/`
- **Purpose**: Execution logs, validation reports, error logs
- **Files**: `*_20250902_*.log` (timestamped execution logs)

### Outputs
- **Location**: `romance-novel-nlp-research/outputs/`
- **Purpose**: Pipeline step outputs, analysis results, reports
- **Structure**: Organized by pipeline step (step5, step6)

### Configuration
- **Location**: `romance-novel-nlp-research/config/` (if needed)
- **Purpose**: YAML configuration files
- **Note**: Currently not used in active project

### Documentation
- **Location**: `romance-novel-nlp-research/docs/` (if needed)
- **Purpose**: Project documentation, protocols, guides
- **Note**: Currently not used in active project

### Tests
- **Location**: `romance-novel-nlp-research/tests/` (if needed)
- **Purpose**: Unit tests, integration tests
- **Note**: Currently not used in active project

## Code Reuse Rules

### Pipeline Execution
- **NEVER write new pipeline code from scratch**
- **ALWAYS modify existing code** in `src/data_quality/` or `src/csv_building/`
- **Use parameters** to control behavior (e.g., `sample_size`)
- **Same script** should work for both full dataset and subsets

### Example: Pipeline Runner
```python
# ✅ CORRECT: Modify existing pipeline
pipeline = DataTypeOptimizer()
pipeline.run_complete_optimization()  # For full dataset

# ❌ WRONG: Create new pipeline code
# Don't create new files like scripts/run_pipeline_5000_sample.py
```

## File Naming Conventions

### Data Files
- **Pipeline Outputs**: `cleaned_romance_novels_step[step]_[timestamp].[format]`
- **Reports**: `*_report_step[step]_[timestamp].json`
- **Summaries**: `STEP[step]_*_SUMMARY.md`

### Log Files
- **Pipeline**: `*_step[step]_[timestamp].log`
- **Validation**: `*_validation_[timestamp].log`
- **Outlier Detection**: `outlier_detection_[timestamp].log`

### Reports
- **Summary**: `STEP[step]_*_SUMMARY.md`
- **JSON**: `*_report_step[step]_[timestamp].json`
- **Execution**: `execution_summary_step[step]_[timestamp].txt`

## Academic Compliance

### Data Handling
- **Raw inputs**: Read-only in `data/raw/`
- **Interim outputs**: `data/intermediate/` (can be deleted)
- **Final outputs**: `data/processed/` (keep for research)
- **Logs**: `logs/` (for reproducibility)
- **Reports**: `outputs/` (for documentation)

## Current Project Structure

```
romance-novel-nlp-research/
├── .cursor/                          # Cursor IDE configuration
│   └── rules/                        # Project-specific rules
├── archive/                          # Archived unused code and outputs
│   ├── data_quality_archive_20250902/  # Archived data quality files
│   │   ├── legacy_analysis/           # Replaced analysis implementations
│   │   ├── development_artifacts/     # Development versions and tests
│   │   ├── ARCHIVE_SUMMARY.md         # Archive documentation
│   │   └── CLEANUP_SUMMARY.md         # Cleanup operation summary
│   ├── pipeline_outputs_20250902/     # Archived pipeline outputs
│   │   ├── step1_3_processed/         # Step 1-3 intermediate datasets
│   │   ├── step1_3_outputs/           # Step 1-3 summary reports
│   │   ├── step4_outputs/             # Step 4 outlier detection outputs
│   │   ├── intermediate_data/          # Title cleaning and validation
│   │   ├── ARCHIVE_SUMMARY.md         # Pipeline archive documentation
│   │   └── CLEANUP_SUMMARY.md         # Pipeline cleanup summary
│   ├── data_quality_artifacts/        # Legacy data quality files
│   ├── unused_code/                   # Previous implementations
│   ├── ARCHIVE_SUMMARY.md             # Main archive documentation
│   └── DATA_QUALITY_ARCHIVE_SUMMARY.md # Data quality archive summary
├── data/                             # Data storage
│   ├── raw/                          # Original Goodreads JSON files (read-only)
│   │   ├── goodreads_book_authors.json.gz
│   │   ├── goodreads_book_genres_initial.json.gz
│   │   ├── goodreads_book_series.json.gz
│   │   ├── goodreads_book_works.json.gz
│   │   ├── goodreads_books_romance.json.gz
│   │   ├── goodreads_interactions_romance.json.gz
│   │   ├── goodreads_reviews_dedup.json.gz
│   │   ├── goodreads_reviews_romance.json.gz
│   │   ├── goodreads_reviews_spoiler.json.gz
│   │   └── README.md
│   ├── intermediate/                 # Temporary processing outputs
│   └── processed/                    # Final cleaned datasets
│       └── README.md                 # Pipeline documentation
├── outputs/                          # Current pipeline outputs
│   ├── final_quality_validation/     # Step 6: Final quality validation
│   │   ├── STEP6_FINAL_QUALITY_VALIDATION_SUMMARY.md
│   │   └── final_quality_validation_report_step6_*.json
│   └── data_type_optimization/       # Step 5: Data type optimization
│       ├── STEP5_DATA_TYPE_OPTIMIZATION_SUMMARY.md
│       ├── cleaned_romance_novels_step5_optimized_*.parquet
│       ├── cleaned_romance_novels_step5_optimized_*.pickle
│       └── data_type_optimization_report_step5_*.json
├── src/                              # Source code (ALL code must be here)
│   ├── data_quality/                 # Data quality pipeline (Steps 4-6)
│   │   ├── __init__.py               # Module exports
│   │   ├── README.md                 # Comprehensive module documentation
│   │   ├── outlier_detection_step4.py        # Step 4: Outlier Detection
│   │   ├── apply_outlier_treatment_step4.py  # Step 4: Outlier Treatment
│   │   ├── data_type_optimization_step5.py   # Step 5: Data Type Optimization
│   │   ├── final_quality_validation_step6.py # Step 6: Final Quality Validation
│   │   ├── run_outlier_detection_step4.py    # Step 4 Runner
│   │   └── README_STEP4_OUTLIER_DETECTION.md # Step 4 detailed docs
│   ├── csv_building/                 # CSV building and data integration
│   │   ├── __init__.py
│   │   ├── README.md
│   │   ├── final_csv_builder_working.py      # Main implementation
│   │   └── run_working_builder.py            # Runner script
│   └── __init__.py
├── logs/                             # Execution logs
│   ├── outlier_detection_*.log       # Step 4 execution logs
│   ├── title_cleaning_*.log          # Title cleaning logs
│   ├── data_integration_*.log        # Data integration logs
│   └── dataset_validation_*.log      # Validation logs
├── notebooks/                        # Jupyter notebooks (if needed)
├── docs/                             # Project documentation (if needed)
├── .cursorrules                      # Cursor rules configuration
├── .cursorignore                     # Cursor ignore patterns
├── .gitignore                        # Git ignore patterns
├── LICENSE                           # MIT License
├── README.md                         # Project documentation
└── requirements.txt                  # Python dependencies
```

## Pipeline Architecture

### Current Pipeline Steps
The project implements a **6-step data cleaning pipeline**:

1. **Steps 1-3**: Initial Cleaning (Missing Values, Duplicates, Data Types)
   - Implemented elsewhere in the project
   - Outputs archived in `archive/pipeline_outputs_20250902/`

2. **Step 4**: Outlier Detection & Treatment
   - **Detection**: `OutlierDetectionReporter` class
   - **Treatment**: `OutlierTreatmentApplier` class
   - **Outputs**: Statistical reports and treated datasets

3. **Step 5**: Data Type Optimization & Persistence
   - **Optimization**: `DataTypeOptimizer` class
   - **Outputs**: Optimized datasets (parquet/pickle) and reports

4. **Step 6**: Final Quality Validation & Certification
   - **Validation**: `FinalQualityValidator` class
   - **Outputs**: Quality scores, validation reports, certification

### Module Structure
- **`src/data_quality/`**: Steps 4-6 implementation
- **`src/csv_building/`**: Data integration and CSV building
- **Archived**: Legacy implementations and development artifacts

## Enforcement

### When Creating New Files
1. **Check existing structure** first
2. **Use appropriate subfolder** in `src/`
3. **Follow naming conventions**
4. **Respect academic constraints**
5. **Reuse existing code** when possible

### When Modifying Code
1. **Modify existing files** in `src/`
2. **Add parameters** for flexibility
3. **Maintain backward compatibility**
4. **Update documentation** if needed

### Archive Management
1. **Archive outdated files** rather than deleting
2. **Document what was archived** and why
3. **Maintain recovery instructions**
4. **Keep archive structure organized**

This rule must be applied to ALL tasks and code changes.
