## Repository Structure

This document defines the logical structure of the Goodreads metadata and reviews topic modeling research repository.

## Code Location
- **ALL code must be stored in**: `src/`
- **Stages are numbered** (01-08) for clear pipeline order
- **NEVER create new folders** outside the defined structure
- **Use existing stage directories** in `src/`

## Data Storage Rules

### Raw Data (Read-only)
- **Location**: `data/raw/`
- **Purpose**: Original Goodreads JSON files
- **Rules**: Never modify, read-only access
- **Files**: `goodreads_*.json.gz`

### Intermediate Data (Scratch)
- **Location**: `data/intermediate/`
- **Purpose**: Temporary processing outputs
- **Rules**: Can be deleted, regenerated during processing

### Processed Data (Final)
- **Location**: `data/processed/`
- **Purpose**: Final cleaned datasets for analysis
- **Rules**: Keep, versioned outputs
- **Files**: Processed CSV, parquet, and pickle files

### Outputs
- **Location**: `outputs/`
- **Purpose**: All research outputs, analysis results, reports, visualizations
- **Structure**: 
  - `datasets/` - All dataset versions (step-by-step + specialized)
  - `logs/` - All pipeline execution logs
  - `reports/` - All analysis reports (JSON + Markdown)
  - `visualizations/` - All plots and charts

### Documentation
- **Location**: `docs/`
- **Purpose**: Project documentation, setup guides, replication instructions
- **Files**: `setup.md`, `replication_guide.md`, etc.

### Notebooks
- **Location**: `notebooks/`
- **Purpose**: Jupyter notebooks for exploration and analysis

### Archive
- **Location**: `archive/`
- **Purpose**: Archived outdated files and development history
- **Structure**: Organized by date and purpose
- **Examples**:
  - `notebooks_02_data_quality_YYYYMMDD/` - Archived Jupyter notebooks (duplicated Python script functionality)
  - `outdated_for_github_prep_YYYYMMDD/` - Archived development files

## Source Code Structure

### Stage 01: Data Integration (`src/01_data_integration/`)
**Purpose**: CSV building, data integration, external data extraction, subdataset sampling

- `csv_building/` - CSV generation and data processing
  - `final_csv_builder.py` - Main CSV builder class (OptimizedFinalCSVBuilder) that processes raw Goodreads JSON files
  - `run_builder.py` - Interactive CLI runner for CSV building
- `subdataset_sampling/` - Representative sampling for research datasets
  - `create_subdataset_6000.py` - Creates 6,000 book subdataset with balanced tier representation
  - `run_subdataset_sampling.py` - CLI runner for subdataset sampling
- `external_data_extraction/` - External dataset extraction tools
  - `extract_romance_books.py` - Extracts author/title pairs from Hugging Face romance-books dataset
  - `bookrix_extractor.py` - Specialized BookRix URL parser for high-accuracy extraction
  - `book_matcher.py` - Matches external dataset books to Goodreads metadata using fuzzy matching

### Stage 02: Data Quality (`src/02_data_quality/`)
**Purpose**: Data quality assurance pipeline and statistical audit

- `data_quality/` - Complete 6-step data quality pipeline
  - `pipeline_runner.py` - Main pipeline orchestrator for all 6 steps
  - `step1_missing_values_cleaning.py` - Missing values detection and treatment (MissingValuesCleaner)
  - `step2_duplicate_detection.py` - Duplicate detection and resolution (DuplicateDetector)
  - `step3_data_type_validation.py` - Data type validation and conversion (DataTypeValidator)
  - `step4_outlier_detection.py` - Outlier detection using statistical methods (OutlierDetectionReporter)
  - `step4_outlier_treatment.py` - Outlier treatment strategies (OutlierTreatmentApplier)
  - `step5_data_type_optimization.py` - Data type optimization for memory efficiency (DataTypeOptimizer)
  - `step6_final_quality_validation.py` - Final quality validation and certification (FinalQualityValidator)
  - `comprehensive_data_cleaner.py` - Comprehensive data cleaning tool (ComprehensiveDataCleaner)
- `data_audit/` - Statistical analysis and data exploration
  - `core/data_auditor.py` - Main audit script for statistical analysis (DataAuditor)
  - `parsing/list_parser.py` - List parsing utilities for structured data extraction
  - `comprehensive_data_analysis.py` - Comprehensive data analysis tool (ComprehensiveDataAnalyzer)
- `utils/` - Utility functions
  - `diff_bridge_runs.py` - Utility for comparing different pipeline runs
- `Makefile` - Makefile for running audit commands
- `requirements.txt` - Python dependencies

### Stage 03: Text Preprocessing (`src/03_text_preprocessing/`)
**Purpose**: NLP text preprocessing and normalization

- `text_preprocessor.py` - Main text preprocessor
- `run_preprocessor.py` - Preprocessor runner
- `test_preprocessor.py` - Preprocessor tests

### Stage 04: Review Extraction (`src/04_review_extraction/`)
**Purpose**: Review extraction and filtering

- `core/` - Main extraction logic
  - `extract_reviews.py` - Main extraction script
- `monitoring/` - Monitoring and time estimation
  - `monitor_extraction.py` - Real-time monitoring
  - `estimate_time.py` - Time estimation utility
  - `log_parser.py` - Shared log parsing utilities
- `scripts/` - Wrapper scripts for running and monitoring
  - `run_extraction.sh` - Extraction wrapper (uses venv)
  - `monitor.sh` - Monitoring wrapper (uses venv)
  - `venv_setup.sh` - Shared venv setup (sourced by wrappers)
- `utils/` - Utility scripts
  - `review_dataset.py` - Dataset review utility

### Stage 05: Prepare Reviews Corpus for BERTopic (`src/05_prepare_reviews_for_BERTopic/`)
**Purpose**: Prepare sentence-level corpus from reviews for BERTopic analysis

- `core/` - Main preparation logic
  - `prepare_bertopic_input.py` - Main preparation script (sentence splitting)
- `monitoring/` - Monitoring and status checking
  - `monitor_preparation.py` - Real-time monitoring
  - `check_status.py` - Quick status check
- `scripts/` - Execution and test scripts
  - `run_preparation.py` - Python wrapper for preparation (uses venv)
  - `run_test.py` - Python test runner (uses venv)
  - `venv_utils.py` - Virtual environment utilities (venv verification)
- `utils/` - Utility modules
  - `data_loading.py` - Data loading utilities
  - `checks_coverage.py` - Coverage checking utilities
- `notebooks/` - Jupyter notebooks for exploration
  - `eda_reviews.ipynb` - Exploratory data analysis

### Stage 06: Topic Modeling (`src/06_topic_modeling/`)
**Purpose**: BERTopic analysis and optimization with OCTIS

- `core/` - Main modeling logic
  - `bertopic_plus_octis.py` - Main BERTopic+OCTIS optimization script
  - `optimizer.py` - OCTIS optimizer configuration
  - `load_raw_sentences.py` - Load sentences for BERTopic
- `scripts/` - Execution and utility scripts
  - `restart_script.py` - Resume interrupted runs
  - `sample_test_dataset.py` - Create test datasets
  - `topic_npy_to_json.py` - Convert topic outputs to JSON
- `config/` - Configuration files
  - `config_bertopic_reviews.yaml` - BERTopic configuration
- `docs/` - Documentation
  - `README.md` - BERTopic_OCTIS documentation
  - `CONFIG_USAGE.md` - Configuration usage guide

### Stage 07: Shelf Normalization (`src/07_shelf_normalization/`)
**Purpose**: Shelf tag normalization and canonicalization

- `core/` - Core normalization logic
  - `shelf_normalize.py` - Main normalization script
  - `simple_shelf_cleaner.py` - Simple cleaning utilities
  - `hybrid_classifier.py` - Hybrid classification approach
  - `simple_semantic_cluster.py` - Semantic clustering
  - `extract_shelves.py` - Shelf extraction utilities
- `bridge/` - Integration with other pipeline steps
  - `bridge_audit_normalize.py` - Bridge normalization with parsed data
- `diagnostics/` - Quality assurance and validation
  - `diagnostics_explore.py` - Diagnostic exploration
  - `validate_bridge.py` - Bridge output validation
- `scripts/` - Execution scripts
  - `Makefile` - Pipeline execution Makefile
- `docs/` - Documentation
  - `LABELLING_GUIDE.md` - Labelling guide
  - `SEMANTIC_CLUSTERING_RESULTS.md` - Semantic clustering results
  - `SHELF_NORMALIZATION_IMPROVEMENTS.md` - Improvement documentation
  - `SIMPLE_IMPROVEMENTS.md` - Simple improvements documentation
  - `TESTING_RESULTS.md` - Testing results
- `config/` - Configuration files
- `tests/` - Test files
- `outputs/` - Stage-specific outputs (test outputs, checkpoints, etc.)
- `requirements.txt` - Python dependencies

### Stage 08: Corpus Analysis (`src/08_corpus_analysis/`)
**Purpose**: Corpus statistics and analysis

- `generate_corpus_statistics.py` - Corpus statistics generation
- `README.md` - Stage documentation
- `README_SCIENTIFIC.md` - Scientific documentation

## File Naming Conventions

### Data Files
- **Pipeline Outputs**: `*_step[step]_[timestamp].[format]`
- **Processed Data**: `*_[description]_[timestamp].[format]`
- **Reports**: `*_report_[timestamp].json` or `*_report_[timestamp].md`

### Source Code Files
- **Stage Implementations**: Descriptive names (e.g., `extract_reviews.py`)
- **Main Runners**: `run_[description].py` or `pipeline_runner.py`
- **Utilities**: Descriptive names indicating purpose

### Output Files
- **Stage Outputs**: Organized by stage in `outputs/`
- **Timestamped Files**: `*_[YYYYMMDD_HHMMSS].[format]`
- **Quality Reports**: `*_quality_report_[timestamp].md`

## Academic Compliance

### Data Handling
- **Raw inputs**: Read-only in `data/raw/`
- **Interim outputs**: `data/intermediate/` (can be deleted)
- **Final outputs**: `data/processed/` (keep for research)
- **Logs**: `outputs/logs/` (for reproducibility)
- **Reports**: `outputs/reports/` (for documentation)

## Current Project Structure

```
goodreads-topic-modeling/
├── .cursor/                          # Cursor IDE configuration
│   └── rules/                        # Project-specific rules
├── archive/                          # Archived development history
│   ├── notebooks_02_data_quality_YYYYMMDD/  # Archived notebooks (duplicated functionality)
│   └── outdated_for_github_prep_YYYYMMDD/    # Archived development files
├── data/                             # Data storage
│   ├── raw/                          # Original Goodreads JSON files (read-only)
│   ├── intermediate/                 # Temporary processing outputs
│   └── processed/                    # Final cleaned datasets
├── outputs/                          # All research outputs
│   ├── datasets/                     # All dataset versions
│   ├── logs/                         # All pipeline execution logs
│   ├── reports/                      # All analysis reports
│   └── visualizations/               # Publication-ready plots
├── src/                              # Source code (organized by research stage)
│   ├── 01_data_integration/          # CSV building, data integration
│   │   ├── csv_building/              # Main dataset building
│   │   ├── subdataset_sampling/       # Representative sampling
│   │   ├── external_data_extraction/ # External dataset integration
│   │   ├── README.md                  # Stage documentation
│   │   └── README_SCIENTIFIC.md      # Scientific documentation
│   ├── 02_data_quality/              # 6-step data quality pipeline + audit
│   │   ├── data_quality/              # 6-step quality pipeline
│   │   ├── data_audit/                # Statistical analysis and audit
│   │   ├── utils/                      # Utility functions
│   │   ├── README.md                   # Stage documentation
│   │   └── README_SCIENTIFIC.md        # Scientific documentation
│   ├── 03_text_preprocessing/        # NLP text preprocessing
│   ├── 04_review_extraction/         # Review extraction and filtering
│   ├── 05_prepare_reviews_for_BERTopic/  # Prepare sentence-level corpus
│   ├── 06_topic_modeling/            # BERTopic analysis and optimization
│   ├── 07_shelf_normalization/       # Shelf tag normalization
│   └── 08_corpus_analysis/            # Corpus statistics and analysis
├── docs/                              # Project documentation
│   ├── setup.md                       # Setup instructions
│   └── replication_guide.md          # How to use with other datasets
├── notebooks/                         # Jupyter notebooks for exploration
├── README.md                          # Main project README
├── README_SCIENTIFIC.md               # Scientific README
├── LICENSE                            # MIT License
├── requirements.txt                   # Python dependencies
└── .gitignore                         # Git ignore patterns
```

## Pipeline Architecture

### Research Pipeline Flow

1. **Stage 01: Data Integration** → CSV building, data integration, sampling
2. **Stage 02: Data Quality** → 6-step quality pipeline + statistical audit
3. **Stage 03: Text Preprocessing** → Text cleaning and normalization
4. **Stage 04: Review Extraction** → Extract and filter reviews
5. **Stage 05: Prepare Reviews Corpus for BERTopic** → Sentence splitting and corpus creation
6. **Stage 06: Topic Modeling** → BERTopic analysis and optimization
7. **Stage 07: Shelf Normalization** → Normalize shelf tags
8. **Stage 08: Corpus Analysis** → Generate corpus statistics

### Stage Documentation

Each stage directory contains:
- `README.md` - Standard README (purpose, I/O, how to run, dependencies, examples)
- `README_SCIENTIFIC.md` - Scientific README (objectives, questions, hypotheses, methodology, tools, results)

## Enforcement

### When Creating New Files
1. **Check existing structure** first
2. **Use appropriate stage directory** in `src/`
3. **Follow naming conventions**
4. **Respect academic constraints**
5. **Reuse existing code** when possible

### When Modifying Code
1. **Modify existing files** in `src/`
2. **Add parameters** for flexibility
3. **Maintain backward compatibility**
4. **Update documentation** if needed

### Archive Management
1. **Archive outdated files** rather than deleting
2. **Document what was archived** and why
3. **Maintain recovery instructions**
4. **Keep archive structure organized**

This rule must be applied to ALL tasks and code changes.
