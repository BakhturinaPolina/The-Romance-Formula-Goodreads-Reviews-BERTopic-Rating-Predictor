## Repository Structure

This document defines the logical structure of the Goodreads metadata and reviews topic modeling research repository.

## Code Location
- **ALL code must be stored in**: `src/`
- **Stages are numbered** (01-07) for clear pipeline order
- **NEVER create new folders** outside the defined structure
- **Use existing stage directories** in `src/`

## Data Storage Rules

### Raw Data (Read-only)
- **Location**: `data/raw/`
- **Purpose**: Original Goodreads JSON files
- **Rules**: Never modify, read-only access
- **Files**: `goodreads_*.json.gz`

### Intermediate Data (Scratch)
- **Location**: `data/intermediate/`
- **Purpose**: Temporary processing outputs
- **Rules**: Can be deleted, regenerated during processing

### Processed Data (Final)
- **Location**: `data/processed/`
- **Purpose**: Final cleaned datasets for analysis
- **Rules**: Keep, versioned outputs
- **Files**: Processed CSV, parquet, and pickle files

### Outputs
- **Location**: `outputs/`
- **Purpose**: All research outputs, analysis results, reports, visualizations
- **Structure**: 
  - `datasets/` - All dataset versions (step-by-step + specialized)
  - `logs/` - All pipeline execution logs
  - `reports/` - All analysis reports (JSON + Markdown)
  - `visualizations/` - All plots and charts

### Documentation
- **Location**: `docs/`
- **Purpose**: Project documentation, setup guides, replication instructions
- **Files**: `setup.md`, `replication_guide.md`, etc.

### Notebooks
- **Location**: `notebooks/`
- **Purpose**: Jupyter notebooks for exploration and analysis

### Archive
- **Location**: `archive/`
- **Purpose**: Archived outdated files and development history
- **Structure**: Organized by date and purpose

## Source Code Structure

### Stage 01: Data Integration (`src/01_data_integration/`)
**Purpose**: CSV building, data integration, external data extraction, subdataset sampling

- `csv_building/` - CSV generation and data processing
  - `final_csv_builder.py` - Main CSV builder class (OptimizedFinalCSVBuilder) that processes raw Goodreads JSON files
  - `run_builder.py` - Interactive CLI runner for CSV building
- `subdataset_sampling/` - Representative sampling for research datasets
  - `create_subdataset_6000.py` - Creates 6,000 book subdataset with balanced tier representation
  - `run_subdataset_sampling.py` - CLI runner for subdataset sampling
- `external_data_extraction/` - External dataset extraction tools
  - `extract_romance_books.py` - Extracts author/title pairs from Hugging Face romance-books dataset
  - `bookrix_extractor.py` - Specialized BookRix URL parser for high-accuracy extraction
  - `book_matcher.py` - Matches external dataset books to Goodreads metadata using fuzzy matching

### Stage 02: Data Quality (`src/02_data_quality/`)
**Purpose**: Data quality assurance pipeline and statistical audit

- `data_quality/` - Complete 6-step data quality pipeline
  - `pipeline_runner.py` - Main pipeline runner
  - `step1_missing_values_cleaning.py` - Missing values cleaning
  - `step2_duplicate_detection.py` - Duplicate detection
  - `step3_data_type_validation.py` - Data type validation
  - `step4_outlier_detection.py` - Outlier detection
  - `step4_outlier_treatment.py` - Outlier treatment
  - `step5_data_type_optimization.py` - Data type optimization
  - `step6_final_quality_validation.py` - Final quality validation
- `data_audit/` - Statistical analysis and data exploration
  - `core/data_auditor.py` - Main audit script
  - `parsing/list_parser.py` - List parsing utilities
  - `notebooks/` - Interactive analysis notebooks

### Stage 03: Text Preprocessing (`src/03_text_preprocessing/`)
**Purpose**: NLP text preprocessing and normalization

- `text_preprocessor.py` - Main text preprocessor
- `run_preprocessor.py` - Preprocessor runner
- `test_preprocessor.py` - Preprocessor tests

### Stage 04: Review Extraction (`src/04_review_extraction/`)
**Purpose**: Review extraction and filtering

- `extract_reviews.py` - Main extraction script
- `monitor_extraction.py` - Real-time monitoring
- `estimate_time.py` - Time estimation utility
- `review_dataset.py` - Dataset review utility

### Stage 05: Topic Modeling (`src/05_topic_modeling/`)
**Purpose**: BERTopic analysis and optimization

- `BERTopic_OCTIS/` - BERTopic with OCTIS integration
- `bertopic_preparation/` - Input preparation for BERTopic
- `data_loading.py` - Data loading utilities
- `checks_coverage.py` - Coverage checking utilities

### Stage 06: Shelf Normalization (`src/06_shelf_normalization/`)
**Purpose**: Shelf tag normalization and canonicalization

- `core/` - Core normalization logic
- `bridge/` - Integration with other pipeline steps
- `diagnostics/` - Quality assurance and validation

### Stage 07: Corpus Analysis (`src/07_corpus_analysis/`)
**Purpose**: Corpus statistics and analysis

- `generate_corpus_statistics.py` - Corpus statistics generation

## File Naming Conventions

### Data Files
- **Pipeline Outputs**: `*_step[step]_[timestamp].[format]`
- **Processed Data**: `*_[description]_[timestamp].[format]`
- **Reports**: `*_report_[timestamp].json` or `*_report_[timestamp].md`

### Source Code Files
- **Stage Implementations**: Descriptive names (e.g., `extract_reviews.py`)
- **Main Runners**: `run_[description].py` or `pipeline_runner.py`
- **Utilities**: Descriptive names indicating purpose

### Output Files
- **Stage Outputs**: Organized by stage in `outputs/`
- **Timestamped Files**: `*_[YYYYMMDD_HHMMSS].[format]`
- **Quality Reports**: `*_quality_report_[timestamp].md`

## Academic Compliance

### Data Handling
- **Raw inputs**: Read-only in `data/raw/`
- **Interim outputs**: `data/intermediate/` (can be deleted)
- **Final outputs**: `data/processed/` (keep for research)
- **Logs**: `outputs/logs/` (for reproducibility)
- **Reports**: `outputs/reports/` (for documentation)

## Current Project Structure

```
goodreads-topic-modeling/
├── .cursor/                          # Cursor IDE configuration
│   └── rules/                        # Project-specific rules
├── archive/                          # Archived development history
│   └── outdated_for_github_prep_YYYYMMDD/
├── data/                             # Data storage
│   ├── raw/                          # Original Goodreads JSON files (read-only)
│   ├── intermediate/                 # Temporary processing outputs
│   └── processed/                    # Final cleaned datasets
├── outputs/                          # All research outputs
│   ├── datasets/                     # All dataset versions
│   ├── logs/                         # All pipeline execution logs
│   ├── reports/                      # All analysis reports
│   └── visualizations/               # Publication-ready plots
├── src/                              # Source code (organized by research stage)
│   ├── 01_data_integration/          # CSV building, data integration
│   │   ├── csv_building/              # Main dataset building
│   │   ├── subdataset_sampling/       # Representative sampling
│   │   ├── external_data_extraction/ # External dataset integration
│   │   ├── README.md                  # Stage documentation
│   │   └── README_SCIENTIFIC.md      # Scientific documentation
│   ├── 02_data_quality/              # 6-step data quality pipeline + audit
│   ├── 03_text_preprocessing/        # NLP text preprocessing
│   ├── 04_review_extraction/         # Review extraction and filtering
│   ├── 05_topic_modeling/            # BERTopic analysis and optimization
│   ├── 06_shelf_normalization/       # Shelf tag normalization
│   └── 07_corpus_analysis/            # Corpus statistics and analysis
├── docs/                              # Project documentation
│   ├── setup.md                       # Setup instructions
│   └── replication_guide.md          # How to use with other datasets
├── notebooks/                         # Jupyter notebooks for exploration
├── README.md                          # Main project README
├── README_SCIENTIFIC.md               # Scientific README
├── LICENSE                            # MIT License
├── requirements.txt                   # Python dependencies
└── .gitignore                         # Git ignore patterns
```

## Pipeline Architecture

### Research Pipeline Flow

1. **Stage 01: Data Integration** → CSV building, data integration, sampling
2. **Stage 02: Data Quality** → 6-step quality pipeline + statistical audit
3. **Stage 03: Text Preprocessing** → Text cleaning and normalization
4. **Stage 04: Review Extraction** → Extract and filter reviews
5. **Stage 05: Topic Modeling** → BERTopic analysis and optimization
6. **Stage 06: Shelf Normalization** → Normalize shelf tags
7. **Stage 07: Corpus Analysis** → Generate corpus statistics

### Stage Documentation

Each stage directory contains:
- `README.md` - Standard README (purpose, I/O, how to run, dependencies, examples)
- `README_SCIENTIFIC.md` - Scientific README (objectives, questions, hypotheses, methodology, tools, results)

## Enforcement

### When Creating New Files
1. **Check existing structure** first
2. **Use appropriate stage directory** in `src/`
3. **Follow naming conventions**
4. **Respect academic constraints**
5. **Reuse existing code** when possible

### When Modifying Code
1. **Modify existing files** in `src/`
2. **Add parameters** for flexibility
3. **Maintain backward compatibility**
4. **Update documentation** if needed

### Archive Management
1. **Archive outdated files** rather than deleting
2. **Document what was archived** and why
3. **Maintain recovery instructions**
4. **Keep archive structure organized**

This rule must be applied to ALL tasks and code changes.
