#!/usr/bin/env python3
"""
Quick Step 2 Planning Analysis
Get key insights for duplicate detection and inconsistency analysis.
"""

import pandas as pd
import sys
from pathlib import Path

# Add the src directory to the path
sys.path.append(str(Path(__file__).parent.parent))

from data_quality.data_quality_assessment import DataQualityAssessment


def quick_step2_analysis():
    """Quick analysis for Step 2 planning."""
    print("ğŸ” Quick Step 2 Planning Analysis")
    print("=" * 50)
    
    # Load the cleaned dataset
    cleaned_files = list(Path("../../data/processed").glob("final_books_*_cleaned_nlp_ready_*.csv"))
    if not cleaned_files:
        print("âŒ No cleaned dataset found")
        return
    
    cleaned_file = cleaned_files[0]
    print(f"ğŸ“š Loading: {cleaned_file.name}")
    
    try:
        data = pd.read_csv(cleaned_file)
        print(f"âœ… Loaded: {len(data):,} books")
    except Exception as e:
        print(f"âŒ Failed: {e}")
        return
    
    print(f"\nğŸ“Š KEY INSIGHTS FOR STEP 2:")
    
    # 1. Work ID analysis
    work_id_duplicates = data['work_id'].duplicated().sum()
    print(f"  ğŸ“š Work ID Duplicates: {work_id_duplicates:,}")
    
    # 2. Title analysis
    title_duplicates = data['title'].duplicated().sum()
    title_unique = data['title'].nunique()
    print(f"  ğŸ“– Title Duplicates: {title_duplicates:,} ({title_duplicates/len(data)*100:.1f}%)")
    print(f"    - Unique titles: {title_unique:,}")
    
    if title_duplicates > 0:
        duplicate_titles = data[data['title'].duplicated(keep=False)]['title'].value_counts().head(5)
        print(f"    - Top duplicates: {duplicate_titles.to_dict()}")
    
    # 3. Author analysis
    author_unique = data['author_id'].nunique()
    avg_books_per_author = len(data) / author_unique
    print(f"  ğŸ‘¤ Authors: {author_unique:,} unique")
    print(f"    - Avg books per author: {avg_books_per_author:.1f}")
    
    # 4. Series analysis
    series_books = data['series_id'].notna().sum()
    standalone_books = len(data) - series_books
    print(f"  ğŸ“š Series: {series_books:,} books in series")
    print(f"    - Standalone: {standalone_books:,} books")
    
    # 5. Genre analysis
    genre_unique = data['genres'].nunique()
    print(f"  ğŸ·ï¸  Genres: {genre_unique:,} unique combinations")
    
    # 6. Publication year
    year_range = f"{data['publication_year'].min()} - {data['publication_year'].max()}"
    print(f"  ğŸ“… Publication Years: {year_range}")
    
    # 7. Rating analysis
    rating_null = data['average_rating_weighted_mean'].isnull().sum()
    print(f"  â­ Ratings: {rating_null:,} missing ({rating_null/len(data)*100:.1f}%)")
    
    print(f"\nğŸ¯ STEP 2 IMPLEMENTATION PLAN:")
    print(f"  Based on this analysis:")
    
    if work_id_duplicates > 0:
        print(f"  1. ğŸ”´ CRITICAL: Fix {work_id_duplicates} duplicate work IDs")
    else:
        print(f"  1. âœ… Work IDs are unique - no action needed")
    
    if title_duplicates > 0:
        print(f"  2. ğŸŸ¡ HIGH: Investigate {title_duplicates} duplicate titles")
        print(f"     - Focus on common titles like 'Broken', 'Second Chances'")
        print(f"     - Check if these are legitimate duplicates or different books")
    
    print(f"  3. ğŸŸ¡ HIGH: Series data validation")
    print(f"     - {series_books:,} books claim to be in series")
    print(f"     - Validate series_works_count vs. actual series size")
    
    print(f"  4. ğŸŸ¢ MEDIUM: Author consistency checks")
    print(f"     - {author_unique:,} unique authors for {len(data):,} books")
    print(f"     - Verify author attribution accuracy")
    
    print(f"  5. ğŸŸ¢ MEDIUM: Genre classification validation")
    print(f"     - {genre_unique:,} unique genre combinations")
    print(f"     - Check for inconsistent genre patterns")
    
    print(f"  6. ğŸŸ¢ MEDIUM: Publication year validation")
    print(f"     - Ensure all years are within 2000-2020 range")
    
    print(f"\nğŸ“‹ NEXT ACTIONS:")
    print(f"  1. Implement duplicate detection algorithms")
    print(f"  2. Create inconsistency validation checks")
    print(f"  3. Generate detailed quality report")
    print(f"  4. Plan data cleaning strategies")
    
    return data


if __name__ == "__main__":
    data = quick_step2_analysis()
    
    if data is not None:
        print(f"\nâœ… Quick analysis completed!")
        print(f"ğŸ“ Ready to implement Step 2: Duplicate and Inconsistency Detection")
    else:
        print(f"\nâŒ Analysis failed!")
        sys.exit(1)
