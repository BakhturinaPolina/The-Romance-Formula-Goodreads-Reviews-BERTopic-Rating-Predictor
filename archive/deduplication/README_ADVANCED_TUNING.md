# Advanced Multi-Objective Hyperparameter Tuning

This document describes the advanced hyperparameter tuning system that implements sophisticated optimization techniques for the deduplication pipeline.

## üéØ Overview

The advanced tuner addresses the limitations of basic hyperparameter search by implementing:

- **Multi-objective optimization** with NSGA-II and MOTPE samplers
- **Multi-fidelity budgets** (5% ‚Üí 20% ‚Üí 100% data) for efficient exploration
- **Early pruning** with Hyperband/ASHA to eliminate weak trials
- **Constraint handling** for minimum coverage and maximum edges
- **Pareto front re-evaluation** on full data for final selection
- **Robust evaluation** with multiple folds to reduce variance
- **Visualization tools** for analysis and interpretation

## üöÄ Quick Start

### 1. Install Dependencies

```bash
pip install optuna>=3.0.0 plotly kaleido
```

### 2. Create Sample Dataset

```bash
# Create a representative sample for fast tuning
python -c "
import pandas as pd
df = pd.read_parquet('data/intermediate/token_pairs.parquet')
sample = df.sample(n=100000, random_state=42)
sample.to_parquet('data/intermediate/token_pairs_sample.parquet', index=False)
print(f'Sampled {len(sample):,} pairs')
"
```

### 3. Run Advanced Tuning

```bash
# NSGA-II with multi-fidelity budgets
python -m src.deduplication.param_tuner_advanced tune \
  --in-pairs data/intermediate/token_pairs_sample.parquet \
  --out-base organized_outputs/advanced_tuning \
  --trials 60 \
  --n-jobs 4 \
  --study-name advanced_nsga2 \
  --storage sqlite:///advanced_tuning.db \
  --sampler nsga2 \
  --pruner hyperband \
  --budgets 0.05,0.20,1.00 \
  --min-coverage 10 \
  --max-edges 20000 \
  --top-k 5
```

### 4. Analyze Results

```bash
# Generate visualizations and analysis
python -m src.deduplication.param_tuner_advanced analyze \
  --study-name advanced_nsga2 \
  --storage sqlite:///advanced_tuning.db \
  --out-dir organized_outputs/analysis
```

## üß† Key Features

### Multi-Objective Optimization

The tuner optimizes three objectives simultaneously:

1. **Maximize good clusters** (quality) - clusters passing size-aware criteria
2. **Maximize coverage** (breadth) - number of tokens normalized
3. **Minimize edges** (efficiency) - graph sparsity for faster processing

**Samplers:**
- **NSGA-II**: Genetic algorithm for multi-objective optimization
- **MOTPE**: Multi-objective Tree-structured Parzen Estimator

### Multi-Fidelity Budgets

Instead of evaluating on full data immediately, the tuner uses progressive budgets:

- **5% budget**: Fast exploration, identify promising regions
- **20% budget**: Refined evaluation, prune weak candidates  
- **100% budget**: Final evaluation, select Pareto winners

This approach is **5-10x faster** than full-data evaluation while maintaining quality.

### Early Pruning

**Hyperband Pruner**: Eliminates weak trials early based on intermediate results
- Reduces computational waste on poor configurations
- Focuses budget on promising parameter regions
- Configurable reduction factor (default: 3)

### Constraint Handling

Hard constraints ensure viable solutions:
- `min_coverage`: Minimum number of tokens that must be clustered
- `max_edges`: Maximum graph size for efficiency
- Violations are tracked and reported

### Robust Evaluation

- **Multiple folds**: Each trial evaluated on 2-3 random folds
- **Median aggregation**: Reduces variance from random sampling
- **Deterministic subsampling**: Consistent budgets across trials

## üìä Usage Examples

### Basic Advanced Tuning

```bash
python -m src.deduplication.param_tuner_advanced tune \
  --in-pairs input_pairs.parquet \
  --out-base outputs/tuning \
  --trials 40 \
  --sampler nsga2 \
  --budgets 0.1,0.5,1.0
```

### High-Performance Tuning

```bash
python -m src.deduplication.param_tuner_advanced tune \
  --in-pairs input_pairs.parquet \
  --out-base outputs/tuning \
  --trials 100 \
  --n-jobs 8 \
  --study-name production_tuning \
  --storage sqlite:///production.db \
  --sampler motpe \
  --pruner hyperband \
  --folds 3 \
  --budgets 0.05,0.20,1.00 \
  --min-coverage 50 \
  --max-edges 50000 \
  --timeout-sec 3600
```

### Sampler Comparison

```bash
# Compare NSGA-II vs MOTPE
for sampler in nsga2 motpe; do
  python -m src.deduplication.param_tuner_advanced tune \
    --in-pairs input_pairs.parquet \
    --out-base outputs/comparison_$sampler \
    --trials 30 \
    --sampler $sampler \
    --study-name comparison_$sampler \
    --storage sqlite:///comparison.db
done
```

## üîß Configuration Options

### Search Space

The tuner optimizes these parameters:

**Graph Building:**
- `base_threshold`: [0.55, 0.90] - Minimum similarity floor
- `k`: [3, 15] - Top-k neighbors per token
- `mutual_nn`: {True, False} - Mutual nearest neighbor filtering
- `max_degree`: [30, 200] - Hub capping limit
- `adaptive_short_thr`: [0.70, 0.95] - Threshold for short tokens

**Quality Gates:**
- `pair_min_sim`: [0.80, 0.92] - Minimum similarity for pairs
- `small_min_sim`: [0.70, 0.90] - Minimum similarity for small clusters
- `small_min_tri`: [0.10, 0.60] - Minimum triangle rate for small clusters
- `big_min_sim`: [0.65, 0.85] - Minimum similarity for big clusters
- `big_min_mean`: [0.72, 0.86] - Minimum mean similarity for big clusters
- `big_min_tri`: [0.10, 0.40] - Minimum triangle rate for big clusters

### Multi-Fidelity Configuration

```bash
--budgets 0.05,0.20,1.00    # Progressive budgets
--folds 2                   # Robustness folds
--pruner hyperband          # Early stopping
```

### Constraint Configuration

```bash
--min-coverage 10           # Minimum tokens clustered
--max-edges 20000          # Maximum graph size
```

## üìà Results Analysis

### Pareto Front

The tuner finds a **Pareto front** of non-dominated solutions:

```python
import pandas as pd

# Load Pareto results
pareto = pd.read_csv('outputs/tuning/pareto_trials.csv')
print("Pareto Front Solutions:")
print(pareto[['trial', 'good_clusters', 'coverage', 'edges']].head())

# Full data re-evaluation results
full_results = pd.read_csv('outputs/tuning/pareto_full_results.csv')
print("\nFull Data Results:")
print(full_results[['rank', 'trial', 'good_clusters', 'coverage', 'edges']])
```

### Visualization

The tuner creates interactive visualizations:

- **Pareto Front Plot**: Shows trade-offs between objectives
- **Parameter Importance**: Identifies most influential parameters
- **Optimization History**: Tracks convergence over trials

```bash
# View visualizations
open outputs/tuning/pareto_front.html
open outputs/tuning/param_importance.html
open outputs/tuning/optimization_history.html
```

### Selection Strategy

After tuning, select from Pareto solutions:

1. **Primary**: Maximize `good_clusters` (quality first)
2. **Secondary**: Maximize `coverage` (normalization breadth)
3. **Tertiary**: Minimize `edges` (efficiency)

```python
# Select best configuration
best = full_results.iloc[0]  # Already sorted by objectives
params = {
    'base_threshold': best['base_threshold'],
    'k': int(best['k']),
    'max_degree': int(best['max_degree']),
    # ... other parameters
}
```

## üöÄ Production Deployment

### 1. Scale Up Winners

```bash
# Re-run top 3 Pareto solutions on full dataset
python -m src.deduplication.param_tuner_advanced tune \
  --in-pairs data/intermediate/token_pairs.parquet \
  --out-base outputs/production_validation \
  --trials 3 \
  --study-name production_validation \
  --storage sqlite:///production.db \
  --sampler nsga2 \
  --budgets 1.0 \
  --top-k 3
```

### 2. Use Best Parameters

```bash
# Apply optimized parameters to production pipeline
python src/deduplication/dedupe_pipeline.py refined-all \
  data/intermediate/token_pairs.parquet \
  outputs/production \
  --base-threshold 0.65 \
  --k 8 \
  --max-degree 100 \
  --adaptive-short-thr 0.80 \
  --pair-min-sim 0.88 \
  --small-min-sim 0.75 \
  --small-min-tri 0.35 \
  --big-min-sim 0.70 \
  --big-min-mean 0.78 \
  --big-min-tri 0.20 \
  --mutual-nn
```

## üî¨ Advanced Usage

### Custom Objectives

To add custom objectives, modify the objective function:

```python
def custom_objective(trial: optuna.Trial) -> Tuple[float, float, float, float]:
    # ... existing code ...
    
    # Add custom metric
    custom_metric = compute_custom_metric(out_dir)
    
    # Return 4 objectives
    return (-good_clusters, -coverage, edges, -custom_metric)
```

### Distributed Tuning

For cluster-scale tuning, use Ray Tune integration:

```python
import ray
from ray import tune
from ray.tune.schedulers import ASHAScheduler
from ray.tune.search.optuna import OptunaSearch

# Configure Ray
ray.init()

# Define search space
search_space = {
    "base_threshold": tune.uniform(0.55, 0.90),
    "k": tune.randint(3, 15),
    "max_degree": tune.randint(30, 200),
    # ... other parameters
}

# Run distributed tuning
analysis = tune.run(
    objective_function,
    search_alg=OptunaSearch(),
    scheduler=ASHAScheduler(metric="good_clusters", mode="max"),
    config=search_space,
    num_samples=1000,
    resources_per_trial={"cpu": 2, "gpu": 0}
)
```

### Alternative Samplers

The system supports multiple optimization algorithms:

- **NSGA-II**: Genetic algorithm, good for discrete spaces
- **MOTPE**: Bayesian optimization, efficient for continuous spaces
- **Random**: Baseline comparison
- **Grid**: Exhaustive search (small spaces only)

## üìö Performance Comparison

| Method | Trials to Good Results | Parameter Learning | Multi-Objective | Efficiency |
|--------|----------------------|-------------------|-----------------|------------|
| **Grid Search** | 1000+ | None | No | Very Low |
| **Random Search** | 500+ | None | No | Low |
| **TPE (Basic)** | 50-100 | Full | No | High |
| **NSGA-II (Advanced)** | 30-60 | Full | **Yes** | **Very High** |
| **MOTPE (Advanced)** | 40-80 | Full | **Yes** | **Very High** |

## üêõ Troubleshooting

### Common Issues

1. **Timeout errors**: Increase `--timeout-sec` or reduce sample size
2. **Memory issues**: Use smaller budgets or reduce `max_degree`
3. **No good clusters**: Lower quality thresholds or increase `base_threshold`
4. **Too many edges**: Increase `k` or enable `mutual_nn`
5. **Pruning too aggressive**: Reduce `reduction_factor` in Hyperband

### Debug Mode

```bash
# Run with verbose output and keep all artifacts
python -m src.deduplication.param_tuner_advanced tune \
  --keep-artifacts \
  --timeout-sec 1200 \
  --trials 5 \
  --budgets 0.1,1.0
```

### Performance Tips

- Use SSD storage for SQLite database
- Enable parallel execution with `--n-jobs`
- Start with smaller sample sizes (50k-100k pairs)
- Use `--keep-artifacts` for debugging
- Monitor memory usage with `psutil`

## üìñ References

- [Optuna Multi-Objective Optimization](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/003_efficient_optimization_algorithms.html#multi-objective-optimization)
- [NSGA-II Algorithm](https://ieeexplore.ieee.org/document/996017)
- [Hyperband Pruning](https://arxiv.org/abs/1603.06560)
- [Multi-Fidelity Optimization](https://arxiv.org/abs/1807.01838)
- [Ray Tune Documentation](https://docs.ray.io/en/latest/tune/index.html)

## üéØ Key Insights

1. **Multi-objective is crucial** - Single metrics miss important trade-offs
2. **Multi-fidelity budgets** - 5-10x faster than full-data evaluation
3. **Early pruning** - Eliminates computational waste on poor configs
4. **Pareto analysis** - Multiple good solutions, not just one "best"
5. **Constraint handling** - Ensures viable, production-ready solutions
6. **Robust evaluation** - Multiple folds reduce variance and improve reliability

This advanced tuning system transforms your deduplication pipeline from manual parameter guessing to systematic, data-driven optimization with state-of-the-art multi-objective techniques.
