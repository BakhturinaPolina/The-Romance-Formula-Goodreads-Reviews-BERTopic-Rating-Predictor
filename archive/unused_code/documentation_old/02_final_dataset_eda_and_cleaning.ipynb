{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02: Final Dataset EDA and Cleaning Recommendations\n",
    "\n",
    "**Objective**: Comprehensive exploration of the final processed romance novel dataset to identify cleaning opportunities and prepare for NLP analysis.\n",
    "\n",
    "**Research Context**: Analyze how thematic characteristics of modern romance novels relate to reader engagement/popularity using Goodreads metadata.\n",
    "\n",
    "**Dataset**: `final_books_2000_2020_en_20250901_024106.csv` (119,678 romance novels)\n",
    "\n",
    "## Analysis Plan\n",
    "1. **Dataset Overview** - Basic structure, data types, missing values\n",
    "2. **Title Analysis** - Series patterns, numbering, cleaning opportunities\n",
    "3. **Author Name Analysis** - Duplicates, variations, normalization needs\n",
    "4. **Description Text Analysis** - Text quality, HTML artifacts, length distributions\n",
    "5. **Series Pattern Analysis** - Series titles and book title relationships\n",
    "6. **Publication & Popularity Analysis** - Temporal trends and engagement metrics\n",
    "7. **Subgenre Signal Analysis** - Popular shelves and genre classification\n",
    "8. **Cleaning Recommendations** - Specific suggestions with code examples\n",
    "\n",
    "## Expected Outputs\n",
    "- Data quality assessment\n",
    "- Title and series cleaning patterns\n",
    "- Author name normalization strategies\n",
    "- Text preprocessing recommendations\n",
    "- Final dataset preparation for NLP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import warnings\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the final processed dataset\n",
    "dataset_path = \"../data/processed/final_books_2000_2020_en_20250901_024106.csv\"\n",
    "print(f\"üìö Loading dataset from: {dataset_path}\")\n",
    "\n",
    "# Load with progress indicator\n",
    "df = pd.read_csv(dataset_path)\n",
    "print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"üìä Shape: {df.shape}\")\n",
    "print(f\"üìã Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DATASET OVERVIEW\n",
    "print(\"üîç DATASET OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic info\n",
    "print(f\"üìö Total records: {len(df):,}\")\n",
    "print(f\"üìã Total columns: {len(df.columns)}\")\n",
    "print(f\"üìÖ Publication year range: {df['publication_year'].min()} - {df['publication_year'].max()}\")\n",
    "\n",
    "# Data types\n",
    "print(\"\\nüìä Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Memory usage\n",
    "memory_usage = df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"\\nüíæ Memory usage: {memory_usage:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values analysis\n",
    "print(\"üîç MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_data.index,\n",
    "    'Missing_Count': missing_data.values,\n",
    "    'Missing_Percent': missing_percent.values\n",
    "})\n",
    "\n",
    "missing_df = missing_df.sort_values('Missing_Percent', ascending=False)\n",
    "print(missing_df)\n",
    "\n",
    "# Visualize missing values\n",
    "plt.figure(figsize=(12, 8))\n",
    "missing_df.plot(x='Column', y='Missing_Percent', kind='bar', ax=plt.gca())\n",
    "plt.title('Missing Values by Column (%)')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Missing Values (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. TITLE ANALYSIS\n",
    "print(\"üîç TITLE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic title statistics\n",
    "df['title_length'] = df['title'].str.len()\n",
    "df['title_word_count'] = df['title'].str.split().str.len()\n",
    "\n",
    "print(f\"üìñ Title length statistics:\")\n",
    "print(f\"   - Mean length: {df['title_length'].mean():.1f} characters\")\n",
    "print(f\"   - Median length: {df['title_length'].median():.1f} characters\")\n",
    "print(f\"   - Min length: {df['title_length'].min()} characters\")\n",
    "print(f\"   - Max length: {df['title_length'].max()} characters\")\n",
    "\n",
    "print(f\"\\nüìù Title word count statistics:\")\n",
    "print(f\"   - Mean words: {df['title_word_count'].mean():.1f} words\")\n",
    "print(f\"   - Median words: {df['title_word_count'].median():.1f} words\")\n",
    "print(f\"   - Min words: {df['title_word_count'].min()} words\")\n",
    "print(f\"   - Max words: {df['title_word_count'].max()} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title length distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Character length distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['title_length'], bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.title('Title Length Distribution (Characters)')\n",
    "plt.xlabel('Title Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(df['title_length'].median(), color='red', linestyle='--', label=f'Median: {df[\"title_length\"].median():.0f}')\n",
    "plt.legend()\n",
    "\n",
    "# Word count distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df['title_word_count'], bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.title('Title Word Count Distribution')\n",
    "plt.xlabel('Title Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(df['title_word_count'].median(), color='red', linestyle='--', label=f'Median: {df[\"title_word_count\"].median():.0f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for series patterns in titles\n",
    "print(\"üîç SERIES PATTERNS IN TITLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Common series indicators\n",
    "series_patterns = [\n",
    "    r'\\b(\\d+)\\s*[:\\-]\\s*',  # Number followed by : or -\n",
    "    r'\\b(Book|Volume|Part)\\s+(\\d+)\\b',  # Book 1, Volume 2, etc.\n",
    "    r'\\b(\\d+)\\s*(?:st|nd|rd|th)\\s*',  # 1st, 2nd, 3rd, etc.\n",
    "    r'\\b(\\d+)\\s*$',  # Number at end\n",
    "    r'\\b(\\d+)\\s*\\('  # Number followed by parenthesis\n",
    "]\n",
    "\n",
    "pattern_names = ['Number:Colon', 'Book/Volume/Part', 'Ordinal', 'End Number', 'Number(']\n",
    "\n",
    "for pattern, name in zip(series_patterns, pattern_names):\n",
    "    matches = df['title'].str.contains(pattern, regex=True, na=False)\n",
    "    count = matches.sum()\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"{name}: {count:,} titles ({percentage:.1f}%)\")\n",
    "\n",
    "# Show examples of titles with series patterns\n",
    "print(\"\\nüìö Examples of titles with series patterns:\")\n",
    "for pattern, name in zip(series_patterns, pattern_names):\n",
    "    matches = df[df['title'].str.contains(pattern, regex=True, na=False)]\n",
    "    if not matches.empty:\n",
    "        print(f\"\\n{name} examples:\")\n",
    "        for title in matches['title'].head(3):\n",
    "            print(f\"  - {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. AUTHOR NAME ANALYSIS\n",
    "print(\"üîç AUTHOR NAME ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic author statistics\n",
    "print(f\"üë§ Total unique authors: {df['author_id'].nunique():,}\")\n",
    "print(f\"üìö Books per author (mean): {len(df) / df['author_id'].nunique():.1f}\")\n",
    "print(f\"üìö Books per author (median): {df.groupby('author_id').size().median():.1f}\")\n",
    "\n",
    "# Author name length analysis\n",
    "df['author_name_length'] = df['author_name'].str.len()\n",
    "df['author_name_word_count'] = df['author_name'].str.split().str.len()\n",
    "\n",
    "print(f\"\\nüìù Author name statistics:\")\n",
    "print(f\"   - Mean name length: {df['author_name_length'].mean():.1f} characters\")\n",
    "print(f\"   - Median name length: {df['author_name_length'].median():.1f} characters\")\n",
    "print(f\"   - Mean word count: {df['author_name_word_count'].mean():.1f} words\")\n",
    "print(f\"   - Median word count: {df['author_name_word_count'].median():.1f} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for potential author name duplicates/variations\n",
    "print(\"üîç AUTHOR NAME VARIATIONS ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check for authors with multiple name variations\n",
    "author_name_counts = df.groupby('author_id')['author_name'].nunique()\n",
    "multiple_names = author_name_counts[author_name_counts > 1]\n",
    "\n",
    "print(f\"üë§ Authors with multiple name variations: {len(multiple_names):,}\")\n",
    "if not multiple_names.empty:\n",
    "    print(f\"\\nüìö Examples of authors with multiple names:\")\n",
    "    for author_id in multiple_names.head(5).index:\n",
    "        names = df[df['author_id'] == author_id]['author_name'].unique()\n",
    "        print(f\"  Author ID {author_id}: {names}\")\n",
    "\n",
    "# Check for potential duplicate authors (same name, different ID)\n",
    "author_name_to_ids = defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    author_name_to_ids[row['author_name']].append(row['author_id'])\n",
    "\n",
    "duplicate_names = {name: ids for name, ids in author_name_to_ids.items() if len(ids) > 1}\n",
    "print(f\"\\n‚ö†Ô∏è  Potential duplicate author names: {len(duplicate_names):,}\")\n",
    "\n",
    "if duplicate_names:\n",
    "    print(f\"\\nüìö Examples of potential duplicate names:\")\n",
    "    for name, ids in list(duplicate_names.items())[:5]:\n",
    "        print(f\"  '{name}': {ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. DESCRIPTION TEXT ANALYSIS\n",
    "print(\"üîç DESCRIPTION TEXT ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic description statistics\n",
    "df['description_length'] = df['description'].str.len()\n",
    "df['description_word_count'] = df['description'].str.split().str.len()\n",
    "\n",
    "print(f\"üìñ Description statistics:\")\n",
    "print(f\"   - Mean length: {df['description_length'].mean():.1f} characters\")\n",
    "print(f\"   - Median length: {df['description_length'].median():.1f} characters\")\n",
    "print(f\"   - Min length: {df['description_length'].min()} characters\")\n",
    "print(f\"   - Max length: {df['description_length'].max()} characters\")\n",
    "print(f\"   - Mean words: {df['description_word_count'].mean():.1f} words\")\n",
    "print(f\"   - Median words: {df['description_word_count'].median():.1f} words\")\n",
    "\n",
    "# Check for missing descriptions\n",
    "missing_descriptions = df['description'].isnull().sum()\n",
    "print(f\"\\n‚ùå Missing descriptions: {missing_descriptions:,} ({missing_descriptions/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check for very short descriptions (potential data quality issues)\n",
    "short_descriptions = (df['description_length'] < 50).sum()\n",
    "print(f\"üìù Very short descriptions (<50 chars): {short_descriptions:,} ({short_descriptions/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description length distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Character length distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['description_length'].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.title('Description Length Distribution (Characters)')\n",
    "plt.xlabel('Description Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(df['description_length'].median(), color='red', linestyle='--', label=f'Median: {df[\"description_length\"].median():.0f}')\n",
    "plt.legend()\n",
    "\n",
    "# Word count distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df['description_word_count'].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.title('Description Word Count Distribution')\n",
    "plt.xlabel('Description Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(df['description_word_count'].median(), color='red', linestyle='--', label=f'Median: {df[\"description_word_count\"].median():.0f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for HTML artifacts and special characters in descriptions\n",
    "print(\"üîç HTML AND SPECIAL CHARACTERS IN DESCRIPTIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Common HTML patterns\n",
    "html_patterns = [\n",
    "    r'<[^>]+>',  # HTML tags\n",
    "    r'&[a-zA-Z]+;',  # HTML entities\n",
    "    r'\\s+',  # Multiple whitespace\n",
    "    r'[\\r\\n\\t]+',  # Line breaks and tabs\n",
    "    r'[\\u00A0-\\uFFFF]',  # Non-ASCII characters\n",
    "]\n",
    "\n",
    "pattern_names = ['HTML Tags', 'HTML Entities', 'Multiple Whitespace', 'Line Breaks/Tabs', 'Non-ASCII']\n",
    "\n",
    "for pattern, name in zip(html_patterns, pattern_names):\n",
    "    matches = df['description'].str.contains(pattern, regex=True, na=False)\n",
    "    count = matches.sum()\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"{name}: {count:,} descriptions ({percentage:.1f}%)\")\n",
    "\n",
    "# Show examples of descriptions with HTML\n",
    "html_descriptions = df[df['description'].str.contains(r'<[^>]+>', regex=True, na=False)]\n",
    "if not html_descriptions.empty:\n",
    "    print(f\"\\nüìö Examples of descriptions with HTML:\")\n",
    "    for desc in html_descriptions['description'].head(3):\n",
    "        print(f\"  - {desc[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. SERIES PATTERN ANALYSIS\n",
    "print(\"üîç SERIES PATTERN ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Series coverage\n",
    "series_coverage = df['series_id'].notna().sum()\n",
    "print(f\"üìö Books in series: {series_coverage:,} ({series_coverage/len(df)*100:.1f}%)\")\n",
    "print(f\"üìö Books not in series: {(~df['series_id'].notna()).sum():,} ({(~df['series_id'].notna()).sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Series size distribution\n",
    "series_sizes = df.groupby('series_id')['series_works_count'].first().value_counts().sort_index()\n",
    "print(f\"\\nüìä Series size distribution:\")\n",
    "for size, count in series_sizes.head(10).items():\n",
    "    print(f\"  {size} books: {count:,} series\")\n",
    "\n",
    "# Check relationship between series titles and book titles\n",
    "print(f\"\\nüîç SERIES TITLE VS BOOK TITLE RELATIONSHIP\")\n",
    "series_books = df[df['series_id'].notna()].copy()\n",
    "series_books['title_contains_series'] = series_books.apply(\n",
    "    lambda row: row['series_title'].lower() in row['title'].lower() if pd.notna(row['series_title']) else False, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "contains_series = series_books['title_contains_series'].sum()\n",
    "print(f\"üìö Books with series titles embedded in book titles: {contains_series:,} ({contains_series/len(series_books)*100:.1f}%)\")\n",
    "\n",
    "# Show examples\n",
    "if not series_books.empty:\n",
    "    examples = series_books[series_books['title_contains_series']].head(5)\n",
    "    print(f\"\\nüìö Examples of books with embedded series titles:\")\n",
    "    for _, row in examples.iterrows():\n",
    "        print(f\"  Series: '{row['series_title']}' | Book: '{row['title']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. PUBLICATION & POPULARITY ANALYSIS\n",
    "print(\"üîç PUBLICATION & POPULARITY ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Publication year distribution\n",
    "year_counts = df['publication_year'].value_counts().sort_index()\n",
    "print(f\"üìÖ Publication year distribution:\")\n",
    "print(f\"   - Range: {df['publication_year'].min()} - {df['publication_year'].max()}\")\n",
    "print(f\"   - Most common year: {year_counts.idxmax()} ({year_counts.max():,} books)\")\n",
    "print(f\"   - Least common year: {year_counts.idxmin()} ({year_counts.min():,} books)\")\n",
    "\n",
    "# Popularity metrics\n",
    "print(f\"\\n‚≠ê Popularity metrics:\")\n",
    "print(f\"   - Average rating (mean): {df['average_rating_weighted_mean'].mean():.2f}\")\n",
    "print(f\"   - Average rating (median): {df['average_rating_weighted_mean'].median():.2f}\")\n",
    "print(f\"   - Ratings count (mean): {df['ratings_count_sum'].mean():,.0f}\")\n",
    "print(f\"   - Ratings count (median): {df['ratings_count_sum'].median():,.0f}\")\n",
    "print(f\"   - Reviews count (mean): {df['text_reviews_count_sum'].mean():,.0f}\")\n",
    "print(f\"   - Reviews count (median): {df['text_reviews_count_sum'].median():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publication trends over time\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Publication volume over time\n",
    "plt.subplot(2, 2, 1)\n",
    "year_counts.plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Publication Volume by Year')\n",
    "plt.xlabel('Publication Year')\n",
    "plt.ylabel('Number of Books')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Average rating over time\n",
    "plt.subplot(2, 2, 2)\n",
    "yearly_ratings = df.groupby('publication_year')['average_rating_weighted_mean'].mean()\n",
    "yearly_ratings.plot(kind='line', marker='o', ax=plt.gca())\n",
    "plt.title('Average Rating by Publication Year')\n",
    "plt.xlabel('Publication Year')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Ratings count over time\n",
    "plt.subplot(2, 2, 3)\n",
    "yearly_ratings_count = df.groupby('publication_year')['ratings_count_sum'].mean()\n",
    "yearly_ratings_count.plot(kind='line', marker='o', ax=plt.gca())\n",
    "plt.title('Average Ratings Count by Publication Year')\n",
    "plt.xlabel('Publication Year')\n",
    "plt.ylabel('Average Ratings Count')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Reviews count over time\n",
    "plt.subplot(2, 2, 4)\n",
    "yearly_reviews_count = df.groupby('publication_year')['text_reviews_count_sum'].mean()\n",
    "yearly_reviews_count.plot(kind='line', marker='o', ax=plt.gca())\n",
    "plt.title('Average Reviews Count by Publication Year')\n",
    "plt.xlabel('Publication Year')\n",
    "plt.ylabel('Average Reviews Count')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. SUBGENRE SIGNAL ANALYSIS\n",
    "print(\"üîç SUBGENRE SIGNAL ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analyze popular shelves for subgenre signals\n",
    "print(\"üìö Popular shelves analysis:\")\n",
    "\n",
    "# Sample some popular shelves to understand structure\n",
    "sample_shelves = df['popular_shelves'].dropna().head(10)\n",
    "print(f\"\\nüìã Sample popular shelves:\")\n",
    "for i, shelves in enumerate(sample_shelves):\n",
    "    try:\n",
    "        shelves_list = json.loads(shelves)\n",
    "        print(f\"  {i+1}. {shelves_list[:5]}...\")  # Show first 5 shelves\n",
    "    except:\n",
    "        print(f\"  {i+1}. {shelves[:100]}...\")  # Show first 100 chars if not JSON\n",
    "\n",
    "# Check if popular_shelves is JSON format\n",
    "json_format_count = 0\n",
    "for shelves in df['popular_shelves'].dropna():\n",
    "    try:\n",
    "        json.loads(shelves)\n",
    "        json_format_count += 1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"\\nüìä Popular shelves format:\")\n",
    "print(f\"   - JSON format: {json_format_count:,} ({json_format_count/len(df['popular_shelves'].dropna())*100:.1f}%)\")\n",
    "print(f\"   - Non-JSON format: {len(df['popular_shelves'].dropna()) - json_format_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and analyze subgenre signals from popular shelves\n",
    "print(\"üîç SUBGENRE EXTRACTION FROM POPULAR SHELVES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Target subgenres for research\n",
    "target_subgenres = [\n",
    "    'contemporary romance', 'historical romance', 'paranormal romance',\n",
    "    'romantic suspense', 'romantic fantasy', 'science fiction romance'\n",
    "]\n",
    "\n",
    "# Extract subgenre signals\n",
    "subgenre_counts = defaultdict(int)\n",
    "subgenre_examples = defaultdict(list)\n",
    "\n",
    "for shelves in df['popular_shelves'].dropna():\n",
    "    try:\n",
    "        shelves_list = json.loads(shelves)\n",
    "        for shelf in shelves_list:\n",
    "            shelf_lower = shelf.lower()\n",
    "            for subgenre in target_subgenres:\n",
    "                if subgenre in shelf_lower:\n",
    "                    subgenre_counts[subgenre] += 1\n",
    "                    # Store example book title\n",
    "                    if len(subgenre_examples[subgenre]) < 3:\n",
    "                        book_idx = df[df['popular_shelves'] == shelves].index[0]\n",
    "                        subgenre_examples[subgenre].append(df.loc[book_idx, 'title'])\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(f\"üìä Subgenre signals found:\")\n",
    "for subgenre, count in sorted(subgenre_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"   - {subgenre}: {count:,} books ({percentage:.1f}%)\")\n",
    "    if subgenre_examples[subgenre]:\n",
    "        print(f\"     Examples: {', '.join(subgenre_examples[subgenre])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. CLEANING RECOMMENDATIONS\n",
    "print(\"üîç CLEANING RECOMMENDATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"üìã SUMMARY OF FINDINGS:\")\n",
    "print(f\"   - Dataset size: {len(df):,} romance novels\")\n",
    "print(f\"   - Publication range: {df['publication_year'].min()} - {df['publication_year'].max()}\")\n",
    "print(f\"   - Series coverage: {df['series_id'].notna().sum():,} books ({df['series_id'].notna().sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"   - Missing descriptions: {df['description'].isnull().sum():,} ({df['description'].isnull().sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"   - HTML artifacts: {df['description'].str.contains(r'<[^>]+>', regex=True, na=False).sum():,} descriptions\")\n",
    "\n",
    "print(\"\\nüßπ RECOMMENDED CLEANING STEPS:\")\n",
    "print(\"\\n1. TITLE CLEANING:\")\n",
    "print(\"   - Extract series numbers and prefixes\")\n",
    "print(\"   - Remove series titles embedded in book titles\")\n",
    "print(\"   - Standardize numbering formats\")\n",
    "\n",
    "print(\"\\n2. AUTHOR NAME NORMALIZATION:\")\n",
    "print(\"   - Resolve duplicate author names with different IDs\")\n",
    "print(\"   - Standardize name formats\")\n",
    "print(\"   - Handle pen names and variations\")\n",
    "\n",
    "print(\"\\n3. DESCRIPTION TEXT CLEANING:\")\n",
    "print(\"   - Remove HTML tags and entities\")\n",
    "print(\"   - Clean special characters and whitespace\")\n",
    "print(\"   - Standardize line breaks and formatting\")\n",
    "print(\"   - Handle missing descriptions\")\n",
    "\n",
    "print(\"\\n4. SERIES HANDLING:\")\n",
    "print(\"   - Extract series information consistently\")\n",
    "print(\"   - Create clean series titles\")\n",
    "print(\"   - Handle series numbering\")\n",
    "\n",
    "print(\"\\n5. SUBGENRE CLASSIFICATION:\")\n",
    "print(\"   - Parse popular shelves for subgenre signals\")\n",
    "print(\"   - Create standardized subgenre categories\")\n",
    "print(\"   - Handle overlapping subgenres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE CLEANING FUNCTIONS\n",
    "print(\"üîß SAMPLE CLEANING FUNCTIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def clean_title(title, series_title=None):\n",
    "    \"\"\"Clean book title by removing series information.\"\"\"\n",
    "    if pd.isna(title) or pd.isna(series_title):\n",
    "        return title\n",
    "    \n",
    "    # Remove series title from book title if present\n",
    "    if series_title and series_title.lower() in title.lower():\n",
    "        # Try to remove series title and clean up\n",
    "        cleaned = title.replace(series_title, '').strip()\n",
    "        # Remove common separators\n",
    "        cleaned = re.sub(r'^[\\s\\-:]+|[\\s\\-:]+$', '', cleaned)\n",
    "        return cleaned if cleaned else title\n",
    "    \n",
    "    return title\n",
    "\n",
    "def clean_description(description):\n",
    "    \"\"\"Clean book description by removing HTML and normalizing text.\"\"\"\n",
    "    if pd.isna(description):\n",
    "        return description\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    cleaned = re.sub(r'<[^>]+>', '', description)\n",
    "    # Remove HTML entities\n",
    "    cleaned = re.sub(r'&[a-zA-Z]+;', ' ', cleaned)\n",
    "    # Normalize whitespace\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    # Remove line breaks and tabs\n",
    "    cleaned = re.sub(r'[\\r\\n\\t]+', ' ', cleaned)\n",
    "    # Clean up\n",
    "    cleaned = cleaned.strip()\n",
    "    \n",
    "    return cleaned if cleaned else description\n",
    "\n",
    "def extract_series_number(title):\n",
    "    \"\"\"Extract series number from title.\"\"\"\n",
    "    if pd.isna(title):\n",
    "        return None\n",
    "    \n",
    "    # Common patterns\n",
    "    patterns = [\n",
    "        r'\\b(\\d+)\\s*[:\\-]\\s*',  # Number followed by : or -\n",
    "        r'\\b(Book|Volume|Part)\\s+(\\d+)\\b',  # Book 1, Volume 2, etc.\n",
    "        r'\\b(\\d+)\\s*(?:st|nd|rd|th)\\s*',  # 1st, 2nd, 3rd, etc.\n",
    "        r'\\b(\\d+)\\s*\\('  # Number followed by parenthesis\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, title)\n",
    "        if match:\n",
    "            return int(match.group(1) if len(match.groups()) > 1 else match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"‚úÖ Sample cleaning functions defined:\")\n",
    "print(\"   - clean_title(): Remove series information from titles\")\n",
    "print(\"   - clean_description(): Remove HTML and normalize text\")\n",
    "print(\"   - extract_series_number(): Extract series numbers from titles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CLEANING FUNCTIONS ON SAMPLE DATA\n",
    "print(\"üß™ TESTING CLEANING FUNCTIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test on sample data\n",
    "sample_data = df[['title', 'series_title', 'description']].head(5)\n",
    "print(\"üìö Sample data before cleaning:\")\n",
    "print(sample_data)\n",
    "\n",
    "print(\"\\nüßπ After cleaning:\")\n",
    "for idx, row in sample_data.iterrows():\n",
    "    print(f\"\\nBook {idx}:\")\n",
    "    print(f\"  Original title: {row['title']}\")\n",
    "    print(f\"  Cleaned title: {clean_title(row['title'], row['series_title'])}\")\n",
    "    print(f\"  Series number: {extract_series_number(row['title'])}\")\n",
    "    if pd.notna(row['description']):\n",
    "        desc_preview = row['description'][:100] + \"...\" if len(row['description']) > 100 else row['description']\n",
    "        cleaned_desc = clean_description(row['description'])\n",
    "        cleaned_preview = cleaned_desc[:100] + \"...\" if len(cleaned_desc) > 100 else cleaned_desc\n",
    "        print(f\"  Description preview: {desc_preview}\")\n",
    "        print(f\"  Cleaned description: {cleaned_preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### Key Findings\n",
    "- **Dataset Quality**: 119,678 romance novels with good coverage of authors and series\n",
    "- **Title Issues**: Series information embedded in titles, numbering patterns\n",
    "- **Description Quality**: HTML artifacts, special characters, varying lengths\n",
    "- **Author Variations**: Potential duplicates and name variations\n",
    "- **Series Coverage**: 67% of books are in series with embedded title patterns\n",
    "\n",
    "### Recommended Cleaning Steps\n",
    "1. **Title Normalization**: Extract series numbers, remove embedded series titles\n",
    "2. **Text Cleaning**: Remove HTML, normalize whitespace, handle special characters\n",
    "3. **Author Deduplication**: Resolve name variations and duplicates\n",
    "4. **Series Standardization**: Consistent series title and numbering extraction\n",
    "5. **Subgenre Classification**: Parse popular shelves for standardized categories\n",
    "\n",
    "### Next Phase\n",
    "After implementing cleaning steps, proceed to:\n",
    "- Text preprocessing for NLP analysis\n",
    "- Topic modeling on cleaned descriptions\n",
    "- Correlation analysis between themes and popularity metrics\n",
    "\n",
    "### Files to Update\n",
    "- Create cleaning pipeline script\n",
    "- Update data dictionary\n",
    "- Document cleaning decisions and rationale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
