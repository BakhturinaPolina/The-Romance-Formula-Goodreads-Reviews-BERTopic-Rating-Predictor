{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01: Explore Goodreads Romance JSON Files\n",
    "\n",
    "**Objective**: Systematic exploration of romance-related Goodreads JSON files to understand data structure, quality, and content before analysis.\n",
    "\n",
    "**Research Context**: Analyze how thematic characteristics of modern romance novels relate to reader engagement/popularity using Goodreads metadata.\n",
    "\n",
    "**Target Subgenres**: contemporary, historical, paranormal, romantic suspense, romantic fantasy, science-fiction romance\n",
    "\n",
    "**Engagement Metrics**: ratings, ratings_count, reviews_count, review text signals, shelf/status signals (\"to-read\", \"currently-reading\")\n",
    "\n",
    "## Analysis Plan\n",
    "1. **Pre-flight Manifest Scan** - Detect file presence, sizes, compression status\n",
    "2. **Schema Inventory** - Field names, examples, dtypes, null percentages\n",
    "3. **Key Coverage Analysis** - book_id, work_id, author_id mapping\n",
    "4. **Subgenre Signal Discovery** - Popular shelves, genre tags, classification signals\n",
    "5. **Engagement Metrics Feasibility** - Rating distributions, review counts, shelf signals\n",
    "6. **Final Column Contract Draft** - Define target schema for analysis\n",
    "7. **Quality Filters Dry-Run** - Test filtering criteria on samples\n",
    "8. **Synthesis** - Summary and next steps\n",
    "\n",
    "## Expected Outputs\n",
    "- File manifest with sizes and record estimates\n",
    "- Schema inventory per file with field types and quality metrics\n",
    "- Key field coverage analysis\n",
    "- Subgenre classification feasibility assessment\n",
    "- Engagement metrics availability report\n",
    "- Final column contract for analysis dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Project root detected: /home/polina/Documents/goodreads_romance_research_cursor/romance-novel-nlp-research\n",
      "‚úÖ Data raw directory: /home/polina/Documents/goodreads_romance_research_cursor/romance-novel-nlp-research/data/raw\n",
      "‚úÖ Data raw exists: True\n",
      "=== PRE-FLIGHT MANIFEST SCAN ===\n",
      "Run ID: 20250820_001037\n",
      "Start Time: 2025-08-20 00:10:37.839626\n",
      "Project Root: /home/polina/Documents/goodreads_romance_research_cursor/romance-novel-nlp-research\n",
      "Data Raw Directory: /home/polina/Documents/goodreads_romance_research_cursor/romance-novel-nlp-research/data/raw\n",
      "Python Version: 3.9.18 (main, Jan 29 2025, 16:13:49) \n",
      "[GCC 13.3.0]\n",
      "Platform: linux\n",
      "Scanning 9 expected files...\n",
      "============================================================\n",
      "‚úÖ goodreads_books_romance.json.gz\n",
      "   Size: 347.88 MB\n",
      "   Compression: gzip\n",
      "   Estimated Records: 335,449\n",
      "\n",
      "‚úÖ goodreads_interactions_romance.json.gz\n",
      "   Size: 2186.71 MB\n",
      "   Compression: gzip\n",
      "   Estimated Records: 42,792,856\n",
      "\n",
      "‚úÖ goodreads_reviews_romance.json.gz\n",
      "   Size: 1240.85 MB\n",
      "   Compression: gzip\n",
      "   Estimated Records: 3,565,378\n",
      "\n",
      "‚úÖ goodreads_book_authors.json.gz\n",
      "   Size: 17.16 MB\n",
      "   Compression: gzip\n",
      "   Estimated Records: 829,529\n",
      "\n",
      "‚úÖ goodreads_book_works.json.gz\n",
      "   Size: 71.90 MB\n",
      "   Compression: gzip\n",
      "   Estimated Records: 1,521,962\n",
      "\n",
      "‚úÖ goodreads_book_genres_initial.json.gz\n",
      "   Size: 23.12 MB\n",
      "   Compression: gzip\n",
      "   Estimated Records: 2,360,655\n",
      "\n",
      "‚úÖ goodreads_reviews_dedup.json.gz\n",
      "   Size: 5096.12 MB\n",
      "   Compression: gzip\n",
      "   Estimated Records: 15,739,967\n",
      "\n",
      "‚úÖ goodreads_reviews_spoiler.json.gz\n",
      "   Size: 591.47 MB\n",
      "   Compression: gzip\n",
      "   Estimated Records: 1,378,033\n",
      "\n",
      "‚úÖ goodreads_book_series.json.gz\n",
      "   Size: 27.04 MB\n",
      "   Compression: gzip\n",
      "   Estimated Records: 400,390\n",
      "\n",
      "============================================================\n",
      "SCAN SUMMARY\n",
      "Files Found: 9/9\n",
      "Files Missing: 0\n",
      "Total Size: 9602.25 MB\n",
      "Total Records (Estimate): 68,924,219\n",
      "Duration: 458.79 seconds\n",
      "End Time: 2025-08-20 00:18:16.624761\n",
      "üìÅ OUTPUTS SAVED:\n",
      "   Full Log: /home/polina/Documents/goodreads_romance_research_cursor/romance-novel-nlp-research/logs/exploration/full/20250820_001037_preflight_manifest.log\n",
      "   Summary Log: /home/polina/Documents/goodreads_romance_research_cursor/romance-novel-nlp-research/logs/exploration/summary/20250820_001037_preflight_summary.log\n",
      "   JSON Manifest: /home/polina/Documents/goodreads_romance_research_cursor/romance-novel-nlp-research/data/intermediate/manifests/manifest_20250820_001037.json\n",
      "=== STEP 1 COMPLETE ===\n",
      "End Time: 2025-08-20 00:18:16.624761\n",
      "Files Processed: 9\n",
      "Files Found: 9\n",
      "Files Missing: 0\n",
      "Total Data Size: 9602.25 MB\n",
      "Estimated Total Records: 68,924,219\n",
      "Outputs: 3 files saved\n"
     ]
    }
   ],
   "source": [
    "# PLAN: Pre-flight manifest scan - detect file presence, sizes, compression, record estimates\n",
    "# FILES: Read from data/raw/, write to logs/exploration/ and data/intermediate/manifests/\n",
    "# EXPECTED: File manifest with sizes, compression status, estimated records, missing files\n",
    "# TIMESTAMP: 2024-01-XX HH:MM:SS\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "# FIXED: Proper project root detection - go up from notebooks/ to project root\n",
    "notebook_dir = Path().absolute()\n",
    "project_root = notebook_dir.parent  # Go up one level from notebooks/ to project root\n",
    "\n",
    "# Validate we're in the right place\n",
    "if not (project_root / \"data\" / \"raw\").exists():\n",
    "    # Try alternative detection if we're already in project root\n",
    "    if (notebook_dir / \"data\" / \"raw\").exists():\n",
    "        project_root = notebook_dir\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Cannot find data/raw/ directory. Current: {notebook_dir}, Project root: {project_root}\")\n",
    "\n",
    "print(f\"‚úÖ Project root detected: {project_root}\")\n",
    "print(f\"‚úÖ Data raw directory: {project_root / 'data' / 'raw'}\")\n",
    "print(f\"‚úÖ Data raw exists: {(project_root / 'data' / 'raw').exists()}\")\n",
    "\n",
    "# Add project root to path for imports\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Create required directories\n",
    "log_full_dir = project_root / \"logs\" / \"exploration\" / \"full\"\n",
    "log_summary_dir = project_root / \"logs\" / \"exploration\" / \"summary\"\n",
    "manifest_dir = project_root / \"data\" / \"intermediate\" / \"manifests\"\n",
    "\n",
    "for dir_path in [log_full_dir, log_summary_dir, manifest_dir]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generate run ID\n",
    "run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Expected files (based on actual data/raw contents)\n",
    "expected_files = [\n",
    "    \"goodreads_books_romance.json.gz\",\n",
    "    \"goodreads_interactions_romance.json.gz\",\n",
    "    \"goodreads_reviews_romance.json.gz\",\n",
    "    \"goodreads_book_authors.json.gz\",\n",
    "    \"goodreads_book_works.json.gz\",\n",
    "    \"goodreads_book_genres_initial.json.gz\",\n",
    "    \"goodreads_reviews_dedup.json.gz\",\n",
    "    \"goodreads_reviews_spoiler.json.gz\",\n",
    "    \"goodreads_book_series.json.gz\"\n",
    "]\n",
    "\n",
    "data_raw_dir = project_root / \"data\" / \"raw\"\n",
    "manifest_data = {\n",
    "    \"run_id\": run_id,\n",
    "    \"timestamp\": start_time.isoformat(),\n",
    "    \"environment\": {\n",
    "        \"python_version\": sys.version,\n",
    "        \"platform\": sys.platform,\n",
    "        \"project_root\": str(project_root),\n",
    "        \"notebook_dir\": str(notebook_dir),\n",
    "        \"data_raw_dir\": str(data_raw_dir)\n",
    "    },\n",
    "    \"files\": {}\n",
    "}\n",
    "\n",
    "print(f\"=== PRE-FLIGHT MANIFEST SCAN ===\")\n",
    "print(f\"Run ID: {run_id}\")\n",
    "print(f\"Start Time: {start_time}\")\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"Data Raw Directory: {data_raw_dir}\")\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"Platform: {sys.platform}\")\n",
    "print(f\"Scanning {len(expected_files)} expected files...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Scan each expected file\n",
    "for filename in expected_files:\n",
    "    file_path = data_raw_dir / filename\n",
    "    \n",
    "    if file_path.exists():\n",
    "        # Get file size\n",
    "        file_size = file_path.stat().st_size\n",
    "        file_size_mb = file_size / (1024 * 1024)\n",
    "        \n",
    "        # Estimate record count by sampling first 1000 lines\n",
    "        record_count_estimate = None\n",
    "        try:\n",
    "            with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "                lines = []\n",
    "                for i, line in enumerate(f):\n",
    "                    if i >= 1000:  # Sample first 1000 lines\n",
    "                        break\n",
    "                    lines.append(line.strip())\n",
    "                \n",
    "                # Count valid JSON lines\n",
    "                valid_lines = 0\n",
    "                for line in lines:\n",
    "                    if line.strip() and line.strip() != '[' and line.strip() != ']':\n",
    "                        try:\n",
    "                            json.loads(line.rstrip(','))\n",
    "                            valid_lines += 1\n",
    "                        except json.JSONDecodeError:\n",
    "                            continue\n",
    "                \n",
    "                # Estimate total records\n",
    "                if valid_lines > 0:\n",
    "                    # Get total file size and estimate total lines\n",
    "                    with gzip.open(file_path, 'rt', encoding='utf-8') as f2:\n",
    "                        total_lines = sum(1 for _ in f2)\n",
    "                    \n",
    "                    # Estimate records based on valid JSON ratio\n",
    "                    valid_ratio = valid_lines / len(lines)\n",
    "                    record_count_estimate = int(total_lines * valid_ratio)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Could not estimate records for {filename}: {e}\")\n",
    "        \n",
    "        # Store file info\n",
    "        file_info = {\n",
    "            \"status\": \"found\",\n",
    "            \"size_bytes\": file_size,\n",
    "            \"size_mb\": round(file_size_mb, 2),\n",
    "            \"compression\": \"gzip\",\n",
    "            \"record_count_estimate\": record_count_estimate,\n",
    "            \"sampling_method\": \"first_1000_lines\"\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ {filename}\")\n",
    "        print(f\"   Size: {file_size_mb:.2f} MB\")\n",
    "        print(f\"   Compression: gzip\")\n",
    "        if record_count_estimate:\n",
    "            print(f\"   Estimated Records: {record_count_estimate:,}\")\n",
    "        else:\n",
    "            print(f\"   Estimated Records: Could not determine\")\n",
    "        \n",
    "    else:\n",
    "        file_info = {\n",
    "            \"status\": \"missing\",\n",
    "            \"error\": \"File not found\"\n",
    "        }\n",
    "        print(f\"‚ùå {filename} - FILE NOT FOUND\")\n",
    "    \n",
    "    manifest_data[\"files\"][filename] = file_info\n",
    "    print()\n",
    "\n",
    "# Calculate summary statistics\n",
    "found_files = [f for f, info in manifest_data[\"files\"].items() if info[\"status\"] == \"found\"]\n",
    "missing_files = [f for f, info in manifest_data[\"files\"].items() if info[\"status\"] == \"missing\"]\n",
    "total_size_mb = sum(info[\"size_mb\"] for info in manifest_data[\"files\"].values() if info[\"status\"] == \"found\")\n",
    "total_records = sum(info.get(\"record_count_estimate\", 0) for info in manifest_data[\"files\"].values() if info[\"status\"] == \"found\")\n",
    "\n",
    "manifest_data[\"summary\"] = {\n",
    "    \"files_found\": len(found_files),\n",
    "    \"files_missing\": len(missing_files),\n",
    "    \"total_size_mb\": round(total_size_mb, 2),\n",
    "    \"total_records_estimate\": total_records,\n",
    "    \"missing_files\": missing_files\n",
    "}\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = end_time - start_time\n",
    "manifest_data[\"duration_seconds\"] = duration.total_seconds()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"SCAN SUMMARY\")\n",
    "print(f\"Files Found: {len(found_files)}/{len(expected_files)}\")\n",
    "print(f\"Files Missing: {len(missing_files)}\")\n",
    "print(f\"Total Size: {total_size_mb:.2f} MB\")\n",
    "print(f\"Total Records (Estimate): {total_records:,}\")\n",
    "print(f\"Duration: {duration.total_seconds():.2f} seconds\")\n",
    "print(f\"End Time: {end_time}\")\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"MISSING FILES (will continue with available files):\")\n",
    "    for file in missing_files:\n",
    "        print(f\"   - {file}\")\n",
    "\n",
    "# Save detailed log\n",
    "log_full_path = log_full_dir / f\"{run_id}_preflight_manifest.log\"\n",
    "with open(log_full_path, 'w') as f:\n",
    "    f.write(f\"=== PRE-FLIGHT MANIFEST SCAN LOG ===\")\n",
    "    f.write(f\"Run ID: {run_id}\")\n",
    "    f.write(f\"Start Time: {start_time}\")\n",
    "    f.write(f\"End Time: {end_time}\")\n",
    "    f.write(f\"Duration: {duration.total_seconds():.2f} seconds\")\n",
    "    f.write(f\"Project Root: {project_root}\")\n",
    "    f.write(f\"Notebook Dir: {notebook_dir}\")\n",
    "    f.write(f\"Data Raw Dir: {data_raw_dir}\")\n",
    "    f.write(f\"Python Version: {sys.version}\")\n",
    "    f.write(f\"Platform: {sys.platform}\")\n",
    "    \n",
    "    f.write(f\"FILE DETAILS:\")\n",
    "    for filename, info in manifest_data[\"files\"].items():\n",
    "        f.write(f\"{filename}:\")\n",
    "        for key, value in info.items():\n",
    "            f.write(f\"  {key}: {value}\")\n",
    "    \n",
    "    f.write(f\"SUMMARY:\")\n",
    "    for key, value in manifest_data[\"summary\"].items():\n",
    "        f.write(f\"  {key}: {value}\")\n",
    "\n",
    "# Save summary log\n",
    "log_summary_path = log_summary_dir / f\"{run_id}_preflight_summary.log\"\n",
    "with open(log_summary_path, 'w') as f:\n",
    "    f.write(f\"Pre-flight Manifest Scan Summary\")\n",
    "    f.write(f\"Run ID: {run_id}\")\n",
    "    f.write(f\"Files Found: {len(found_files)}/{len(expected_files)}\")\n",
    "    f.write(f\"Files Missing: {len(missing_files)}\")\n",
    "    f.write(f\"Total Size: {total_size_mb:.2f} MB\")\n",
    "    f.write(f\"Total Records (Estimate): {total_records:,}\")\n",
    "    f.write(f\"Duration: {duration.total_seconds():.2f} seconds\")\n",
    "    if missing_files:\n",
    "        f.write(f\"Missing Files: {', '.join(missing_files)}\")\n",
    "\n",
    "# Save JSON manifest\n",
    "manifest_path = manifest_dir / f\"manifest_{run_id}.json\"\n",
    "with open(manifest_path, 'w') as f:\n",
    "    json.dump(manifest_data, f, indent=2)\n",
    "\n",
    "print(f\"üìÅ OUTPUTS SAVED:\")\n",
    "print(f\"   Full Log: {log_full_path}\")\n",
    "print(f\"   Summary Log: {log_summary_path}\")\n",
    "print(f\"   JSON Manifest: {manifest_path}\")\n",
    "\n",
    "# LOG: End time, metrics, outputs\n",
    "print(f\"=== STEP 1 COMPLETE ===\")\n",
    "print(f\"End Time: {end_time}\")\n",
    "print(f\"Files Processed: {len(expected_files)}\")\n",
    "print(f\"Files Found: {len(found_files)}\")\n",
    "print(f\"Files Missing: {len(missing_files)}\")\n",
    "print(f\"Total Data Size: {total_size_mb:.2f} MB\")\n",
    "print(f\"Estimated Total Records: {total_records:,}\")\n",
    "print(f\"Outputs: 3 files saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Utility Functions\n",
    "\n",
    "This cell contains utility functions adapted from the Python modules for better file handling and data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Utility functions loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS (Adapted from src/utils/lightweight_handlers.py)\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Iterator, Optional, Union\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Set up basic logging for notebook\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class NotebookJSONHandler:\n",
    "    \"\"\"Enhanced JSON file handler for notebook use.\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 1000):\n",
    "        self.chunk_size = chunk_size\n",
    "    \n",
    "    def read_json_gz(self, file_path: Path, max_records: Optional[int] = None) -> Iterator[Dict[str, Any]]:\n",
    "        \"\"\"Read a gzipped JSON file line by line with progress tracking.\"\"\"\n",
    "        if not file_path.exists():\n",
    "            print(f\"‚ùå File not found: {file_path}\")\n",
    "            return\n",
    "            \n",
    "        print(f\"üìñ Reading JSON file: {file_path.name}\")\n",
    "        \n",
    "        records_read = 0\n",
    "        try:\n",
    "            with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "                for line_num, line in enumerate(f):\n",
    "                    if max_records and records_read >= max_records:\n",
    "                        break\n",
    "                        \n",
    "                    try:\n",
    "                        record = json.loads(line.strip())\n",
    "                        yield record\n",
    "                        records_read += 1\n",
    "                        \n",
    "                        # Progress indicator\n",
    "                        if records_read % 1000 == 0:\n",
    "                            print(f\"   Processed {records_read:,} records...\")\n",
    "                        \n",
    "                    except json.JSONDecodeError as e:\n",
    "                        if records_read < 10:  # Only show first few errors\n",
    "                            print(f\"‚ö†Ô∏è  JSON decode error at line {line_num}: {e}\")\n",
    "                        continue\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error reading file {file_path}: {e}\")\n",
    "            raise\n",
    "            \n",
    "        print(f\"‚úÖ Read {records_read:,} records from {file_path.name}\")\n",
    "    \n",
    "    def sample_records(self, file_path: Path, sample_size: int = 1000) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Sample records from a gzipped JSON file.\"\"\"\n",
    "        if not file_path.exists():\n",
    "            return []\n",
    "            \n",
    "        records = []\n",
    "        try:\n",
    "            with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    if len(records) >= sample_size:\n",
    "                        break\n",
    "                    try:\n",
    "                        record = json.loads(line.strip())\n",
    "                        records.append(record)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error sampling records from {file_path}: {e}\")\n",
    "            return []\n",
    "            \n",
    "        return records\n",
    "\n",
    "\n",
    "class NotebookSchemaInspector:\n",
    "    \"\"\"Schema inspector adapted for notebook use.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.handler = NotebookJSONHandler()\n",
    "    \n",
    "    def inspect_file_schema(self, file_path: Path, sample_size: int = 1000) -> Dict[str, Any]:\n",
    "        \"\"\"Inspect the schema of a JSON file.\"\"\"\n",
    "        if not file_path.exists():\n",
    "            print(f\"‚ùå File not found: {file_path}\")\n",
    "            return {}\n",
    "            \n",
    "        print(f\"üîç Inspecting schema for: {file_path.name}\")\n",
    "        \n",
    "        # Sample records\n",
    "        records = self.handler.sample_records(file_path, sample_size)\n",
    "        if not records:\n",
    "            print(f\"‚ö†Ô∏è  No valid records found in {file_path.name}\")\n",
    "            return {}\n",
    "            \n",
    "        # Analyze schema\n",
    "        field_types = defaultdict(set)\n",
    "        field_values = defaultdict(set)\n",
    "        missing_values = defaultdict(int)\n",
    "        field_lengths = defaultdict(list)\n",
    "        \n",
    "        for record in records:\n",
    "            for field_name, field_value in record.items():\n",
    "                # Track field types\n",
    "                field_types[field_name].add(type(field_value).__name__)\n",
    "                \n",
    "                # Track field lengths\n",
    "                if isinstance(field_value, str):\n",
    "                    field_lengths[field_name].append(len(field_value))\n",
    "                elif isinstance(field_value, list):\n",
    "                    field_lengths[field_name].append(len(field_value))\n",
    "                elif isinstance(field_value, dict):\n",
    "                    field_lengths[field_name].append(len(field_value))\n",
    "                \n",
    "                # Track unique values (limit to avoid memory issues)\n",
    "                if isinstance(field_value, (str, int, float)) and len(field_values[field_name]) < 100:\n",
    "                    field_values[field_name].add(str(field_value))\n",
    "                \n",
    "                # Track missing values\n",
    "                if field_value is None or field_value == \"\":\n",
    "                    missing_values[field_name] += 1\n",
    "        \n",
    "        # Compile schema information\n",
    "        schema_info = {\n",
    "            \"file_name\": file_path.name,\n",
    "            \"file_path\": str(file_path),\n",
    "            \"file_size_mb\": file_path.stat().st_size / (1024 * 1024),\n",
    "            \"records_analyzed\": len(records),\n",
    "            \"fields\": {},\n",
    "            \"sample_records\": records[:3] if records else []\n",
    "        }\n",
    "        \n",
    "        for field_name in field_types:\n",
    "            schema_info[\"fields\"][field_name] = {\n",
    "                \"types\": list(field_types[field_name]),\n",
    "                \"missing_count\": missing_values[field_name],\n",
    "                \"missing_percentage\": (missing_values[field_name] / len(records)) * 100,\n",
    "                \"unique_values_count\": len(field_values[field_name]),\n",
    "                \"sample_values\": list(field_values[field_name])[:10]\n",
    "            }\n",
    "            \n",
    "            # Add length statistics\n",
    "            if field_lengths[field_name]:\n",
    "                lengths = field_lengths[field_name]\n",
    "                schema_info[\"fields\"][field_name][\"length_stats\"] = {\n",
    "                    \"min_length\": min(lengths),\n",
    "                    \"max_length\": max(lengths),\n",
    "                    \"avg_length\": sum(lengths) / len(lengths)\n",
    "                }\n",
    "        \n",
    "        return schema_info\n",
    "\n",
    "\n",
    "# Utility functions\n",
    "def save_json_artifact(data: Dict[str, Any], file_path: Path) -> None:\n",
    "    \"\"\"Save data as JSON artifact.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        print(f\"üíæ Saved artifact: {file_path.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving artifact {file_path}: {e}\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Utility functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 2: Schema Inventory\n",
    "\n",
    "**Objective**: Analyze field structure, data types, and quality metrics for each JSON file.\n",
    "\n",
    "**Process**: Sample records from each file to understand:\n",
    "- Field names and data types\n",
    "- Missing value percentages\n",
    "- Sample values and value distributions\n",
    "- Data quality indicators\n",
    "\n",
    "**Output**: Comprehensive schema report for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Project root detected: /home/polina/Documents/goodreads_romance_research_cursor/romance-novel-nlp-research\n",
      "‚úÖ Data raw directory: /home/polina/Documents/goodreads_romance_research_cursor/romance-novel-nlp-research/data/raw\n",
      "‚úÖ Data raw exists: True\n",
      "=== SCHEMA INVENTORY ANALYSIS ===\n",
      "Run ID: 20250820_001816\n",
      "Start Time: 2025-08-20 00:18:16.808377\n",
      "Project Root: /home/polina/Documents/goodreads_romance_research_cursor/romance-novel-nlp-research\n",
      "Data Raw Directory: /home/polina/Documents/goodreads_romance_research_cursor/romance-novel-nlp-research/data/raw\n",
      "Python Version: 3.9.18 (main, Jan 29 2025, 16:13:49) \n",
      "[GCC 13.3.0]\n",
      "Platform: linux\n",
      "\n",
      "Analyzing schema for 9 files...\n",
      "============================================================\n",
      "\n",
      "üîç Analyzing: goodreads_books_romance.json.gz\n",
      "üîç Inspecting schema for: goodreads_books_romance.json.gz\n",
      "   ‚úÖ Fields: 29\n",
      "   ‚úÖ Records Analyzed: 1,000\n",
      "   ‚úÖ File Size: 347.88 MB\n",
      "   üìã Key Fields: isbn, text_reviews_count, series, country_code, language_code\n",
      "   ‚ö†Ô∏è  High Missing Fields: isbn, asin, isbn13\n",
      "\n",
      "üîç Analyzing: goodreads_interactions_romance.json.gz\n",
      "üîç Inspecting schema for: goodreads_interactions_romance.json.gz\n",
      "   ‚úÖ Fields: 10\n",
      "   ‚úÖ Records Analyzed: 1,000\n",
      "   ‚úÖ File Size: 2186.71 MB\n",
      "   üìã Key Fields: user_id, book_id, review_id, is_read, rating\n",
      "   ‚ö†Ô∏è  High Missing Fields: review_text_incomplete, read_at, started_at\n",
      "\n",
      "üîç Analyzing: goodreads_reviews_romance.json.gz\n",
      "üîç Inspecting schema for: goodreads_reviews_romance.json.gz\n",
      "   ‚úÖ Fields: 11\n",
      "   ‚úÖ Records Analyzed: 1,000\n",
      "   ‚úÖ File Size: 1240.85 MB\n",
      "   üìã Key Fields: user_id, book_id, review_id, rating, review_text\n",
      "\n",
      "üîç Analyzing: goodreads_book_authors.json.gz\n",
      "üîç Inspecting schema for: goodreads_book_authors.json.gz\n",
      "   ‚úÖ Fields: 5\n",
      "   ‚úÖ Records Analyzed: 1,000\n",
      "   ‚úÖ File Size: 17.16 MB\n",
      "   üìã Key Fields: average_rating, author_id, text_reviews_count, name, ratings_count\n",
      "\n",
      "üîç Analyzing: goodreads_book_works.json.gz\n",
      "üîç Inspecting schema for: goodreads_book_works.json.gz\n",
      "   ‚úÖ Fields: 16\n",
      "   ‚úÖ Records Analyzed: 1,000\n",
      "   ‚úÖ File Size: 71.90 MB\n",
      "   üìã Key Fields: books_count, reviews_count, original_publication_month, default_description_language_code, text_reviews_count\n",
      "   ‚ö†Ô∏è  High Missing Fields: default_description_language_code, default_chaptering_book_id, original_language_id\n",
      "\n",
      "üîç Analyzing: goodreads_book_genres_initial.json.gz\n",
      "üîç Inspecting schema for: goodreads_book_genres_initial.json.gz\n",
      "   ‚úÖ Fields: 2\n",
      "   ‚úÖ Records Analyzed: 1,000\n",
      "   ‚úÖ File Size: 23.12 MB\n",
      "   üìã Key Fields: book_id, genres\n",
      "\n",
      "üîç Analyzing: goodreads_reviews_dedup.json.gz\n",
      "üîç Inspecting schema for: goodreads_reviews_dedup.json.gz\n",
      "   ‚úÖ Fields: 11\n",
      "   ‚úÖ Records Analyzed: 1,000\n",
      "   ‚úÖ File Size: 5096.12 MB\n",
      "   üìã Key Fields: user_id, book_id, review_id, rating, review_text\n",
      "\n",
      "üîç Analyzing: goodreads_reviews_spoiler.json.gz\n",
      "üîç Inspecting schema for: goodreads_reviews_spoiler.json.gz\n",
      "   ‚úÖ Fields: 7\n",
      "   ‚úÖ Records Analyzed: 1,000\n",
      "   ‚úÖ File Size: 591.47 MB\n",
      "   üìã Key Fields: user_id, timestamp, review_sentences, rating, has_spoiler\n",
      "\n",
      "üîç Analyzing: goodreads_book_series.json.gz\n",
      "üîç Inspecting schema for: goodreads_book_series.json.gz\n",
      "   ‚úÖ Fields: 7\n",
      "   ‚úÖ Records Analyzed: 1,000\n",
      "   ‚úÖ File Size: 27.04 MB\n",
      "   üìã Key Fields: numbered, note, description, title, series_works_count\n",
      "   ‚ö†Ô∏è  High Missing Fields: note, description\n",
      "\n",
      "============================================================\n",
      "SCHEMA ANALYSIS SUMMARY\n",
      "Files Analyzed: 9/9\n",
      "Total Fields Discovered: 98\n",
      "Total Records Analyzed: 9,000\n",
      "Duration: 1.18 seconds\n",
      "End Time: 2025-08-20 00:18:17.990169\n",
      "üíæ Saved artifact: schema_analysis_20250820_001816.json\n",
      "\n",
      "üìÅ OUTPUTS SAVED:\n",
      "   Schema Analysis: /home/polina/Documents/goodreads_romance_research_cursor/romance-novel-nlp-research/data/intermediate/schemas/schema_analysis_20250820_001816.json\n",
      "   Summary Log: /home/polina/Documents/goodreads_romance_research_cursor/romance-novel-nlp-research/logs/exploration/summary/20250820_001816_schema_summary.log\n",
      "\n",
      "=== STEP 2 COMPLETE ===\n",
      "End Time: 2025-08-20 00:18:17.990169\n",
      "Files Analyzed: 9\n",
      "Total Fields Discovered: 98\n",
      "Total Records Analyzed: 9,000\n",
      "Outputs: 2 files saved\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: SCHEMA INVENTORY\n",
    "# ============================================================================\n",
    "\n",
    "# PLAN: Analyze field structure, data types, missing values, sample data\n",
    "# FILES: Read from data/raw/, write to logs/exploration/ and data/intermediate/schemas/\n",
    "# EXPECTED: Schema inventory per file with field types and quality metrics\n",
    "# TIMESTAMP: 2024-01-XX HH:MM:SS\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "# FIXED: Proper project root detection - go up from notebooks/ to project root\n",
    "notebook_dir = Path().absolute()\n",
    "project_root = notebook_dir.parent  # Go up one level from notebooks/ to project root\n",
    "\n",
    "# Validate we're in the right place\n",
    "if not (project_root / \"data\" / \"raw\").exists():\n",
    "    # Try alternative detection if we're already in project root\n",
    "    if (notebook_dir / \"data\" / \"raw\").exists():\n",
    "        project_root = notebook_dir\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Cannot find data/raw/ directory. Current: {notebook_dir}, Project root: {project_root}\")\n",
    "\n",
    "print(f\"‚úÖ Project root detected: {project_root}\")\n",
    "print(f\"‚úÖ Data raw directory: {project_root / 'data' / 'raw'}\")\n",
    "print(f\"‚úÖ Data raw exists: {(project_root / 'data' / 'raw').exists()}\")\n",
    "\n",
    "# Add project root to path for imports\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Create required directories\n",
    "log_full_dir = project_root / \"logs\" / \"exploration\" / \"full\"\n",
    "log_summary_dir = project_root / \"logs\" / \"exploration\" / \"summary\"\n",
    "schema_dir = project_root / \"data\" / \"intermediate\" / \"schemas\"\n",
    "\n",
    "for dir_path in [log_full_dir, log_summary_dir, schema_dir]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generate run ID\n",
    "run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Expected files (from Step 1 results)\n",
    "expected_files = [\n",
    "    \"goodreads_books_romance.json.gz\",\n",
    "    \"goodreads_interactions_romance.json.gz\",\n",
    "    \"goodreads_reviews_romance.json.gz\",\n",
    "    \"goodreads_book_authors.json.gz\",\n",
    "    \"goodreads_book_works.json.gz\",\n",
    "    \"goodreads_book_genres_initial.json.gz\",\n",
    "    \"goodreads_reviews_dedup.json.gz\",\n",
    "    \"goodreads_reviews_spoiler.json.gz\",\n",
    "    \"goodreads_book_series.json.gz\"\n",
    "]\n",
    "\n",
    "data_raw_dir = project_root / \"data\" / \"raw\"\n",
    "schema_data = {\n",
    "    \"run_id\": run_id,\n",
    "    \"timestamp\": start_time.isoformat(),\n",
    "    \"environment\": {\n",
    "        \"python_version\": sys.version,\n",
    "        \"platform\": sys.platform,\n",
    "        \"project_root\": str(project_root),\n",
    "        \"notebook_dir\": str(notebook_dir),\n",
    "        \"data_raw_dir\": str(data_raw_dir)\n",
    "    },\n",
    "    \"files\": {}\n",
    "}\n",
    "\n",
    "print(f\"=== SCHEMA INVENTORY ANALYSIS ===\")\n",
    "print(f\"Run ID: {run_id}\")\n",
    "print(f\"Start Time: {start_time}\")\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"Data Raw Directory: {data_raw_dir}\")\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"Platform: {sys.platform}\")\n",
    "print(f\"\\nAnalyzing schema for {len(expected_files)} files...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize schema inspector\n",
    "inspector = NotebookSchemaInspector()\n",
    "\n",
    "# Analyze each file\n",
    "for filename in expected_files:\n",
    "    file_path = data_raw_dir / filename\n",
    "    \n",
    "    if file_path.exists():\n",
    "        print(f\"\\nüîç Analyzing: {filename}\")\n",
    "        \n",
    "        # Analyze schema (sample 1000 records for efficiency)\n",
    "        schema_info = inspector.inspect_file_schema(file_path, sample_size=1000)\n",
    "        \n",
    "        if schema_info:\n",
    "            schema_data[\"files\"][filename] = schema_info\n",
    "            \n",
    "            # Display summary\n",
    "            print(f\"   ‚úÖ Fields: {len(schema_info['fields'])}\")\n",
    "            print(f\"   ‚úÖ Records Analyzed: {schema_info['records_analyzed']:,}\")\n",
    "            print(f\"   ‚úÖ File Size: {schema_info['file_size_mb']:.2f} MB\")\n",
    "            \n",
    "            # Show key fields (first 5)\n",
    "            field_names = list(schema_info['fields'].keys())[:5]\n",
    "            print(f\"   üìã Key Fields: {', '.join(field_names)}\")\n",
    "            \n",
    "            # Show data quality summary\n",
    "            high_missing = [f for f, data in schema_info['fields'].items() \n",
    "                           if data['missing_percentage'] > 50]\n",
    "            if high_missing:\n",
    "                print(f\"   ‚ö†Ô∏è  High Missing Fields: {', '.join(high_missing[:3])}\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Failed to analyze schema\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå {filename} - FILE NOT FOUND\")\n",
    "\n",
    "# Calculate summary statistics\n",
    "analyzed_files = [f for f, info in schema_data[\"files\"].items()]\n",
    "total_fields = sum(len(info['fields']) for info in schema_data[\"files\"].values())\n",
    "total_records_analyzed = sum(info['records_analyzed'] for info in schema_data[\"files\"].values())\n",
    "\n",
    "schema_data[\"summary\"] = {\n",
    "    \"files_analyzed\": len(analyzed_files),\n",
    "    \"total_fields_discovered\": total_fields,\n",
    "    \"total_records_analyzed\": total_records_analyzed,\n",
    "    \"files_analyzed\": analyzed_files\n",
    "}\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = end_time - start_time\n",
    "schema_data[\"duration_seconds\"] = duration.total_seconds()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"SCHEMA ANALYSIS SUMMARY\")\n",
    "print(f\"Files Analyzed: {len(analyzed_files)}/{len(expected_files)}\")\n",
    "print(f\"Total Fields Discovered: {total_fields}\")\n",
    "print(f\"Total Records Analyzed: {total_records_analyzed:,}\")\n",
    "print(f\"Duration: {duration.total_seconds():.2f} seconds\")\n",
    "print(f\"End Time: {end_time}\")\n",
    "\n",
    "# Save detailed schema report\n",
    "schema_path = schema_dir / f\"schema_analysis_{run_id}.json\"\n",
    "save_json_artifact(schema_data, schema_path)\n",
    "\n",
    "# Save summary log\n",
    "log_summary_path = log_summary_dir / f\"{run_id}_schema_summary.log\"\n",
    "summary_content = f\"Schema Analysis Summary\\n\"\n",
    "summary_content += f\"Run ID: {run_id}\\n\"\n",
    "summary_content += f\"Files Analyzed: {len(analyzed_files)}/{len(expected_files)}\\n\"\n",
    "summary_content += f\"Total Fields: {total_fields}\\n\"\n",
    "summary_content += f\"Total Records Analyzed: {total_records_analyzed:,}\\n\"\n",
    "summary_content += f\"Duration: {duration.total_seconds():.2f} seconds\\n\"\n",
    "\n",
    "with open(log_summary_path, 'w') as f:\n",
    "    f.write(summary_content)\n",
    "\n",
    "print(f\"\\nüìÅ OUTPUTS SAVED:\")\n",
    "print(f\"   Schema Analysis: {schema_path}\")\n",
    "print(f\"   Summary Log: {log_summary_path}\")\n",
    "\n",
    "# LOG: End time, metrics, outputs\n",
    "print(f\"\\n=== STEP 2 COMPLETE ===\")\n",
    "print(f\"End Time: {end_time}\")\n",
    "print(f\"Files Analyzed: {len(analyzed_files)}\")\n",
    "print(f\"Total Fields Discovered: {total_fields}\")\n",
    "print(f\"Total Records Analyzed: {total_records_analyzed:,}\")\n",
    "print(f\"Outputs: 2 files saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
