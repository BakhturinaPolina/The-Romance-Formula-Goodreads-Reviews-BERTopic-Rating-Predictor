{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Data Loading and Initial Setup\n",
    "\n",
    "This section handles the loading of the main dataset and performs initial data exploration to understand the structure and content of our romance books data.\n",
    "\n",
    "### What this section does:\n",
    "- Loads the main final dataset from CSV file\n",
    "- Drops unnecessary columns to focus on core variables\n",
    "- Performs detailed column-by-column analysis\n",
    "- Identifies data types, missing values, and unique value counts\n",
    "- Provides sample data for initial inspection\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded main final dataset: 53349 books\n",
      "INFO:__main__:Dropped columns: []\n",
      "INFO:__main__:Replaced NaN values in series_works_count_numeric with 'stand_alone'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape after dropping columns: (53349, 19)\n",
      "\n",
      "Remaining column names:\n",
      "['work_id', 'book_id_list_en', 'title', 'publication_year', 'num_pages_median', 'description', 'language_codes_en', 'author_id', 'author_name', 'author_average_rating', 'author_ratings_count', 'series_id', 'series_title', 'ratings_count_sum', 'text_reviews_count_sum', 'average_rating_weighted_mean', 'genres_str', 'shelves_str', 'series_works_count_numeric']\n",
      "\n",
      "Data types:\n",
      "work_id                           int64\n",
      "book_id_list_en                  object\n",
      "title                            object\n",
      "publication_year                  int64\n",
      "num_pages_median                float64\n",
      "description                      object\n",
      "language_codes_en                object\n",
      "author_id                         int64\n",
      "author_name                      object\n",
      "author_average_rating           float64\n",
      "author_ratings_count              int64\n",
      "series_id                        object\n",
      "series_title                     object\n",
      "ratings_count_sum                 int64\n",
      "text_reviews_count_sum            int64\n",
      "average_rating_weighted_mean    float64\n",
      "genres_str                       object\n",
      "shelves_str                      object\n",
      "series_works_count_numeric       object\n",
      "dtype: object\n",
      "\n",
      "============================================================\n",
      "COLUMN: work_id\n",
      "============================================================\n",
      "Data type: int64\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "üîë ID COLUMN - Excluded from numerical analysis\n",
      "üîë ID COLUMN - Basic stats skipped\n",
      "Unique values: 53349\n",
      "\n",
      "============================================================\n",
      "COLUMN: book_id_list_en\n",
      "============================================================\n",
      "Data type: object\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "üîë ID COLUMN - Excluded from numerical analysis\n",
      "Unique values: 53349\n",
      "Sample values:\n",
      "  [1] ['9416', '227650', '9423', '6088685', '19826270', '2574859', '13576133', '736396', '8724433', '32084...\n",
      "  [2] ['3462', '6338758', '289110', '6386960', '17787192']\n",
      "  [3] ['110391', '6077588', '25322247', '1859059', '298367', '5466440', '11793041', '23460960', '32177084'...\n",
      "  [4] ['861326', '6077587', '25322244', '353066', '9138773', '7961903', '32812329']\n",
      "  [5] ['22649', '22655', '31107', '6560878', '2576684', '12863586']\n",
      "  [6] ['469901', '11725217', '847901', '759501', '5410943']\n",
      "  [7] ['815150', '112753', '8130560', '6988762', '1982705', '344553', '22093943']\n",
      "  [8] ['89160', '6699943', '268595', '6146611', '7909185', '3062807', '2494210', '3062808', '6994134', '30...\n",
      "  [9] ['17781', '682359', '608949', '2875448']\n",
      "  [10] ['213975', '6660825', '531717', '10334321', '2097994', '8018051', '13553494', '1280346', '3062909', ...\n",
      "  ‚ö†Ô∏è  Contains list-like strings - may need parsing\n",
      "String length stats: min=8, max=1049, mean=21.0\n",
      "\n",
      "============================================================\n",
      "COLUMN: title\n",
      "============================================================\n",
      "Data type: object\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 46352\n",
      "Sample values:\n",
      "  [1] Confessions of a Shopaholic\n",
      "  [2] The Rescue\n",
      "  [3] The Duke and I\n",
      "  [4] The Viscount Who Loved Me\n",
      "  [5] Bookends\n",
      "  [6] Mr. Perfect\n",
      "  [7] The Highlander's Touch\n",
      "  [8] Judgment in Death\n",
      "  [9] Heart of the Sea\n",
      "  [10] Witness in Death\n",
      "String length stats: min=1, max=128, mean=16.5\n",
      "\n",
      "============================================================\n",
      "COLUMN: publication_year\n",
      "============================================================\n",
      "Data type: int64\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "üìä NUMERICAL COLUMN - Valid for analysis\n",
      "Basic stats:\n",
      "  count: 53349.0\n",
      "  mean: 2012.727736227483\n",
      "  std: 3.2824543292995014\n",
      "  min: 2000.0\n",
      "  25%: 2012.0\n",
      "  50%: 2013.0\n",
      "  75%: 2015.0\n",
      "  max: 2017.0\n",
      "Value counts (low cardinality - 18 unique values):\n",
      "  2013: 8998 (16.9%)\n",
      "  2014: 8565 (16.1%)\n",
      "  2015: 7072 (13.3%)\n",
      "  2012: 6318 (11.8%)\n",
      "  2016: 5778 (10.8%)\n",
      "  2017: 3682 (6.9%)\n",
      "  2011: 3614 (6.8%)\n",
      "  2010: 2172 (4.1%)\n",
      "  2009: 1663 (3.1%)\n",
      "  2008: 1206 (2.3%)\n",
      "\n",
      "============================================================\n",
      "COLUMN: num_pages_median\n",
      "============================================================\n",
      "Data type: float64\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "üìä NUMERICAL COLUMN - Valid for analysis\n",
      "Basic stats:\n",
      "  count: 53349.0\n",
      "  mean: 260.72378113929034\n",
      "  std: 100.93586926638137\n",
      "  min: 90.0\n",
      "  25%: 187.5\n",
      "  50%: 256.0\n",
      "  75%: 328.0\n",
      "  max: 980.0\n",
      "\n",
      "============================================================\n",
      "COLUMN: description\n",
      "============================================================\n",
      "Data type: object\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 53316\n",
      "Sample values:\n",
      "  [1] Unabridged audible download; approximately 11 hours 45 minutes If you've ever paid off one credit ca...\n",
      "  [2] When confronted by raging fires or deadly accidents, volunteer fireman Taylor McAden feels compelled...\n",
      "  [3] Can there be any greater challenge to London's Ambitious Mamas than an unmarried duke? --Lady Whistl...\n",
      "  [4] Alternate cover for ISBN: 0380815575/9780380815579 1814 promises to be another eventful season, but ...\n",
      "  [5] On the heels of her national bestsellers Jemima Jand Mr. Maybe, British sensation Jane Green deliver...\n",
      "  [6] What would make the perfect man?That's the delicious topic heating up the proceedings at a certain t...\n",
      "  [7] A Warrior of Immortal Powers He was a mighty Scottish warrior who lived in a world bound by ancient ...\n",
      "  [8] 'She stood in Purgatory and studied death. The blood and the gore of it, the ferocity of its glee. I...\n",
      "  [9] The breathtaking conclusion to the New York Times bestselling trilogy that began with Jewels of the ...\n",
      "  [10] LENGTH -- 10 hrs and 51 mins Opening night at New York's New Globe Theater turns from stage scene to...\n",
      "String length stats: min=4, max=14363, mean=979.4\n",
      "\n",
      "============================================================\n",
      "COLUMN: language_codes_en\n",
      "============================================================\n",
      "Data type: object\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 1\n",
      "Sample values:\n",
      "  [1] eng\n",
      "  [2] eng\n",
      "  [3] eng\n",
      "  [4] eng\n",
      "  [5] eng\n",
      "  [6] eng\n",
      "  [7] eng\n",
      "  [8] eng\n",
      "  [9] eng\n",
      "  [10] eng\n",
      "String length stats: min=3, max=3, mean=3.0\n",
      "\n",
      "============================================================\n",
      "COLUMN: author_id\n",
      "============================================================\n",
      "Data type: int64\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "üîë ID COLUMN - Excluded from numerical analysis\n",
      "üîë ID COLUMN - Basic stats skipped\n",
      "Unique values: 17810\n",
      "\n",
      "============================================================\n",
      "COLUMN: author_name\n",
      "============================================================\n",
      "Data type: object\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 17810\n",
      "Sample values:\n",
      "  [1] Sophie Kinsella\n",
      "  [2] Nicholas Sparks\n",
      "  [3] Julia Quinn\n",
      "  [4] Julia Quinn\n",
      "  [5] Jane Green\n",
      "  [6] Linda Howard\n",
      "  [7] Karen Marie Moning\n",
      "  [8] J.D. Robb\n",
      "  [9] Nora Roberts\n",
      "  [10] J.D. Robb\n",
      "String length stats: min=2, max=50, mean=12.9\n",
      "\n",
      "============================================================\n",
      "COLUMN: author_average_rating\n",
      "============================================================\n",
      "Data type: float64\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "üìä NUMERICAL COLUMN - Valid for analysis\n",
      "Basic stats:\n",
      "  count: 53349.0\n",
      "  mean: 3.9088455266265534\n",
      "  std: 0.25981381851075186\n",
      "  min: 1.27\n",
      "  25%: 3.75\n",
      "  50%: 3.92\n",
      "  75%: 4.08\n",
      "  max: 5.0\n",
      "\n",
      "============================================================\n",
      "COLUMN: author_ratings_count\n",
      "============================================================\n",
      "Data type: int64\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "üìä NUMERICAL COLUMN - Valid for analysis\n",
      "Basic stats:\n",
      "  count: 53349.0\n",
      "  mean: 48181.19623610564\n",
      "  std: 193397.0028438366\n",
      "  min: 1.0\n",
      "  25%: 843.0\n",
      "  50%: 5101.0\n",
      "  75%: 25548.0\n",
      "  max: 5280268.0\n",
      "\n",
      "============================================================\n",
      "COLUMN: series_id\n",
      "============================================================\n",
      "Data type: object\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "üîë ID COLUMN - Excluded from numerical analysis\n",
      "Unique values: 36066\n",
      "Sample values:\n",
      "  [1] 165735.0\n",
      "  [2] stand_alone\n",
      "  [3] 153045.0\n",
      "  [4] 144491.0\n",
      "  [5] stand_alone\n",
      "  [6] stand_alone\n",
      "  [7] 288256.0\n",
      "  [8] 145625.0\n",
      "  [9] 167116.0\n",
      "  [10] 162517.0\n",
      "String length stats: min=8, max=11, mean=9.0\n",
      "\n",
      "============================================================\n",
      "COLUMN: series_title\n",
      "============================================================\n",
      "Data type: object\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 17013\n",
      "Sample values:\n",
      "  [1] Shopaholic\n",
      "  [2] stand_alone\n",
      "  [3] Bridgertons\n",
      "  [4] Bridgertons\n",
      "  [5] stand_alone\n",
      "  [6] stand_alone\n",
      "  [7] Highlander\n",
      "  [8] In Death\n",
      "  [9] Gallaghers of Ardmore\n",
      "  [10] In Death\n",
      "String length stats: min=2, max=76, mean=13.7\n",
      "\n",
      "============================================================\n",
      "COLUMN: ratings_count_sum\n",
      "============================================================\n",
      "Data type: int64\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "üìä NUMERICAL COLUMN - Valid for analysis\n",
      "Basic stats:\n",
      "  count: 53349.0\n",
      "  mean: 1734.0819134379276\n",
      "  std: 14322.839048619157\n",
      "  min: 1.0\n",
      "  25%: 48.0\n",
      "  50%: 182.0\n",
      "  75%: 802.0\n",
      "  max: 1686868.0\n",
      "\n",
      "============================================================\n",
      "COLUMN: text_reviews_count_sum\n",
      "============================================================\n",
      "Data type: int64\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "üìä NUMERICAL COLUMN - Valid for analysis\n",
      "Basic stats:\n",
      "  count: 53349.0\n",
      "  mean: 141.86205927008942\n",
      "  std: 739.997994905451\n",
      "  min: 0.0\n",
      "  25%: 11.0\n",
      "  50%: 32.0\n",
      "  75%: 97.0\n",
      "  max: 74298.0\n",
      "\n",
      "============================================================\n",
      "COLUMN: average_rating_weighted_mean\n",
      "============================================================\n",
      "Data type: float64\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "üìä NUMERICAL COLUMN - Valid for analysis\n",
      "Basic stats:\n",
      "  count: 53349.0\n",
      "  mean: 3.9148518636413057\n",
      "  std: 0.34712338801084197\n",
      "  min: 1.27\n",
      "  25%: 3.71\n",
      "  50%: 3.93\n",
      "  75%: 4.15\n",
      "  max: 5.0\n",
      "\n",
      "============================================================\n",
      "COLUMN: genres_str\n",
      "============================================================\n",
      "Data type: object\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 137\n",
      "Sample values:\n",
      "  [1] fiction,romance,young adult\n",
      "  [2] fiction,mystery,romance,young adult\n",
      "  [3] biography,fiction,historical fiction,history,romance\n",
      "  [4] biography,fiction,historical fiction,history,romance\n",
      "  [5] fiction,romance\n",
      "  [6] fiction,mystery,romance\n",
      "  [7] biography,fantasy,fiction,historical fiction,history,paranormal,romance\n",
      "  [8] fantasy,fiction,mystery,paranormal,romance\n",
      "  [9] biography,fantasy,fiction,historical fiction,history,mystery,paranormal,romance\n",
      "  [10] fantasy,fiction,mystery,paranormal,romance\n",
      "String length stats: min=7, max=107, mean=28.0\n",
      "\n",
      "============================================================\n",
      "COLUMN: shelves_str\n",
      "============================================================\n",
      "Data type: object\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 53346\n",
      "Sample values:\n",
      "  [1] 3-stars,5-stars,abandoned,adult-fiction,audio,audiobook,audiobooks,beach-read,beach-reads,book-club,...\n",
      "  [2] 2000,2001,2012-reads,adult,adult-fiction,already-read,audio,audio-book,audio-books,audiobook,audiobo...\n",
      "  [3] 19th-century,1st-in-series,2012-reads,2016-reads,3-stars,4-stars,5-stars,adult,adult-fiction,adult-r...\n",
      "  [4] 1,19th-century,2016-reads,3-stars,4-stars,5-stars,adult,adult-fiction,adult-romance,all-time-favorit...\n",
      "  [5] 2002,2003,2004,2005,2006,5-stars,abandoned,adult,adult-fiction,audiobooks,beach-read,beach-reads,bea...\n",
      "  [6] 3-stars,4-stars,5-star,5-stars,aar-top-100,action,adult,adult-fiction,adult-romance,all-time-favorit...\n",
      "  [7] 4-star,4-stars,5-stars,adult,adult-fiction,adult-romance,alpha-male,audible,audio,audio-book,audio-b...\n",
      "  [8] 2015-reads,4-stars,5-stars,action,adult,adult-fiction,audible,audio,audio-book,audio-books,audiobook...\n",
      "  [9] adult-fiction,adult-romance,audible,audio,audiobook,audiobooks,author-nora-roberts,books,books-i-hav...\n",
      "  [10] 4-stars,5-stars,action,adult,adult-fiction,audible,audio,audio-book,audio-books,audiobook,audiobooks...\n",
      "String length stats: min=7, max=2283, mean=1019.0\n",
      "\n",
      "============================================================\n",
      "COLUMN: series_works_count_numeric\n",
      "============================================================\n",
      "Data type: object\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 70\n",
      "Sample values:\n",
      "  [1] 12.0\n",
      "  [2] stand_alone\n",
      "  [3] 19.0\n",
      "  [4] 19.0\n",
      "  [5] stand_alone\n",
      "  [6] stand_alone\n",
      "  [7] 12.0\n",
      "  [8] 115.0\n",
      "  [9] 6.0\n",
      "  [10] 115.0\n",
      "String length stats: min=3, max=11, mean=5.7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_id</th>\n",
       "      <th>book_id_list_en</th>\n",
       "      <th>title</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>num_pages_median</th>\n",
       "      <th>description</th>\n",
       "      <th>language_codes_en</th>\n",
       "      <th>author_id</th>\n",
       "      <th>author_name</th>\n",
       "      <th>author_average_rating</th>\n",
       "      <th>author_ratings_count</th>\n",
       "      <th>series_id</th>\n",
       "      <th>series_title</th>\n",
       "      <th>ratings_count_sum</th>\n",
       "      <th>text_reviews_count_sum</th>\n",
       "      <th>average_rating_weighted_mean</th>\n",
       "      <th>genres_str</th>\n",
       "      <th>shelves_str</th>\n",
       "      <th>series_works_count_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3237433</td>\n",
       "      <td>['9416', '227650', '9423', '6088685', '1982627...</td>\n",
       "      <td>Confessions of a Shopaholic</td>\n",
       "      <td>2000</td>\n",
       "      <td>320.0</td>\n",
       "      <td>Unabridged audible download; approximately 11 ...</td>\n",
       "      <td>eng</td>\n",
       "      <td>6160</td>\n",
       "      <td>Sophie Kinsella</td>\n",
       "      <td>3.74</td>\n",
       "      <td>2169284</td>\n",
       "      <td>165735.0</td>\n",
       "      <td>Shopaholic</td>\n",
       "      <td>555675</td>\n",
       "      <td>10488</td>\n",
       "      <td>3.62</td>\n",
       "      <td>fiction,romance,young adult</td>\n",
       "      <td>3-stars,5-stars,abandoned,adult-fiction,audio,...</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1268663</td>\n",
       "      <td>['3462', '6338758', '289110', '6386960', '1778...</td>\n",
       "      <td>The Rescue</td>\n",
       "      <td>2000</td>\n",
       "      <td>372.0</td>\n",
       "      <td>When confronted by raging fires or deadly acci...</td>\n",
       "      <td>eng</td>\n",
       "      <td>2345</td>\n",
       "      <td>Nicholas Sparks</td>\n",
       "      <td>4.06</td>\n",
       "      <td>4600277</td>\n",
       "      <td>stand_alone</td>\n",
       "      <td>stand_alone</td>\n",
       "      <td>148062</td>\n",
       "      <td>3150</td>\n",
       "      <td>4.10</td>\n",
       "      <td>fiction,mystery,romance,young adult</td>\n",
       "      <td>2000,2001,2012-reads,adult,adult-fiction,alrea...</td>\n",
       "      <td>stand_alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>846763</td>\n",
       "      <td>['110391', '6077588', '25322247', '1859059', '...</td>\n",
       "      <td>The Duke and I</td>\n",
       "      <td>2000</td>\n",
       "      <td>371.0</td>\n",
       "      <td>Can there be any greater challenge to London's...</td>\n",
       "      <td>eng</td>\n",
       "      <td>63898</td>\n",
       "      <td>Julia Quinn</td>\n",
       "      <td>3.98</td>\n",
       "      <td>567004</td>\n",
       "      <td>153045.0</td>\n",
       "      <td>Bridgertons</td>\n",
       "      <td>61848</td>\n",
       "      <td>2444</td>\n",
       "      <td>4.11</td>\n",
       "      <td>biography,fiction,historical fiction,history,r...</td>\n",
       "      <td>19th-century,1st-in-series,2012-reads,2016-rea...</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3363</td>\n",
       "      <td>['861326', '6077587', '25322244', '353066', '9...</td>\n",
       "      <td>The Viscount Who Loved Me</td>\n",
       "      <td>2000</td>\n",
       "      <td>381.0</td>\n",
       "      <td>Alternate cover for ISBN: 0380815575/978038081...</td>\n",
       "      <td>eng</td>\n",
       "      <td>63898</td>\n",
       "      <td>Julia Quinn</td>\n",
       "      <td>3.98</td>\n",
       "      <td>567004</td>\n",
       "      <td>144491.0</td>\n",
       "      <td>Bridgertons</td>\n",
       "      <td>38086</td>\n",
       "      <td>1404</td>\n",
       "      <td>4.19</td>\n",
       "      <td>biography,fiction,historical fiction,history,r...</td>\n",
       "      <td>1,19th-century,2016-reads,3-stars,4-stars,5-st...</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2363</td>\n",
       "      <td>['22649', '22655', '31107', '6560878', '257668...</td>\n",
       "      <td>Bookends</td>\n",
       "      <td>2000</td>\n",
       "      <td>368.0</td>\n",
       "      <td>On the heels of her national bestsellers Jemim...</td>\n",
       "      <td>eng</td>\n",
       "      <td>12915</td>\n",
       "      <td>Jane Green</td>\n",
       "      <td>3.58</td>\n",
       "      <td>502125</td>\n",
       "      <td>stand_alone</td>\n",
       "      <td>stand_alone</td>\n",
       "      <td>34139</td>\n",
       "      <td>842</td>\n",
       "      <td>3.70</td>\n",
       "      <td>fiction,romance</td>\n",
       "      <td>2002,2003,2004,2005,2006,5-stars,abandoned,adu...</td>\n",
       "      <td>stand_alone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   work_id                                    book_id_list_en  \\\n",
       "0  3237433  ['9416', '227650', '9423', '6088685', '1982627...   \n",
       "1  1268663  ['3462', '6338758', '289110', '6386960', '1778...   \n",
       "2   846763  ['110391', '6077588', '25322247', '1859059', '...   \n",
       "3     3363  ['861326', '6077587', '25322244', '353066', '9...   \n",
       "4     2363  ['22649', '22655', '31107', '6560878', '257668...   \n",
       "\n",
       "                         title  publication_year  num_pages_median  \\\n",
       "0  Confessions of a Shopaholic              2000             320.0   \n",
       "1                   The Rescue              2000             372.0   \n",
       "2               The Duke and I              2000             371.0   \n",
       "3    The Viscount Who Loved Me              2000             381.0   \n",
       "4                     Bookends              2000             368.0   \n",
       "\n",
       "                                         description language_codes_en  \\\n",
       "0  Unabridged audible download; approximately 11 ...               eng   \n",
       "1  When confronted by raging fires or deadly acci...               eng   \n",
       "2  Can there be any greater challenge to London's...               eng   \n",
       "3  Alternate cover for ISBN: 0380815575/978038081...               eng   \n",
       "4  On the heels of her national bestsellers Jemim...               eng   \n",
       "\n",
       "   author_id      author_name  author_average_rating  author_ratings_count  \\\n",
       "0       6160  Sophie Kinsella                   3.74               2169284   \n",
       "1       2345  Nicholas Sparks                   4.06               4600277   \n",
       "2      63898      Julia Quinn                   3.98                567004   \n",
       "3      63898      Julia Quinn                   3.98                567004   \n",
       "4      12915       Jane Green                   3.58                502125   \n",
       "\n",
       "     series_id series_title  ratings_count_sum  text_reviews_count_sum  \\\n",
       "0     165735.0   Shopaholic             555675                   10488   \n",
       "1  stand_alone  stand_alone             148062                    3150   \n",
       "2     153045.0  Bridgertons              61848                    2444   \n",
       "3     144491.0  Bridgertons              38086                    1404   \n",
       "4  stand_alone  stand_alone              34139                     842   \n",
       "\n",
       "   average_rating_weighted_mean  \\\n",
       "0                          3.62   \n",
       "1                          4.10   \n",
       "2                          4.11   \n",
       "3                          4.19   \n",
       "4                          3.70   \n",
       "\n",
       "                                          genres_str  \\\n",
       "0                        fiction,romance,young adult   \n",
       "1                fiction,mystery,romance,young adult   \n",
       "2  biography,fiction,historical fiction,history,r...   \n",
       "3  biography,fiction,historical fiction,history,r...   \n",
       "4                                    fiction,romance   \n",
       "\n",
       "                                         shelves_str  \\\n",
       "0  3-stars,5-stars,abandoned,adult-fiction,audio,...   \n",
       "1  2000,2001,2012-reads,adult,adult-fiction,alrea...   \n",
       "2  19th-century,1st-in-series,2012-reads,2016-rea...   \n",
       "3  1,19th-century,2016-reads,3-stars,4-stars,5-st...   \n",
       "4  2002,2003,2004,2005,2006,5-stars,abandoned,adu...   \n",
       "\n",
       "  series_works_count_numeric  \n",
       "0                       12.0  \n",
       "1                stand_alone  \n",
       "2                       19.0  \n",
       "3                       19.0  \n",
       "4                stand_alone  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "### üîÑ Dataset Loading and Column Management\n",
    "\n",
    "# Load dataset\n",
    "main_final_path = \"../../data/processed/romance_books_main_final.csv\"\n",
    "main_final = pd.read_csv(main_final_path)\n",
    "logger.info(f\"Loaded main final dataset: {len(main_final)} books\")\n",
    "\n",
    "# Drop specified columns\n",
    "columns_to_drop = ['series_works_count', 'popular_shelves', 'genres', 'decade', \n",
    "                   'book_length_category', 'rating_category', 'popularity_category', \n",
    "                   'has_collection_indicators']\n",
    "main_final = main_final.drop(columns=columns_to_drop, errors='ignore')\n",
    "logger.info(f\"Dropped columns: {[col for col in columns_to_drop if col in main_final.columns]}\")\n",
    "\n",
    "# Clean series_works_count_numeric: replace NaN with 'stand_alone'\n",
    "main_final['series_works_count_numeric'] = main_final['series_works_count_numeric'].fillna('stand_alone')\n",
    "logger.info(f\"Replaced NaN values in series_works_count_numeric with 'stand_alone'\")\n",
    "\n",
    "### üìã Basic Dataset Information\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Dataset shape after dropping columns: {main_final.shape}\")\n",
    "print(f\"\\nRemaining column names:\")\n",
    "print(main_final.columns.tolist())\n",
    "print(f\"\\nData types:\")\n",
    "print(main_final.dtypes)\n",
    "\n",
    "### üîç Detailed Column Investigation\n",
    "\n",
    "# Define ID columns to exclude from numerical analysis\n",
    "id_columns = ['work_id', 'book_id_list_en', 'author_id', 'series_id']\n",
    "\n",
    "for col in main_final.columns:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COLUMN: {col}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"Data type: {main_final[col].dtype}\")\n",
    "    print(f\"Non-null count: {main_final[col].count()} / {len(main_final)} ({main_final[col].count()/len(main_final)*100:.1f}%)\")\n",
    "    print(f\"Null count: {main_final[col].isnull().sum()} ({main_final[col].isnull().sum()/len(main_final)*100:.1f}%)\")\n",
    "    \n",
    "    # Mark ID columns\n",
    "    if col in id_columns:\n",
    "        print(\"üîë ID COLUMN - Excluded from numerical analysis\")\n",
    "    \n",
    "    # Type-specific analysis\n",
    "    if main_final[col].dtype in ['object']:\n",
    "        print(f\"Unique values: {main_final[col].nunique()}\")\n",
    "        print(f\"Sample values:\")\n",
    "        sample_values = main_final[col].dropna().head(10).tolist()\n",
    "        for i, val in enumerate(sample_values):\n",
    "            val_str = str(val)\n",
    "            if len(val_str) > 100:\n",
    "                val_str = val_str[:100] + \"...\"\n",
    "            print(f\"  [{i+1}] {val_str}\")\n",
    "        \n",
    "        # Check for list-like strings\n",
    "        if any(main_final[col].dropna().astype(str).str.startswith('[').head(100)):\n",
    "            print(\"  ‚ö†Ô∏è  Contains list-like strings - may need parsing\")\n",
    "        \n",
    "        # Value length distribution for string columns\n",
    "        lengths = main_final[col].dropna().astype(str).str.len()\n",
    "        print(f\"String length stats: min={lengths.min()}, max={lengths.max()}, mean={lengths.mean():.1f}\")\n",
    "        \n",
    "    elif main_final[col].dtype in ['int64', 'float64'] and col not in id_columns:\n",
    "        print(f\"üìä NUMERICAL COLUMN - Valid for analysis\")\n",
    "        print(f\"Basic stats:\")\n",
    "        stats = main_final[col].describe()\n",
    "        for stat_name, stat_val in stats.items():\n",
    "            print(f\"  {stat_name}: {stat_val}\")\n",
    "        \n",
    "        # Check for potential categorical numeric columns\n",
    "        unique_count = main_final[col].nunique()\n",
    "        if unique_count <= 20:\n",
    "            print(f\"Value counts (low cardinality - {unique_count} unique values):\")\n",
    "            vc = main_final[col].value_counts().head(10)\n",
    "            for val, count in vc.items():\n",
    "                print(f\"  {val}: {count} ({count/len(main_final)*100:.1f}%)\")\n",
    "    \n",
    "    elif main_final[col].dtype in ['int64', 'float64'] and col in id_columns:\n",
    "        print(f\"üîë ID COLUMN - Basic stats skipped\")\n",
    "        unique_count = main_final[col].nunique()\n",
    "        print(f\"Unique values: {unique_count}\")\n",
    "        \n",
    "    elif main_final[col].dtype in ['bool']:\n",
    "        print(f\"Boolean distribution:\")\n",
    "        vc = main_final[col].value_counts()\n",
    "        for val, count in vc.items():\n",
    "            print(f\"  {val}: {count} ({count/len(main_final)*100:.1f}%)\")\n",
    "\n",
    "### üìä Sample Data Preview\n",
    "\n",
    "main_final.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Universal String Canonicalization\n",
    "\n",
    "This section performs comprehensive canonicalization of genre and shelf strings to create standardized, normalized versions for consistent analysis and comparison.\n",
    "\n",
    "### What this section does:\n",
    "- Applies consistent normalization rules to all genre and shelf strings\n",
    "- Creates canonical mappings between original and normalized forms\n",
    "- Handles case normalization, whitespace cleaning, and separator standardization\n",
    "- Generates comprehensive statistics on transformation patterns\n",
    "- Prepares clean, standardized data for downstream similarity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[21:02:34] üîß CELL 2: UNIVERSAL STRING CANONICALIZATION (v0)\n",
      "======================================================================\n",
      "üìã CANONICALIZATION CONFIG:\n",
      "  normalize_case: True\n",
      "  remove_extra_whitespace: True\n",
      "  remove_special_chars: False\n",
      "  standardize_separators: True\n",
      "  min_token_length: 1\n",
      "  max_token_length: 100\n",
      "\n",
      "üìö GENRE CANONICALIZATION\n",
      "----------------------------------------\n",
      "\n",
      "üîß PROCESSING GENRES:\n",
      "Canonicalizing genres from main_final dataset...\n",
      "Found 13 unique genres\n",
      "  ‚úÖ Processed 13 genres\n",
      "\n",
      "üìö SHELF CANONICALIZATION\n",
      "----------------------------------------\n",
      "\n",
      "üîß PROCESSING SHELVES:\n",
      "Canonicalizing shelves from main_final dataset...\n",
      "Found 255,664 unique shelves\n",
      "  ‚úÖ Processed 255,664 shelves\n",
      "\n",
      "üìä CANONICALIZATION RESULTS:\n",
      "----------------------------------------\n",
      "üìö GENRES:\n",
      "  Original count: 13\n",
      "  Canonical count: 13\n",
      "  Compression ratio: 1.000\n",
      "  Changes made: 0\n",
      "  Unchanged: 13\n",
      "\n",
      "üìö SHELVES:\n",
      "  Original count: 255,664\n",
      "  Canonical count: 254,778\n",
      "  Compression ratio: 0.997\n",
      "  Changes made: 230,934\n",
      "  Unchanged: 24,730\n",
      "\n",
      "üíæ SAVING CANONICAL MAPPINGS:\n",
      "----------------------------------------\n",
      "  ‚úÖ Saved genre mappings to: romance-novel-nlp-research/src/eda_analysis/outputs/genre_canonical_mappings.csv\n",
      "  ‚úÖ Saved shelf mappings to: romance-novel-nlp-research/src/eda_analysis/outputs/shelf_canonical_mappings.csv\n",
      "  ‚úÖ Saved metadata to: romance-novel-nlp-research/src/eda_analysis/outputs/canonicalization_metadata.json\n",
      "\n",
      "[21:03:00] ‚úÖ Cell 2: Universal String Canonicalization completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: UNIVERSAL STRING CANONICALIZATION (v0)\n",
    "# =============================================================================\n",
    "\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"\\n[{time.strftime('%H:%M:%S')}] üîß CELL 2: UNIVERSAL STRING CANONICALIZATION (v0)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "### ‚öôÔ∏è Configuration Setup\n",
    "\n",
    "# Configuration for canonicalization\n",
    "CANONICAL_CONFIG = {\n",
    "    'normalize_case': True,\n",
    "    'remove_extra_whitespace': True,\n",
    "    'remove_special_chars': False,  # Keep for genre/shelf analysis\n",
    "    'standardize_separators': True,\n",
    "    'min_token_length': 1,\n",
    "    'max_token_length': 100\n",
    "}\n",
    "\n",
    "print(f\"üìã CANONICALIZATION CONFIG:\")\n",
    "for key, value in CANONICAL_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "### üìö Genre Canonicalization\n",
    "\n",
    "print(f\"\\nüìö GENRE CANONICALIZATION\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def canonicalize_genre(genre_str):\n",
    "    \"\"\"\n",
    "    Canonicalize a single genre string.\n",
    "    \n",
    "    Args:\n",
    "        genre_str (str): Raw genre string\n",
    "        \n",
    "    Returns:\n",
    "        str: Canonicalized genre string\n",
    "    \"\"\"\n",
    "    if not isinstance(genre_str, str) or not genre_str.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    # Normalize case\n",
    "    canonical = genre_str.lower() if CANONICAL_CONFIG['normalize_case'] else genre_str\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    if CANONICAL_CONFIG['remove_extra_whitespace']:\n",
    "        canonical = ' '.join(canonical.split())\n",
    "    \n",
    "    # Standardize separators (hyphens to spaces for consistency)\n",
    "    if CANONICAL_CONFIG['standardize_separators']:\n",
    "        canonical = re.sub(r'[-_]+', ' ', canonical)\n",
    "        canonical = ' '.join(canonical.split())  # Clean up multiple spaces\n",
    "    \n",
    "    # Length validation\n",
    "    if len(canonical) < CANONICAL_CONFIG['min_token_length'] or len(canonical) > CANONICAL_CONFIG['max_token_length']:\n",
    "        return \"\"\n",
    "    \n",
    "    return canonical.strip()\n",
    "\n",
    "# Apply canonicalization to unique genres\n",
    "print(f\"\\nüîß PROCESSING GENRES:\")\n",
    "print(f\"Canonicalizing genres from main_final dataset...\")\n",
    "\n",
    "# Extract unique genres from the dataset\n",
    "unique_genres = set()\n",
    "for idx, row in main_final.iterrows():\n",
    "    if pd.notna(row.get('genres_str')) and row['genres_str'].strip():\n",
    "        genres_list = [g.strip() for g in row['genres_str'].split(',') if g.strip()]\n",
    "        unique_genres.update(genres_list)\n",
    "\n",
    "print(f\"Found {len(unique_genres):,} unique genres\")\n",
    "\n",
    "canonical_genres = {}\n",
    "genre_mapping_stats = defaultdict(list)\n",
    "\n",
    "for original_genre in unique_genres:\n",
    "    canonical = canonicalize_genre(original_genre)\n",
    "    canonical_genres[original_genre] = canonical\n",
    "    \n",
    "    # Track mapping for analysis\n",
    "    if canonical != original_genre.lower():\n",
    "        genre_mapping_stats['changed'].append((original_genre, canonical))\n",
    "    else:\n",
    "        genre_mapping_stats['unchanged'].append(original_genre)\n",
    "\n",
    "print(f\"  ‚úÖ Processed {len(canonical_genres):,} genres\")\n",
    "\n",
    "### üìö Shelf Canonicalization\n",
    "\n",
    "print(f\"\\nüìö SHELF CANONICALIZATION\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def canonicalize_shelf(shelf_str):\n",
    "    \"\"\"\n",
    "    Canonicalize a single shelf string.\n",
    "    \n",
    "    Args:\n",
    "        shelf_str (str): Raw shelf string\n",
    "        \n",
    "    Returns:\n",
    "        str: Canonicalized shelf string\n",
    "    \"\"\"\n",
    "    if not isinstance(shelf_str, str) or not shelf_str.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    # Normalize case\n",
    "    canonical = shelf_str.lower() if CANONICAL_CONFIG['normalize_case'] else shelf_str\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    if CANONICAL_CONFIG['remove_extra_whitespace']:\n",
    "        canonical = ' '.join(canonical.split())\n",
    "    \n",
    "    # Standardize separators (hyphens to spaces for consistency)\n",
    "    if CANONICAL_CONFIG['standardize_separators']:\n",
    "        canonical = re.sub(r'[-_]+', ' ', canonical)\n",
    "        canonical = ' '.join(canonical.split())  # Clean up multiple spaces\n",
    "    \n",
    "    # Length validation\n",
    "    if len(canonical) < CANONICAL_CONFIG['min_token_length'] or len(canonical) > CANONICAL_CONFIG['max_token_length']:\n",
    "        return \"\"\n",
    "    \n",
    "    return canonical.strip()\n",
    "\n",
    "# Apply canonicalization to unique shelves\n",
    "print(f\"\\nüîß PROCESSING SHELVES:\")\n",
    "print(f\"Canonicalizing shelves from main_final dataset...\")\n",
    "\n",
    "# Extract unique shelves from the dataset\n",
    "unique_shelves = set()\n",
    "for idx, row in main_final.iterrows():\n",
    "    if pd.notna(row.get('shelves_str')) and row['shelves_str'].strip():\n",
    "        shelves_list = [s.strip() for s in row['shelves_str'].split(',') if s.strip()]\n",
    "        unique_shelves.update(shelves_list)\n",
    "\n",
    "print(f\"Found {len(unique_shelves):,} unique shelves\")\n",
    "\n",
    "canonical_shelves = {}\n",
    "shelf_mapping_stats = defaultdict(list)\n",
    "\n",
    "for original_shelf in unique_shelves:\n",
    "    canonical = canonicalize_shelf(original_shelf)\n",
    "    canonical_shelves[original_shelf] = canonical\n",
    "    \n",
    "    # Track mapping for analysis\n",
    "    if canonical != original_shelf.lower():\n",
    "        shelf_mapping_stats['changed'].append((original_shelf, canonical))\n",
    "    else:\n",
    "        shelf_mapping_stats['unchanged'].append(original_shelf)\n",
    "\n",
    "print(f\"  ‚úÖ Processed {len(canonical_shelves):,} shelves\")\n",
    "\n",
    "### üìä Canonicalization Results\n",
    "\n",
    "print(f\"\\nüìä CANONICALIZATION RESULTS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Get unique canonical values\n",
    "unique_canonical_genres = set(canonical_genres.values())\n",
    "unique_canonical_shelves = set(canonical_shelves.values())\n",
    "\n",
    "# Calculate compression ratios\n",
    "genre_compression_ratio = len(unique_canonical_genres) / len(unique_genres) if len(unique_genres) > 0 else 0\n",
    "shelf_compression_ratio = len(unique_canonical_shelves) / len(unique_shelves) if len(unique_shelves) > 0 else 0\n",
    "\n",
    "print(f\"üìö GENRES:\")\n",
    "print(f\"  Original count: {len(unique_genres):,}\")\n",
    "print(f\"  Canonical count: {len(unique_canonical_genres):,}\")\n",
    "print(f\"  Compression ratio: {genre_compression_ratio:.3f}\")\n",
    "print(f\"  Changes made: {len(genre_mapping_stats['changed']):,}\")\n",
    "print(f\"  Unchanged: {len(genre_mapping_stats['unchanged']):,}\")\n",
    "\n",
    "print(f\"\\nüìö SHELVES:\")\n",
    "print(f\"  Original count: {len(unique_shelves):,}\")\n",
    "print(f\"  Canonical count: {len(unique_canonical_shelves):,}\")\n",
    "print(f\"  Compression ratio: {shelf_compression_ratio:.3f}\")\n",
    "print(f\"  Changes made: {len(shelf_mapping_stats['changed']):,}\")\n",
    "print(f\"  Unchanged: {len(shelf_mapping_stats['unchanged']):,}\")\n",
    "\n",
    "### üíæ Save Canonical Mappings\n",
    "\n",
    "print(f\"\\nüíæ SAVING CANONICAL MAPPINGS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create outputs directory\n",
    "outputs_dir = Path(\"romance-novel-nlp-research/src/eda_analysis/outputs\")\n",
    "outputs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save genre mappings\n",
    "genre_mappings_df = pd.DataFrame([\n",
    "    {'original': orig, 'canonical': canon} \n",
    "    for orig, canon in canonical_genres.items()\n",
    "])\n",
    "genre_mappings_path = outputs_dir / \"genre_canonical_mappings.csv\"\n",
    "genre_mappings_df.to_csv(genre_mappings_path, index=False)\n",
    "print(f\"  ‚úÖ Saved genre mappings to: {genre_mappings_path}\")\n",
    "\n",
    "# Save shelf mappings\n",
    "shelf_mappings_df = pd.DataFrame([\n",
    "    {'original': orig, 'canonical': canon} \n",
    "    for orig, canon in canonical_shelves.items()\n",
    "])\n",
    "shelf_mappings_path = outputs_dir / \"shelf_canonical_mappings.csv\"\n",
    "shelf_mappings_df.to_csv(shelf_mappings_path, index=False)\n",
    "print(f\"  ‚úÖ Saved shelf mappings to: {shelf_mappings_path}\")\n",
    "\n",
    "# Save canonicalization metadata\n",
    "canonical_meta = {\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': CANONICAL_CONFIG,\n",
    "    'stats': {\n",
    "        'genres': {\n",
    "            'original_count': len(unique_genres),\n",
    "            'canonical_count': len(unique_canonical_genres),\n",
    "            'compression_ratio': genre_compression_ratio,\n",
    "            'changes_count': len(genre_mapping_stats['changed']),\n",
    "            'duplicates_eliminated': len(unique_genres) - len(unique_canonical_genres)\n",
    "        },\n",
    "        'shelves': {\n",
    "            'original_count': len(unique_shelves),\n",
    "            'canonical_count': len(unique_canonical_shelves),\n",
    "            'compression_ratio': shelf_compression_ratio,\n",
    "            'changes_count': len(shelf_mapping_stats['changed']),\n",
    "            'duplicates_eliminated': len(unique_shelves) - len(unique_canonical_shelves)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = outputs_dir / \"canonicalization_metadata.json\"\n",
    "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(canonical_meta, f, indent=2, ensure_ascii=False)\n",
    "print(f\"  ‚úÖ Saved metadata to: {metadata_path}\")\n",
    "\n",
    "print(f\"\\n[{time.strftime('%H:%M:%S')}] ‚úÖ Cell 2: Universal String Canonicalization completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Character Similarity Index & Neighbor Retrieval\n",
    "\n",
    "This section builds a comprehensive character-based similarity index using TF-IDF vectorization and approximate nearest neighbor (ANN) search to identify potential duplicate shelf names.\n",
    "\n",
    "### What this section does:\n",
    "- Creates TF-IDF vectors from canonical shelf tokens using character n-grams\n",
    "- Builds an approximate nearest neighbor index for efficient similarity search\n",
    "- Retrieves candidate similar pairs based on cosine similarity thresholds\n",
    "- Generates comprehensive statistics on similarity patterns and coverage\n",
    "- Exports sample data for manual validation and quality assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[21:03:38] üîç Starting Cell 3: Character Similarity Index & Neighbor Retrieval\n",
      "================================================================================\n",
      "üìã TF-IDF CONFIGURATION:\n",
      "  N-gram range: (2, 4)\n",
      "  Min document frequency: 2\n",
      "  Max features: 10000\n",
      "  Similarity threshold: 0.3\n",
      "  Top-K neighbors: 50\n",
      "\n",
      "üî§ TOKEN VECTORIZATION:\n",
      "------------------------------\n",
      "  üìä Total canonical tokens: 254,778\n",
      "  ‚úÖ TF-IDF fitted: 254,778 tokens √ó 10,000 features\n",
      "  ‚è±Ô∏è  Vectorization time: 24.93 seconds\n",
      "\n",
      "üîç BUILDING ANN INDEX & RETRIEVING NEIGHBORS:\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: CHARACTER SIMILARITY INDEX & NEIGHBOR RETRIEVAL\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(f\"\\n[{time.strftime('%H:%M:%S')}] üîç Starting Cell 3: Character Similarity Index & Neighbor Retrieval\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "### ‚öôÔ∏è Configuration Setup\n",
    "\n",
    "# TF-IDF Configuration\n",
    "NGRAM_RANGE = (2, 4)  # Character n-grams from 2 to 4 characters\n",
    "MIN_DF = 2  # Minimum document frequency\n",
    "MAX_FEATURES = 10000  # Maximum number of features\n",
    "SIMILARITY_THRESHOLD = 0.3  # Minimum cosine similarity for candidate pairs\n",
    "TOP_K_NEIGHBORS = 50  # Number of nearest neighbors to retrieve\n",
    "\n",
    "print(f\"üìã TF-IDF CONFIGURATION:\")\n",
    "print(f\"  N-gram range: {NGRAM_RANGE}\")\n",
    "print(f\"  Min document frequency: {MIN_DF}\")\n",
    "print(f\"  Max features: {MAX_FEATURES}\")\n",
    "print(f\"  Similarity threshold: {SIMILARITY_THRESHOLD}\")\n",
    "print(f\"  Top-K neighbors: {TOP_K_NEIGHBORS}\")\n",
    "\n",
    "### üî§ Token Vectorization\n",
    "\n",
    "print(f\"\\nüî§ TOKEN VECTORIZATION:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Get canonical tokens from previous cell\n",
    "canonical_tokens = list(unique_canonical_shelves)\n",
    "print(f\"  üìä Total canonical tokens: {len(canonical_tokens):,}\")\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=NGRAM_RANGE,\n",
    "    min_df=MIN_DF,\n",
    "    max_features=MAX_FEATURES,\n",
    "    lowercase=False,  # Already canonicalized\n",
    "    token_pattern=None  # Use character n-grams\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "vectorize_start = time.time()\n",
    "tfidf_matrix = vectorizer.fit_transform(canonical_tokens)\n",
    "vectorize_time = time.time() - vectorize_start\n",
    "\n",
    "print(f\"  ‚úÖ TF-IDF fitted: {tfidf_matrix.shape[0]:,} tokens √ó {tfidf_matrix.shape[1]:,} features\")\n",
    "print(f\"  ‚è±Ô∏è  Vectorization time: {vectorize_time:.2f} seconds\")\n",
    "\n",
    "### üîç Build ANN Index & Retrieve Neighbors\n",
    "\n",
    "print(f\"\\nüîç BUILDING ANN INDEX & RETRIEVING NEIGHBORS:\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "ann_start = time.time()\n",
    "\n",
    "# Build approximate nearest neighbor index\n",
    "nbrs = NearestNeighbors(\n",
    "    n_neighbors=min(TOP_K_NEIGHBORS + 1, len(canonical_tokens)),  # +1 to exclude self\n",
    "    algorithm='auto',\n",
    "    metric='cosine'\n",
    ")\n",
    "nbrs.fit(tfidf_matrix)\n",
    "\n",
    "# Retrieve neighbors for all tokens\n",
    "distances, indices = nbrs.kneighbors(tfidf_matrix)\n",
    "\n",
    "ann_time = time.time() - ann_start\n",
    "print(f\"  ‚úÖ ANN index built and neighbors retrieved\")\n",
    "print(f\"  ‚è±Ô∏è  ANN time: {ann_time:.2f} seconds\")\n",
    "\n",
    "### üìä Generate Candidate Pairs\n",
    "\n",
    "print(f\"\\nüìä GENERATING CANDIDATE PAIRS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "candidate_pairs = []\n",
    "total_pairs_checked = 0\n",
    "\n",
    "for i, token in enumerate(canonical_tokens):\n",
    "    # Get neighbors (excluding self)\n",
    "    neighbor_distances = distances[i][1:]  # Skip first (self)\n",
    "    neighbor_indices = indices[i][1:]  # Skip first (self)\n",
    "    \n",
    "    for j, (dist, neighbor_idx) in enumerate(zip(neighbor_distances, neighbor_indices)):\n",
    "        total_pairs_checked += 1\n",
    "        \n",
    "        # Convert distance to similarity (cosine distance = 1 - cosine similarity)\n",
    "        similarity = 1 - dist\n",
    "        \n",
    "        if similarity >= SIMILARITY_THRESHOLD:\n",
    "            neighbor_token = canonical_tokens[neighbor_idx]\n",
    "            candidate_pairs.append({\n",
    "                'token_a': token,\n",
    "                'token_b': neighbor_token,\n",
    "                'cosine_sim': similarity,\n",
    "                'rank': j + 1\n",
    "            })\n",
    "\n",
    "print(f\"  üìä Total pairs checked: {total_pairs_checked:,}\")\n",
    "print(f\"  üìä Candidate pairs found: {len(candidate_pairs):,}\")\n",
    "print(f\"  üìä Hit rate: {len(candidate_pairs)/total_pairs_checked*100:.2f}%\")\n",
    "\n",
    "### üìà Calculate Additional Metrics\n",
    "\n",
    "print(f\"\\nüìà CALCULATING ADDITIONAL METRICS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Calculate edit distances and other metrics\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "for pair in candidate_pairs:\n",
    "    # Edit distance (Levenshtein distance approximation)\n",
    "    edit_distance = len(pair['token_a']) + len(pair['token_b']) - 2 * SequenceMatcher(None, pair['token_a'], pair['token_b']).get_matching_blocks()[0].size\n",
    "    pair['edit_distance'] = edit_distance\n",
    "    \n",
    "    # Length difference\n",
    "    pair['len_a'] = len(pair['token_a'])\n",
    "    pair['len_b'] = len(pair['token_b'])\n",
    "    pair['len_diff'] = abs(pair['len_a'] - pair['len_b'])\n",
    "    \n",
    "    # Document frequency ratio (if available)\n",
    "    # This would require the original document frequencies from the canonicalization step\n",
    "    pair['df_ratio'] = 1.0  # Placeholder\n",
    "\n",
    "print(f\"  ‚úÖ Additional metrics calculated for {len(candidate_pairs):,} pairs\")\n",
    "\n",
    "### üíæ Save Results\n",
    "\n",
    "print(f\"\\nüíæ SAVING RESULTS:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create outputs directory\n",
    "outputs_dir = Path(\"romance-novel-nlp-research/src/eda_analysis/outputs\")\n",
    "outputs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save candidate pairs\n",
    "candidate_pairs_df = pd.DataFrame(candidate_pairs)\n",
    "candidate_pairs_path = outputs_dir / \"candidate_similarity_pairs.csv\"\n",
    "candidate_pairs_df.to_csv(candidate_pairs_path, index=False)\n",
    "print(f\"  ‚úÖ Saved {len(candidate_pairs):,} candidate pairs to: {candidate_pairs_path}\")\n",
    "\n",
    "# Save a sample for inspection\n",
    "sample_size = min(5000, len(candidate_pairs))\n",
    "sample_pairs = candidate_pairs_df.sample(n=sample_size, random_state=42)\n",
    "sample_path = outputs_dir / \"similarity_sample_inspection.csv\"\n",
    "sample_pairs.to_csv(sample_path, index=False)\n",
    "print(f\"  ‚úÖ Saved {sample_size:,} sample pairs to: {sample_path}\")\n",
    "\n",
    "# Save vectorizer\n",
    "vectorizer_path = outputs_dir / \"tfidf_vectorizer.pkl\"\n",
    "with open(vectorizer_path, 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "print(f\"  ‚úÖ Saved vectorizer to: {vectorizer_path}\")\n",
    "\n",
    "# Save metadata\n",
    "cell3_meta = {\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': {\n",
    "        'ngram_range': NGRAM_RANGE,\n",
    "        'min_df': MIN_DF,\n",
    "        'max_features': MAX_FEATURES,\n",
    "        'similarity_threshold': SIMILARITY_THRESHOLD,\n",
    "        'top_k_neighbors': TOP_K_NEIGHBORS\n",
    "    },\n",
    "    'stats': {\n",
    "        'total_tokens': len(canonical_tokens),\n",
    "        'total_pairs_checked': total_pairs_checked,\n",
    "        'candidate_pairs_found': len(candidate_pairs),\n",
    "        'hit_rate': len(candidate_pairs)/total_pairs_checked*100,\n",
    "        'vectorization_time': vectorize_time,\n",
    "        'ann_time': ann_time\n",
    "    },\n",
    "    'outputs': {\n",
    "        'candidate_pairs_file': str(candidate_pairs_path),\n",
    "        'sample_file': str(sample_path),\n",
    "        'vectorizer_file': str(vectorizer_path)\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = outputs_dir / \"cell3_similarity_metadata.json\"\n",
    "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(cell3_meta, f, indent=2, ensure_ascii=False)\n",
    "print(f\"  ‚úÖ Saved metadata to: {metadata_path}\")\n",
    "\n",
    "### üìä Summary Statistics\n",
    "\n",
    "print(f\"\\nüìä SUMMARY STATISTICS:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "if len(candidate_pairs) > 0:\n",
    "    similarities = [pair['cosine_sim'] for pair in candidate_pairs]\n",
    "    edit_distances = [pair['edit_distance'] for pair in candidate_pairs]\n",
    "    \n",
    "    print(f\"  Cosine similarity range: {min(similarities):.3f} - {max(similarities):.3f}\")\n",
    "    print(f\"  Cosine similarity mean: {np.mean(similarities):.3f}\")\n",
    "    print(f\"  Edit distance range: {min(edit_distances)} - {max(edit_distances)}\")\n",
    "    print(f\"  Edit distance mean: {np.mean(edit_distances):.1f}\")\n",
    "    \n",
    "    # Coverage analysis\n",
    "    unique_tokens_in_pairs = set()\n",
    "    for pair in candidate_pairs:\n",
    "        unique_tokens_in_pairs.add(pair['token_a'])\n",
    "        unique_tokens_in_pairs.add(pair['token_b'])\n",
    "    \n",
    "    coverage = len(unique_tokens_in_pairs) / len(canonical_tokens) * 100\n",
    "    print(f\"  Token coverage: {len(unique_tokens_in_pairs):,} / {len(canonical_tokens):,} ({coverage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n[{time.strftime('%H:%M:%S')}] ‚úÖ Cell 3: Character Similarity Index & Neighbor Retrieval completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Cell 4: Neighbor Similarity Sample Inspection\n",
    "\n",
    "This section loads and thoroughly inspects the neighbor similarity sample CSV file to validate the quality of our character similarity index and identify potential false positives in the candidate edges.\n",
    "\n",
    "### What this section does:\n",
    "- Loads the 5,000-row sample CSV file from Cell 3\n",
    "- Validates the schema and data structure\n",
    "- Performs comprehensive quality checks on similarity pairs\n",
    "- Identifies edge cases and potential false positives\n",
    "- Provides detailed logging for manual verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: NEIGHBOR SIMILARITY SAMPLE INSPECTION\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(f\"\\n[{time.strftime('%H:%M:%S')}] üîç Starting Cell 4: Neighbor Similarity Sample Inspection\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "### üìÇ Load Sample Data\n",
    "\n",
    "print(f\"\\nüìÇ LOADING SAMPLE DATA:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Load the sample file created in Cell 3\n",
    "outputs_dir = Path(\"romance-novel-nlp-research/src/eda_analysis/outputs\")\n",
    "sample_path = outputs_dir / \"similarity_sample_inspection.csv\"\n",
    "\n",
    "if not sample_path.exists():\n",
    "    print(f\"  ‚ùå Sample file not found: {sample_path}\")\n",
    "    print(f\"  üìù Please run Cell 3 first to generate the sample data\")\n",
    "else:\n",
    "    sample_df = pd.read_csv(sample_path)\n",
    "    print(f\"  ‚úÖ Loaded sample data: {len(sample_df):,} rows\")\n",
    "    print(f\"  üìä Columns: {list(sample_df.columns)}\")\n",
    "\n",
    "### üîç Data Quality Inspection\n",
    "\n",
    "print(f\"\\nüîç DATA QUALITY INSPECTION:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if sample_path.exists():\n",
    "    # Basic data validation\n",
    "    print(f\"üìä BASIC VALIDATION:\")\n",
    "    print(f\"  Total rows: {len(sample_df):,}\")\n",
    "    print(f\"  Null values per column:\")\n",
    "    for col in sample_df.columns:\n",
    "        null_count = sample_df[col].isnull().sum()\n",
    "        print(f\"    {col}: {null_count} ({null_count/len(sample_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Data type validation\n",
    "    print(f\"\\nüìä DATA TYPES:\")\n",
    "    print(sample_df.dtypes)\n",
    "    \n",
    "    # Sample data preview\n",
    "    print(f\"\\nüìä SAMPLE DATA PREVIEW:\")\n",
    "    print(sample_df.head(10))\n",
    "\n",
    "### üìà Quality Assessment\n",
    "\n",
    "print(f\"\\nüìà QUALITY ASSESSMENT:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "if sample_path.exists():\n",
    "    # Analyze similarity distributions\n",
    "    print(f\"üìä SIMILARITY DISTRIBUTION:\")\n",
    "    if 'cosine_sim' in sample_df.columns:\n",
    "        similarities = sample_df['cosine_sim']\n",
    "        print(f\"  Cosine similarity range: {similarities.min():.3f} - {similarities.max():.3f}\")\n",
    "        print(f\"  Cosine similarity mean: {similarities.mean():.3f}\")\n",
    "        print(f\"  Cosine similarity std: {similarities.std():.3f}\")\n",
    "        \n",
    "        # Similarity ranges\n",
    "        high_sim = (similarities >= 0.8).sum()\n",
    "        med_sim = ((similarities >= 0.5) & (similarities < 0.8)).sum()\n",
    "        low_sim = (similarities < 0.5).sum()\n",
    "        \n",
    "        print(f\"  High similarity (‚â•0.8): {high_sim:,} ({high_sim/len(sample_df)*100:.1f}%)\")\n",
    "        print(f\"  Medium similarity (0.5-0.8): {med_sim:,} ({med_sim/len(sample_df)*100:.1f}%)\")\n",
    "        print(f\"  Low similarity (<0.5): {low_sim:,} ({low_sim/len(sample_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Analyze edit distances\n",
    "    print(f\"\\nüìä EDIT DISTANCE DISTRIBUTION:\")\n",
    "    if 'edit_distance' in sample_df.columns:\n",
    "        edit_distances = sample_df['edit_distance']\n",
    "        print(f\"  Edit distance range: {edit_distances.min()} - {edit_distances.max()}\")\n",
    "        print(f\"  Edit distance mean: {edit_distances.mean():.1f}\")\n",
    "        print(f\"  Edit distance std: {edit_distances.std():.1f}\")\n",
    "        \n",
    "        # Edit distance ranges\n",
    "        low_edit = (edit_distances <= 2).sum()\n",
    "        med_edit = ((edit_distances > 2) & (edit_distances <= 5)).sum()\n",
    "        high_edit = (edit_distances > 5).sum()\n",
    "        \n",
    "        print(f\"  Low edit distance (‚â§2): {low_edit:,} ({low_edit/len(sample_df)*100:.1f}%)\")\n",
    "        print(f\"  Medium edit distance (3-5): {med_edit:,} ({med_edit/len(sample_df)*100:.1f}%)\")\n",
    "        print(f\"  High edit distance (>5): {high_edit:,} ({high_edit/len(sample_df)*100:.1f}%)\")\n",
    "\n",
    "### üîç Manual Quality Review\n",
    "\n",
    "print(f\"\\nüîç MANUAL QUALITY REVIEW:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "if sample_path.exists():\n",
    "    # Show sample pairs for manual review\n",
    "    print(f\"üìã SAMPLE PAIRS FOR MANUAL REVIEW:\")\n",
    "    print(f\"Showing first 10 pairs for quality assessment:\")\n",
    "    \n",
    "    for i, (idx, row) in enumerate(sample_df.head(10).iterrows()):\n",
    "        print(f\"\\n[{i+1}] Row {idx}:\")\n",
    "        print(f\"  Token A: '{row['token_a']}' (len: {row['len_a']})\")\n",
    "        print(f\"  Token B: '{row['token_b']}' (len: {row['len_b']})\")\n",
    "        print(f\"  Cosine similarity: {row['cosine_sim']:.4f}\")\n",
    "        print(f\"  Edit distance: {row['edit_distance']}\")\n",
    "        \n",
    "        # Quality assessment\n",
    "        if row['edit_distance'] <= 3 and row['cosine_sim'] > 0.5:\n",
    "            print(f\"  ‚úÖ Looks like valid shelf variants\")\n",
    "        elif row['edit_distance'] > 5 and row['cosine_sim'] < 0.3:\n",
    "            print(f\"  ‚ö†Ô∏è  Potential false positive - low similarity\")\n",
    "        else:\n",
    "            print(f\"  üîç Manual review needed\")\n",
    "\n",
    "### ‚ö†Ô∏è Edge Case Analysis\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  EDGE CASE ANALYSIS:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "if sample_path.exists():\n",
    "    # Edge case 1: Low cosine similarity\n",
    "    print(f\"\\nüîç EDGE CASE 1: Low Cosine Similarity (< 0.2)\")\n",
    "    print(\"-\" * 50)\n",
    "    low_cosine = sample_df[sample_df['cosine_sim'] < 0.2]\n",
    "    print(f\"üìä Found {len(low_cosine)} rows with cosine_sim < 0.2\")\n",
    "    \n",
    "    if len(low_cosine) > 0:\n",
    "        print(f\"üîç Sample of low cosine similarity pairs:\")\n",
    "        for i, (idx, row) in enumerate(low_cosine.head(5).iterrows()):\n",
    "            print(f\"  [{i+1}] '{row['token_a']}' ‚Üî '{row['token_b']}' (cosine: {row['cosine_sim']:.4f}, edit: {row['edit_distance']})\")\n",
    "    \n",
    "    # Edge case 2: Low edit distance but low cosine similarity\n",
    "    print(f\"\\nüîç EDGE CASE 2: Low Edit Distance but Low Cosine Similarity\")\n",
    "    print(\"-\" * 60)\n",
    "    low_edit_low_cosine = sample_df[(sample_df['edit_distance'] < 2) & (sample_df['cosine_sim'] < 0.3)]\n",
    "    print(f\"üìä Found {len(low_edit_low_cosine)} rows with edit_distance < 2 AND cosine_sim < 0.3\")\n",
    "    \n",
    "    if len(low_edit_low_cosine) > 0:\n",
    "        print(f\"üîç Sample of low edit distance but low cosine similarity pairs:\")\n",
    "        for i, (idx, row) in enumerate(low_edit_low_cosine.head(5).iterrows()):\n",
    "            print(f\"  [{i+1}] '{row['token_a']}' ‚Üî '{row['token_b']}' (cosine: {row['cosine_sim']:.4f}, edit: {row['edit_distance']})\")\n",
    "\n",
    "### üìä Quality Metrics Summary\n",
    "\n",
    "print(f\"\\nüìä QUALITY METRICS SUMMARY:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "if sample_path.exists():\n",
    "    total_rows = len(sample_df)\n",
    "    \n",
    "    # Calculate quality metrics\n",
    "    high_quality_pairs = len(sample_df[\n",
    "        (sample_df['cosine_sim'] >= 0.5) & \n",
    "        (sample_df['edit_distance'] <= 5)\n",
    "    ])\n",
    "    \n",
    "    potential_false_positives = len(sample_df[\n",
    "        (sample_df['cosine_sim'] < 0.3) | \n",
    "        (sample_df['edit_distance'] > 8)\n",
    "    ])\n",
    "    \n",
    "    print(f\"üìã QUALITY ASSESSMENT:\")\n",
    "    print(f\"  Total pairs analyzed: {total_rows:,}\")\n",
    "    print(f\"  High quality pairs: {high_quality_pairs:,} ({high_quality_pairs/total_rows*100:.1f}%)\")\n",
    "    print(f\"  Potential false positives: {potential_false_positives:,} ({potential_false_positives/total_rows*100:.1f}%)\")\n",
    "    \n",
    "    # Distribution analysis\n",
    "    print(f\"\\nüìä DISTRIBUTION ANALYSIS:\")\n",
    "    \n",
    "    # Cosine similarity distribution\n",
    "    cosine_ranges = [\n",
    "        (0.0, 0.2, \"Very Low\"),\n",
    "        (0.2, 0.4, \"Low\"),\n",
    "        (0.4, 0.6, \"Medium\"),\n",
    "        (0.6, 0.8, \"High\"),\n",
    "        (0.8, 1.0, \"Very High\")\n",
    "    ]\n",
    "    \n",
    "    print(f\"  Cosine Similarity Distribution:\")\n",
    "    for min_val, max_val, label in cosine_ranges:\n",
    "        count = len(sample_df[(sample_df['cosine_sim'] >= min_val) & (sample_df['cosine_sim'] < max_val)])\n",
    "        print(f\"    {label} ({min_val}-{max_val}): {count:,} ({count/total_rows*100:.1f}%)\")\n",
    "    \n",
    "    # Edit distance distribution\n",
    "    edit_ranges = [\n",
    "        (0, 1, \"Very Close\"),\n",
    "        (2, 3, \"Close\"),\n",
    "        (4, 5, \"Medium\"),\n",
    "        (6, 10, \"Far\"),\n",
    "        (11, 100, \"Very Far\")\n",
    "    ]\n",
    "    \n",
    "    print(f\"  Edit Distance Distribution:\")\n",
    "    for min_val, max_val, label in edit_ranges:\n",
    "        count = len(sample_df[(sample_df['edit_distance'] >= min_val) & (sample_df['edit_distance'] <= max_val)])\n",
    "        print(f\"    {label} ({min_val}-{max_val}): {count:,} ({count/total_rows*100:.1f}%)\")\n",
    "\n",
    "### üìã Recommendations\n",
    "\n",
    "print(f\"\\nüìã RECOMMENDATIONS BASED ON ANALYSIS:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "if sample_path.exists():\n",
    "    if potential_false_positives / total_rows > 0.1:\n",
    "        print(f\"  ‚ö†Ô∏è  High false positive rate detected - consider adjusting similarity thresholds\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ False positive rate appears acceptable\")\n",
    "    \n",
    "    if high_quality_pairs / total_rows > 0.7:\n",
    "        print(f\"  ‚úÖ High proportion of quality pairs - similarity index working well\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è  Consider tuning TF-IDF parameters or similarity thresholds\")\n",
    "    \n",
    "    # Additional recommendations\n",
    "    print(f\"\\nüìã ADDITIONAL RECOMMENDATIONS:\")\n",
    "    print(f\"  üîß Consider implementing length-based filtering\")\n",
    "    print(f\"  üîß Add domain-specific stopword filtering\")\n",
    "    print(f\"  üîß Implement confidence scoring based on multiple metrics\")\n",
    "    print(f\"  üîß Consider clustering similar pairs for batch processing\")\n",
    "\n",
    "### üíæ Save Inspection Report\n",
    "\n",
    "print(f\"\\nüíæ SAVING INSPECTION REPORT:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "if sample_path.exists():\n",
    "    # Create inspection report\n",
    "    inspection_report = {\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'sample_file': str(sample_path),\n",
    "        'total_pairs_analyzed': len(sample_df),\n",
    "        'quality_metrics': {\n",
    "            'high_quality_pairs': int(high_quality_pairs),\n",
    "            'high_quality_percentage': float(high_quality_pairs/total_rows*100),\n",
    "            'potential_false_positives': int(potential_false_positives),\n",
    "            'false_positive_percentage': float(potential_false_positives/total_rows*100)\n",
    "        },\n",
    "        'distributions': {\n",
    "            'cosine_similarity': {\n",
    "                'min': float(sample_df['cosine_sim'].min()),\n",
    "                'max': float(sample_df['cosine_sim'].max()),\n",
    "                'mean': float(sample_df['cosine_sim'].mean()),\n",
    "                'std': float(sample_df['cosine_sim'].std())\n",
    "            },\n",
    "            'edit_distance': {\n",
    "                'min': int(sample_df['edit_distance'].min()),\n",
    "                'max': int(sample_df['edit_distance'].max()),\n",
    "                'mean': float(sample_df['edit_distance'].mean()),\n",
    "                'std': float(sample_df['edit_distance'].std())\n",
    "            }\n",
    "        },\n",
    "        'recommendations': [\n",
    "            \"Consider implementing length-based filtering\",\n",
    "            \"Add domain-specific stopword filtering\", \n",
    "            \"Implement confidence scoring based on multiple metrics\",\n",
    "            \"Consider clustering similar pairs for batch processing\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    report_path = outputs_dir / \"similarity_inspection_report.json\"\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(inspection_report, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"  ‚úÖ Saved inspection report to: {report_path}\")\n",
    "\n",
    "print(f\"\\n[{time.strftime('%H:%M:%S')}] ‚úÖ Cell 4: Neighbor Similarity Sample Inspection completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
