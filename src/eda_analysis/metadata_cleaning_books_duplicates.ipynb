{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Data Loading and Initial Setup\n",
    "\n",
    "This section handles the loading of the main dataset and performs initial data exploration to understand the structure and content of our romance books data.\n",
    "\n",
    "### What this section does:\n",
    "- Loads the main final dataset from CSV file\n",
    "- Drops unnecessary columns to focus on core variables\n",
    "- Performs detailed column-by-column analysis\n",
    "- Identifies data types, missing values, and unique value counts\n",
    "- Provides sample data for initial inspection\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded main final dataset: 53349 books\n",
      "INFO:__main__:Dropped columns: []\n",
      "INFO:__main__:Replaced NaN values in series_works_count_numeric with 'stand_alone'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape after dropping columns: (53349, 19)\n",
      "\n",
      "Remaining column names:\n",
      "['work_id', 'book_id_list_en', 'title', 'publication_year', 'num_pages_median', 'description', 'language_codes_en', 'author_id', 'author_name', 'author_average_rating', 'author_ratings_count', 'series_id', 'series_title', 'ratings_count_sum', 'text_reviews_count_sum', 'average_rating_weighted_mean', 'genres_str', 'shelves_str', 'series_works_count_numeric']\n",
      "\n",
      "Data types:\n",
      "work_id                           int64\n",
      "book_id_list_en                  object\n",
      "title                            object\n",
      "publication_year                  int64\n",
      "num_pages_median                float64\n",
      "description                      object\n",
      "language_codes_en                object\n",
      "author_id                         int64\n",
      "author_name                      object\n",
      "author_average_rating           float64\n",
      "author_ratings_count              int64\n",
      "series_id                        object\n",
      "series_title                     object\n",
      "ratings_count_sum                 int64\n",
      "text_reviews_count_sum            int64\n",
      "average_rating_weighted_mean    float64\n",
      "genres_str                       object\n",
      "shelves_str                      object\n",
      "series_works_count_numeric       object\n",
      "dtype: object\n",
      "\n",
      "============================================================\n",
      "COLUMN: work_id\n",
      "============================================================\n",
      "Data type: int64\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "üîë ID COLUMN - Excluded from numerical analysis\n",
      "üîë ID COLUMN - Basic stats skipped\n",
      "Unique values: 53349\n",
      "\n",
      "============================================================\n",
      "COLUMN: book_id_list_en\n",
      "============================================================\n",
      "Data type: object\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "üîë ID COLUMN - Excluded from numerical analysis\n",
      "Unique values: 53349\n",
      "Sample values:\n",
      "  [1] ['9416', '227650', '9423', '6088685', '19826270', '2574859', '13576133', '736396', '8724433', '32084...\n",
      "  [2] ['3462', '6338758', '289110', '6386960', '17787192']\n",
      "  [3] ['110391', '6077588', '25322247', '1859059', '298367', '5466440', '11793041', '23460960', '32177084'...\n",
      "  [4] ['861326', '6077587', '25322244', '353066', '9138773', '7961903', '32812329']\n",
      "  [5] ['22649', '22655', '31107', '6560878', '2576684', '12863586']\n",
      "  [6] ['469901', '11725217', '847901', '759501', '5410943']\n",
      "  [7] ['815150', '112753', '8130560', '6988762', '1982705', '344553', '22093943']\n",
      "  [8] ['89160', '6699943', '268595', '6146611', '7909185', '3062807', '2494210', '3062808', '6994134', '30...\n",
      "  [9] ['17781', '682359', '608949', '2875448']\n",
      "  [10] ['213975', '6660825', '531717', '10334321', '2097994', '8018051', '13553494', '1280346', '3062909', ...\n",
      "  ‚ö†Ô∏è  Contains list-like strings - may need parsing\n",
      "String length stats: min=8, max=1049, mean=21.0\n",
      "\n",
      "============================================================\n",
      "COLUMN: title\n",
      "============================================================\n",
      "Data type: object\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 46352\n",
      "Sample values:\n",
      "  [1] Confessions of a Shopaholic\n",
      "  [2] The Rescue\n",
      "  [3] The Duke and I\n",
      "  [4] The Viscount Who Loved Me\n",
      "  [5] Bookends\n",
      "  [6] Mr. Perfect\n",
      "  [7] The Highlander's Touch\n",
      "  [8] Judgment in Death\n",
      "  [9] Heart of the Sea\n",
      "  [10] Witness in Death\n",
      "String length stats: min=1, max=128, mean=16.5\n",
      "\n",
      "============================================================\n",
      "COLUMN: publication_year\n",
      "============================================================\n",
      "Data type: int64\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "üìä NUMERICAL COLUMN - Valid for analysis\n",
      "Basic stats:\n",
      "  count: 53349.0\n",
      "  mean: 2012.727736227483\n",
      "  std: 3.2824543292995014\n",
      "  min: 2000.0\n",
      "  25%: 2012.0\n",
      "  50%: 2013.0\n",
      "  75%: 2015.0\n",
      "  max: 2017.0\n",
      "Value counts (low cardinality - 18 unique values):\n",
      "  2013: 8998 (16.9%)\n",
      "  2014: 8565 (16.1%)\n",
      "  2015: 7072 (13.3%)\n",
      "  2012: 6318 (11.8%)\n",
      "  2016: 5778 (10.8%)\n",
      "  2017: 3682 (6.9%)\n",
      "  2011: 3614 (6.8%)\n",
      "  2010: 2172 (4.1%)\n",
      "  2009: 1663 (3.1%)\n",
      "  2008: 1206 (2.3%)\n",
      "\n",
      "============================================================\n",
      "COLUMN: num_pages_median\n",
      "============================================================\n",
      "Data type: float64\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "üìä NUMERICAL COLUMN - Valid for analysis\n",
      "Basic stats:\n",
      "  count: 53349.0\n",
      "  mean: 260.72378113929034\n",
      "  std: 100.93586926638137\n",
      "  min: 90.0\n",
      "  25%: 187.5\n",
      "  50%: 256.0\n",
      "  75%: 328.0\n",
      "  max: 980.0\n",
      "\n",
      "============================================================\n",
      "COLUMN: description\n",
      "============================================================\n",
      "Data type: object\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 53316\n",
      "Sample values:\n",
      "  [1] Unabridged audible download; approximately 11 hours 45 minutes If you've ever paid off one credit ca...\n",
      "  [2] When confronted by raging fires or deadly accidents, volunteer fireman Taylor McAden feels compelled...\n",
      "  [3] Can there be any greater challenge to London's Ambitious Mamas than an unmarried duke? --Lady Whistl...\n",
      "  [4] Alternate cover for ISBN: 0380815575/9780380815579 1814 promises to be another eventful season, but ...\n",
      "  [5] On the heels of her national bestsellers Jemima Jand Mr. Maybe, British sensation Jane Green deliver...\n",
      "  [6] What would make the perfect man?That's the delicious topic heating up the proceedings at a certain t...\n",
      "  [7] A Warrior of Immortal Powers He was a mighty Scottish warrior who lived in a world bound by ancient ...\n",
      "  [8] 'She stood in Purgatory and studied death. The blood and the gore of it, the ferocity of its glee. I...\n",
      "  [9] The breathtaking conclusion to the New York Times bestselling trilogy that began with Jewels of the ...\n",
      "  [10] LENGTH -- 10 hrs and 51 mins Opening night at New York's New Globe Theater turns from stage scene to...\n",
      "String length stats: min=4, max=14363, mean=979.4\n",
      "\n",
      "============================================================\n",
      "COLUMN: language_codes_en\n",
      "============================================================\n",
      "Data type: object\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 1\n",
      "Sample values:\n",
      "  [1] eng\n",
      "  [2] eng\n",
      "  [3] eng\n",
      "  [4] eng\n",
      "  [5] eng\n",
      "  [6] eng\n",
      "  [7] eng\n",
      "  [8] eng\n",
      "  [9] eng\n",
      "  [10] eng\n",
      "String length stats: min=3, max=3, mean=3.0\n",
      "\n",
      "============================================================\n",
      "COLUMN: author_id\n",
      "============================================================\n",
      "Data type: int64\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "üîë ID COLUMN - Excluded from numerical analysis\n",
      "üîë ID COLUMN - Basic stats skipped\n",
      "Unique values: 17810\n",
      "\n",
      "============================================================\n",
      "COLUMN: author_name\n",
      "============================================================\n",
      "Data type: object\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 17810\n",
      "Sample values:\n",
      "  [1] Sophie Kinsella\n",
      "  [2] Nicholas Sparks\n",
      "  [3] Julia Quinn\n",
      "  [4] Julia Quinn\n",
      "  [5] Jane Green\n",
      "  [6] Linda Howard\n",
      "  [7] Karen Marie Moning\n",
      "  [8] J.D. Robb\n",
      "  [9] Nora Roberts\n",
      "  [10] J.D. Robb\n",
      "String length stats: min=2, max=50, mean=12.9\n",
      "\n",
      "============================================================\n",
      "COLUMN: author_average_rating\n",
      "============================================================\n",
      "Data type: float64\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "üìä NUMERICAL COLUMN - Valid for analysis\n",
      "Basic stats:\n",
      "  count: 53349.0\n",
      "  mean: 3.9088455266265534\n",
      "  std: 0.25981381851075186\n",
      "  min: 1.27\n",
      "  25%: 3.75\n",
      "  50%: 3.92\n",
      "  75%: 4.08\n",
      "  max: 5.0\n",
      "\n",
      "============================================================\n",
      "COLUMN: author_ratings_count\n",
      "============================================================\n",
      "Data type: int64\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "üìä NUMERICAL COLUMN - Valid for analysis\n",
      "Basic stats:\n",
      "  count: 53349.0\n",
      "  mean: 48181.19623610564\n",
      "  std: 193397.0028438366\n",
      "  min: 1.0\n",
      "  25%: 843.0\n",
      "  50%: 5101.0\n",
      "  75%: 25548.0\n",
      "  max: 5280268.0\n",
      "\n",
      "============================================================\n",
      "COLUMN: series_id\n",
      "============================================================\n",
      "Data type: object\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "üîë ID COLUMN - Excluded from numerical analysis\n",
      "Unique values: 36066\n",
      "Sample values:\n",
      "  [1] 165735.0\n",
      "  [2] stand_alone\n",
      "  [3] 153045.0\n",
      "  [4] 144491.0\n",
      "  [5] stand_alone\n",
      "  [6] stand_alone\n",
      "  [7] 288256.0\n",
      "  [8] 145625.0\n",
      "  [9] 167116.0\n",
      "  [10] 162517.0\n",
      "String length stats: min=8, max=11, mean=9.0\n",
      "\n",
      "============================================================\n",
      "COLUMN: series_title\n",
      "============================================================\n",
      "Data type: object\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 17013\n",
      "Sample values:\n",
      "  [1] Shopaholic\n",
      "  [2] stand_alone\n",
      "  [3] Bridgertons\n",
      "  [4] Bridgertons\n",
      "  [5] stand_alone\n",
      "  [6] stand_alone\n",
      "  [7] Highlander\n",
      "  [8] In Death\n",
      "  [9] Gallaghers of Ardmore\n",
      "  [10] In Death\n",
      "String length stats: min=2, max=76, mean=13.7\n",
      "\n",
      "============================================================\n",
      "COLUMN: ratings_count_sum\n",
      "============================================================\n",
      "Data type: int64\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "üìä NUMERICAL COLUMN - Valid for analysis\n",
      "Basic stats:\n",
      "  count: 53349.0\n",
      "  mean: 1734.0819134379276\n",
      "  std: 14322.839048619157\n",
      "  min: 1.0\n",
      "  25%: 48.0\n",
      "  50%: 182.0\n",
      "  75%: 802.0\n",
      "  max: 1686868.0\n",
      "\n",
      "============================================================\n",
      "COLUMN: text_reviews_count_sum\n",
      "============================================================\n",
      "Data type: int64\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "üìä NUMERICAL COLUMN - Valid for analysis\n",
      "Basic stats:\n",
      "  count: 53349.0\n",
      "  mean: 141.86205927008942\n",
      "  std: 739.997994905451\n",
      "  min: 0.0\n",
      "  25%: 11.0\n",
      "  50%: 32.0\n",
      "  75%: 97.0\n",
      "  max: 74298.0\n",
      "\n",
      "============================================================\n",
      "COLUMN: average_rating_weighted_mean\n",
      "============================================================\n",
      "Data type: float64\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "üìä NUMERICAL COLUMN - Valid for analysis\n",
      "Basic stats:\n",
      "  count: 53349.0\n",
      "  mean: 3.9148518636413057\n",
      "  std: 0.34712338801084197\n",
      "  min: 1.27\n",
      "  25%: 3.71\n",
      "  50%: 3.93\n",
      "  75%: 4.15\n",
      "  max: 5.0\n",
      "\n",
      "============================================================\n",
      "COLUMN: genres_str\n",
      "============================================================\n",
      "Data type: object\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 137\n",
      "Sample values:\n",
      "  [1] fiction,romance,young adult\n",
      "  [2] fiction,mystery,romance,young adult\n",
      "  [3] biography,fiction,historical fiction,history,romance\n",
      "  [4] biography,fiction,historical fiction,history,romance\n",
      "  [5] fiction,romance\n",
      "  [6] fiction,mystery,romance\n",
      "  [7] biography,fantasy,fiction,historical fiction,history,paranormal,romance\n",
      "  [8] fantasy,fiction,mystery,paranormal,romance\n",
      "  [9] biography,fantasy,fiction,historical fiction,history,mystery,paranormal,romance\n",
      "  [10] fantasy,fiction,mystery,paranormal,romance\n",
      "String length stats: min=7, max=107, mean=28.0\n",
      "\n",
      "============================================================\n",
      "COLUMN: shelves_str\n",
      "============================================================\n",
      "Data type: object\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 53346\n",
      "Sample values:\n",
      "  [1] 3-stars,5-stars,abandoned,adult-fiction,audio,audiobook,audiobooks,beach-read,beach-reads,book-club,...\n",
      "  [2] 2000,2001,2012-reads,adult,adult-fiction,already-read,audio,audio-book,audio-books,audiobook,audiobo...\n",
      "  [3] 19th-century,1st-in-series,2012-reads,2016-reads,3-stars,4-stars,5-stars,adult,adult-fiction,adult-r...\n",
      "  [4] 1,19th-century,2016-reads,3-stars,4-stars,5-stars,adult,adult-fiction,adult-romance,all-time-favorit...\n",
      "  [5] 2002,2003,2004,2005,2006,5-stars,abandoned,adult,adult-fiction,audiobooks,beach-read,beach-reads,bea...\n",
      "  [6] 3-stars,4-stars,5-star,5-stars,aar-top-100,action,adult,adult-fiction,adult-romance,all-time-favorit...\n",
      "  [7] 4-star,4-stars,5-stars,adult,adult-fiction,adult-romance,alpha-male,audible,audio,audio-book,audio-b...\n",
      "  [8] 2015-reads,4-stars,5-stars,action,adult,adult-fiction,audible,audio,audio-book,audio-books,audiobook...\n",
      "  [9] adult-fiction,adult-romance,audible,audio,audiobook,audiobooks,author-nora-roberts,books,books-i-hav...\n",
      "  [10] 4-stars,5-stars,action,adult,adult-fiction,audible,audio,audio-book,audio-books,audiobook,audiobooks...\n",
      "String length stats: min=7, max=2283, mean=1019.0\n",
      "\n",
      "============================================================\n",
      "COLUMN: series_works_count_numeric\n",
      "============================================================\n",
      "Data type: object\n",
      "Non-null count: 53349 / 53349 (100.0%)\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 70\n",
      "Sample values:\n",
      "  [1] 12.0\n",
      "  [2] stand_alone\n",
      "  [3] 19.0\n",
      "  [4] 19.0\n",
      "  [5] stand_alone\n",
      "  [6] stand_alone\n",
      "  [7] 12.0\n",
      "  [8] 115.0\n",
      "  [9] 6.0\n",
      "  [10] 115.0\n",
      "String length stats: min=3, max=11, mean=5.7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_id</th>\n",
       "      <th>book_id_list_en</th>\n",
       "      <th>title</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>num_pages_median</th>\n",
       "      <th>description</th>\n",
       "      <th>language_codes_en</th>\n",
       "      <th>author_id</th>\n",
       "      <th>author_name</th>\n",
       "      <th>author_average_rating</th>\n",
       "      <th>author_ratings_count</th>\n",
       "      <th>series_id</th>\n",
       "      <th>series_title</th>\n",
       "      <th>ratings_count_sum</th>\n",
       "      <th>text_reviews_count_sum</th>\n",
       "      <th>average_rating_weighted_mean</th>\n",
       "      <th>genres_str</th>\n",
       "      <th>shelves_str</th>\n",
       "      <th>series_works_count_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3237433</td>\n",
       "      <td>['9416', '227650', '9423', '6088685', '1982627...</td>\n",
       "      <td>Confessions of a Shopaholic</td>\n",
       "      <td>2000</td>\n",
       "      <td>320.0</td>\n",
       "      <td>Unabridged audible download; approximately 11 ...</td>\n",
       "      <td>eng</td>\n",
       "      <td>6160</td>\n",
       "      <td>Sophie Kinsella</td>\n",
       "      <td>3.74</td>\n",
       "      <td>2169284</td>\n",
       "      <td>165735.0</td>\n",
       "      <td>Shopaholic</td>\n",
       "      <td>555675</td>\n",
       "      <td>10488</td>\n",
       "      <td>3.62</td>\n",
       "      <td>fiction,romance,young adult</td>\n",
       "      <td>3-stars,5-stars,abandoned,adult-fiction,audio,...</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1268663</td>\n",
       "      <td>['3462', '6338758', '289110', '6386960', '1778...</td>\n",
       "      <td>The Rescue</td>\n",
       "      <td>2000</td>\n",
       "      <td>372.0</td>\n",
       "      <td>When confronted by raging fires or deadly acci...</td>\n",
       "      <td>eng</td>\n",
       "      <td>2345</td>\n",
       "      <td>Nicholas Sparks</td>\n",
       "      <td>4.06</td>\n",
       "      <td>4600277</td>\n",
       "      <td>stand_alone</td>\n",
       "      <td>stand_alone</td>\n",
       "      <td>148062</td>\n",
       "      <td>3150</td>\n",
       "      <td>4.10</td>\n",
       "      <td>fiction,mystery,romance,young adult</td>\n",
       "      <td>2000,2001,2012-reads,adult,adult-fiction,alrea...</td>\n",
       "      <td>stand_alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>846763</td>\n",
       "      <td>['110391', '6077588', '25322247', '1859059', '...</td>\n",
       "      <td>The Duke and I</td>\n",
       "      <td>2000</td>\n",
       "      <td>371.0</td>\n",
       "      <td>Can there be any greater challenge to London's...</td>\n",
       "      <td>eng</td>\n",
       "      <td>63898</td>\n",
       "      <td>Julia Quinn</td>\n",
       "      <td>3.98</td>\n",
       "      <td>567004</td>\n",
       "      <td>153045.0</td>\n",
       "      <td>Bridgertons</td>\n",
       "      <td>61848</td>\n",
       "      <td>2444</td>\n",
       "      <td>4.11</td>\n",
       "      <td>biography,fiction,historical fiction,history,r...</td>\n",
       "      <td>19th-century,1st-in-series,2012-reads,2016-rea...</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3363</td>\n",
       "      <td>['861326', '6077587', '25322244', '353066', '9...</td>\n",
       "      <td>The Viscount Who Loved Me</td>\n",
       "      <td>2000</td>\n",
       "      <td>381.0</td>\n",
       "      <td>Alternate cover for ISBN: 0380815575/978038081...</td>\n",
       "      <td>eng</td>\n",
       "      <td>63898</td>\n",
       "      <td>Julia Quinn</td>\n",
       "      <td>3.98</td>\n",
       "      <td>567004</td>\n",
       "      <td>144491.0</td>\n",
       "      <td>Bridgertons</td>\n",
       "      <td>38086</td>\n",
       "      <td>1404</td>\n",
       "      <td>4.19</td>\n",
       "      <td>biography,fiction,historical fiction,history,r...</td>\n",
       "      <td>1,19th-century,2016-reads,3-stars,4-stars,5-st...</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2363</td>\n",
       "      <td>['22649', '22655', '31107', '6560878', '257668...</td>\n",
       "      <td>Bookends</td>\n",
       "      <td>2000</td>\n",
       "      <td>368.0</td>\n",
       "      <td>On the heels of her national bestsellers Jemim...</td>\n",
       "      <td>eng</td>\n",
       "      <td>12915</td>\n",
       "      <td>Jane Green</td>\n",
       "      <td>3.58</td>\n",
       "      <td>502125</td>\n",
       "      <td>stand_alone</td>\n",
       "      <td>stand_alone</td>\n",
       "      <td>34139</td>\n",
       "      <td>842</td>\n",
       "      <td>3.70</td>\n",
       "      <td>fiction,romance</td>\n",
       "      <td>2002,2003,2004,2005,2006,5-stars,abandoned,adu...</td>\n",
       "      <td>stand_alone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   work_id                                    book_id_list_en  \\\n",
       "0  3237433  ['9416', '227650', '9423', '6088685', '1982627...   \n",
       "1  1268663  ['3462', '6338758', '289110', '6386960', '1778...   \n",
       "2   846763  ['110391', '6077588', '25322247', '1859059', '...   \n",
       "3     3363  ['861326', '6077587', '25322244', '353066', '9...   \n",
       "4     2363  ['22649', '22655', '31107', '6560878', '257668...   \n",
       "\n",
       "                         title  publication_year  num_pages_median  \\\n",
       "0  Confessions of a Shopaholic              2000             320.0   \n",
       "1                   The Rescue              2000             372.0   \n",
       "2               The Duke and I              2000             371.0   \n",
       "3    The Viscount Who Loved Me              2000             381.0   \n",
       "4                     Bookends              2000             368.0   \n",
       "\n",
       "                                         description language_codes_en  \\\n",
       "0  Unabridged audible download; approximately 11 ...               eng   \n",
       "1  When confronted by raging fires or deadly acci...               eng   \n",
       "2  Can there be any greater challenge to London's...               eng   \n",
       "3  Alternate cover for ISBN: 0380815575/978038081...               eng   \n",
       "4  On the heels of her national bestsellers Jemim...               eng   \n",
       "\n",
       "   author_id      author_name  author_average_rating  author_ratings_count  \\\n",
       "0       6160  Sophie Kinsella                   3.74               2169284   \n",
       "1       2345  Nicholas Sparks                   4.06               4600277   \n",
       "2      63898      Julia Quinn                   3.98                567004   \n",
       "3      63898      Julia Quinn                   3.98                567004   \n",
       "4      12915       Jane Green                   3.58                502125   \n",
       "\n",
       "     series_id series_title  ratings_count_sum  text_reviews_count_sum  \\\n",
       "0     165735.0   Shopaholic             555675                   10488   \n",
       "1  stand_alone  stand_alone             148062                    3150   \n",
       "2     153045.0  Bridgertons              61848                    2444   \n",
       "3     144491.0  Bridgertons              38086                    1404   \n",
       "4  stand_alone  stand_alone              34139                     842   \n",
       "\n",
       "   average_rating_weighted_mean  \\\n",
       "0                          3.62   \n",
       "1                          4.10   \n",
       "2                          4.11   \n",
       "3                          4.19   \n",
       "4                          3.70   \n",
       "\n",
       "                                          genres_str  \\\n",
       "0                        fiction,romance,young adult   \n",
       "1                fiction,mystery,romance,young adult   \n",
       "2  biography,fiction,historical fiction,history,r...   \n",
       "3  biography,fiction,historical fiction,history,r...   \n",
       "4                                    fiction,romance   \n",
       "\n",
       "                                         shelves_str  \\\n",
       "0  3-stars,5-stars,abandoned,adult-fiction,audio,...   \n",
       "1  2000,2001,2012-reads,adult,adult-fiction,alrea...   \n",
       "2  19th-century,1st-in-series,2012-reads,2016-rea...   \n",
       "3  1,19th-century,2016-reads,3-stars,4-stars,5-st...   \n",
       "4  2002,2003,2004,2005,2006,5-stars,abandoned,adu...   \n",
       "\n",
       "  series_works_count_numeric  \n",
       "0                       12.0  \n",
       "1                stand_alone  \n",
       "2                       19.0  \n",
       "3                       19.0  \n",
       "4                stand_alone  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "### üîÑ Dataset Loading and Column Management\n",
    "\n",
    "# Load dataset\n",
    "main_final_path = \"../../data/processed/romance_books_main_final.csv\"\n",
    "main_final = pd.read_csv(main_final_path)\n",
    "logger.info(f\"Loaded main final dataset: {len(main_final)} books\")\n",
    "\n",
    "# Drop specified columns\n",
    "columns_to_drop = ['series_works_count', 'popular_shelves', 'genres', 'decade', \n",
    "                   'book_length_category', 'rating_category', 'popularity_category', \n",
    "                   'has_collection_indicators']\n",
    "main_final = main_final.drop(columns=columns_to_drop, errors='ignore')\n",
    "logger.info(f\"Dropped columns: {[col for col in columns_to_drop if col in main_final.columns]}\")\n",
    "\n",
    "# Clean series_works_count_numeric: replace NaN with 'stand_alone'\n",
    "main_final['series_works_count_numeric'] = main_final['series_works_count_numeric'].fillna('stand_alone')\n",
    "logger.info(f\"Replaced NaN values in series_works_count_numeric with 'stand_alone'\")\n",
    "\n",
    "### üìã Basic Dataset Information\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Dataset shape after dropping columns: {main_final.shape}\")\n",
    "print(f\"\\nRemaining column names:\")\n",
    "print(main_final.columns.tolist())\n",
    "print(f\"\\nData types:\")\n",
    "print(main_final.dtypes)\n",
    "\n",
    "### üîç Detailed Column Investigation\n",
    "\n",
    "# Define ID columns to exclude from numerical analysis\n",
    "id_columns = ['work_id', 'book_id_list_en', 'author_id', 'series_id']\n",
    "\n",
    "for col in main_final.columns:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COLUMN: {col}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"Data type: {main_final[col].dtype}\")\n",
    "    print(f\"Non-null count: {main_final[col].count()} / {len(main_final)} ({main_final[col].count()/len(main_final)*100:.1f}%)\")\n",
    "    print(f\"Null count: {main_final[col].isnull().sum()} ({main_final[col].isnull().sum()/len(main_final)*100:.1f}%)\")\n",
    "    \n",
    "    # Mark ID columns\n",
    "    if col in id_columns:\n",
    "        print(\"üîë ID COLUMN - Excluded from numerical analysis\")\n",
    "    \n",
    "    # Type-specific analysis\n",
    "    if main_final[col].dtype in ['object']:\n",
    "        print(f\"Unique values: {main_final[col].nunique()}\")\n",
    "        print(f\"Sample values:\")\n",
    "        sample_values = main_final[col].dropna().head(10).tolist()\n",
    "        for i, val in enumerate(sample_values):\n",
    "            val_str = str(val)\n",
    "            if len(val_str) > 100:\n",
    "                val_str = val_str[:100] + \"...\"\n",
    "            print(f\"  [{i+1}] {val_str}\")\n",
    "        \n",
    "        # Check for list-like strings\n",
    "        if any(main_final[col].dropna().astype(str).str.startswith('[').head(100)):\n",
    "            print(\"  ‚ö†Ô∏è  Contains list-like strings - may need parsing\")\n",
    "        \n",
    "        # Value length distribution for string columns\n",
    "        lengths = main_final[col].dropna().astype(str).str.len()\n",
    "        print(f\"String length stats: min={lengths.min()}, max={lengths.max()}, mean={lengths.mean():.1f}\")\n",
    "        \n",
    "    elif main_final[col].dtype in ['int64', 'float64'] and col not in id_columns:\n",
    "        print(f\"üìä NUMERICAL COLUMN - Valid for analysis\")\n",
    "        print(f\"Basic stats:\")\n",
    "        stats = main_final[col].describe()\n",
    "        for stat_name, stat_val in stats.items():\n",
    "            print(f\"  {stat_name}: {stat_val}\")\n",
    "        \n",
    "        # Check for potential categorical numeric columns\n",
    "        unique_count = main_final[col].nunique()\n",
    "        if unique_count <= 20:\n",
    "            print(f\"Value counts (low cardinality - {unique_count} unique values):\")\n",
    "            vc = main_final[col].value_counts().head(10)\n",
    "            for val, count in vc.items():\n",
    "                print(f\"  {val}: {count} ({count/len(main_final)*100:.1f}%)\")\n",
    "    \n",
    "    elif main_final[col].dtype in ['int64', 'float64'] and col in id_columns:\n",
    "        print(f\"üîë ID COLUMN - Basic stats skipped\")\n",
    "        unique_count = main_final[col].nunique()\n",
    "        print(f\"Unique values: {unique_count}\")\n",
    "        \n",
    "    elif main_final[col].dtype in ['bool']:\n",
    "        print(f\"Boolean distribution:\")\n",
    "        vc = main_final[col].value_counts()\n",
    "        for val, count in vc.items():\n",
    "            print(f\"  {val}: {count} ({count/len(main_final)*100:.1f}%)\")\n",
    "\n",
    "### üìä Sample Data Preview\n",
    "\n",
    "main_final.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Universal String Canonicalization\n",
    "\n",
    "This section performs comprehensive canonicalization of genre and shelf strings to create standardized, normalized versions for consistent analysis and comparison.\n",
    "\n",
    "### What this section does:\n",
    "- Applies consistent normalization rules to all genre and shelf strings\n",
    "- Creates canonical mappings between original and normalized forms\n",
    "- Handles case normalization, whitespace cleaning, and separator standardization\n",
    "- Generates comprehensive statistics on transformation patterns\n",
    "- Prepares clean, standardized data for downstream similarity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:üìã CONFIGURATION (Embeddings + Tuning)\n",
      "INFO:__main__:  Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:__main__:  Batch size: 1024  | Normalize: True\n",
      "INFO:__main__:  HNSW: M=16, efC=200, efQ=200\n",
      "INFO:__main__:  Top-K neighbors: 50\n",
      "INFO:__main__:  Tokens: 254,778\n",
      "INFO:__main__:  Tuning: True | CSV: None | minP=0.9 | minR=None\n",
      "INFO:__main__:üíæ Memory usage: 3.90 GB\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:üöÄ LOADING EMBEDDING MODEL\n",
      "INFO:__main__:============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã CONFIGURATION (Embeddings + Tuning)\n",
      "  Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "  Batch size: 1024  | Normalize: True\n",
      "  HNSW: M=16, efC=200, efQ=200\n",
      "  Top-K neighbors: 50\n",
      "  Tokens: 254,778\n",
      "  Tuning: True | CSV: None | minP=0.9 | minR=None\n",
      "üíæ Memory usage: 3.90 GB\n",
      "============================================================\n",
      "üöÄ LOADING EMBEDDING MODEL\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8d7e8ffb964da996a1195cff59beb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4693050d83a41898f53e7e6cdccda77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9369262bd784e5daf7b0d5f27f3f80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4001f6295e65419b9c70caa470691746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56cef0c62089432ba35646c778ed7a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b80d54151e740d98c6a436dc87d1a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25751d1a0d8b45ac97ecd7aa07619930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356af55dbd5543d8a48840f5235fb705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00a90274fdb4fe596074ea2d8aea697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18d96bae1214a688d0b34b68c1c5829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec14e5eb93d34457a888ee7ccf8688ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:‚úÖ Model loaded. Embedding dim: 384\n",
      "INFO:__main__:Model load: 22.55s\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:üöÄ THRESHOLD TUNING (QUICK EVAL)\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:‚ö†Ô∏è  No eval pairs found. Skipping tuning and keeping configured threshold.\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:üöÄ EMBEDDING CANONICAL TOKENS\n",
      "INFO:__main__:============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded. Embedding dim: 384\n",
      "‚è±Ô∏è Model load took 22.55 s\n",
      "============================================================\n",
      "üöÄ THRESHOLD TUNING (QUICK EVAL)\n",
      "============================================================\n",
      "‚ö†Ô∏è  No eval pairs found. Skipping tuning and keeping configured threshold.\n",
      "============================================================\n",
      "üöÄ EMBEDDING CANONICAL TOKENS\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d236042cd74a808d09d358c141be2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Embedding: 70.72s\n",
      "INFO:__main__:‚úÖ Embeddings: 254,778 √ó 384 (float32)\n",
      "INFO:__main__:üíæ Memory usage: 5.61 GB\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:üöÄ BUILDING ANN INDEX & RETRIEVING NEIGHBORS\n",
      "INFO:__main__:============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Embedding took 70.72 s\n",
      "‚úÖ Embeddings: 254,778 √ó 384 (float32)\n",
      "üíæ Memory usage: 5.61 GB\n",
      "============================================================\n",
      "üöÄ BUILDING ANN INDEX & RETRIEVING NEIGHBORS\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:HNSW build + KNN: 160.52s\n",
      "INFO:__main__:‚úÖ ANN completed\n",
      "INFO:__main__:üíæ Memory usage: 6.12 GB\n",
      "INFO:__main__:üîé Using similarity threshold: 0.300\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:üöÄ GENERATING CANDIDATE PAIRS\n",
      "INFO:__main__:============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è HNSW build + KNN took 160.52 s\n",
      "‚úÖ ANN completed\n",
      "üíæ Memory usage: 6.12 GB\n",
      "üîé Using similarity threshold: 0.300\n",
      "============================================================\n",
      "üöÄ GENERATING CANDIDATE PAIRS\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   18.6s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   43.3s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed:  6.8min\n"
     ]
    }
   ],
   "source": [
    "# romance-novel-nlp-research/src/eda_analysis/cell3_similarity_embeddings_tuned.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Iterable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "from joblib import Parallel, delayed\n",
    "from difflib import SequenceMatcher\n",
    "import hnswlib\n",
    "\n",
    "# -----------------------\n",
    "# Logging\n",
    "# -----------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.FileHandler(\"cell3_pipeline.log\", mode=\"w\"), logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def log_info(msg: str) -> None:\n",
    "    print(msg); logger.info(msg)\n",
    "\n",
    "def log_stage(stage: str) -> None:\n",
    "    sep = \"=\" * 60\n",
    "    print(sep); print(f\"üöÄ {stage}\"); print(sep)\n",
    "    logger.info(sep); logger.info(f\"üöÄ {stage}\"); logger.info(sep)\n",
    "\n",
    "def log_time(label: str, start: float) -> None:\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"‚è±Ô∏è {label} took {elapsed:.2f} s\"); logger.info(f\"{label}: {elapsed:.2f}s\")\n",
    "\n",
    "def log_memory() -> None:\n",
    "    mem_gb = psutil.Process(os.getpid()).memory_info().rss / (1024**3)\n",
    "    msg = f\"üíæ Memory usage: {mem_gb:.2f} GB\"\n",
    "    print(msg); logger.info(msg)\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "MODEL_NAME: str = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384-d, fast\n",
    "USE_MULTILINGUAL: bool = False\n",
    "BATCH_SIZE: int = 1024\n",
    "NORMALIZE: bool = True\n",
    "\n",
    "TOP_K_NEIGHBORS: int = 50\n",
    "SIMILARITY_THRESHOLD: float = 0.30  # initial; will be overridden if tuning enabled\n",
    "\n",
    "# HNSW\n",
    "HNSW_M: int = 16\n",
    "HNSW_EF_CONSTRUCTION: int = 200\n",
    "HNSW_EF: int = 200\n",
    "QUERY_BATCH_SIZE: int = 25000\n",
    "\n",
    "# ---- Threshold tuning ----\n",
    "ENABLE_THRESHOLD_TUNING: bool = True\n",
    "EVAL_CSV_PATH: Optional[str] = None  # CSV with columns: token_a, token_b, label (1/0)\n",
    "EVAL_MIN_PRECISION: Optional[float] = 0.90  # set None to disable constraint\n",
    "EVAL_MIN_RECALL: Optional[float] = None     # set None to disable constraint\n",
    "EVAL_GRID_START: float = 0.05\n",
    "EVAL_GRID_END: float = 0.95\n",
    "EVAL_GRID_STEP: float = 0.01\n",
    "\n",
    "# In-memory eval fallback (used if CSV not provided). Define in the notebook before running:\n",
    "# eval_positive_pairs = [(\"shelf a\", \"shelf a \"), ...]\n",
    "# eval_negative_pairs = [(\"shelf a\", \"shelf z\"), ...]\n",
    "# They are optional; if neither CSV nor lists exist, tuning is skipped.\n",
    "\n",
    "OUTPUTS_DIR = Path(\"romance-novel-nlp-research/src/eda_analysis/outputs\")\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------\n",
    "# Inputs (from previous cells)\n",
    "# -----------------------\n",
    "try:\n",
    "    canonical_tokens: List[str] = list(unique_canonical_shelves)  # noqa: F821\n",
    "except NameError as e:\n",
    "    raise RuntimeError(\"Expected `unique_canonical_shelves` to be defined in previous cells.\") from e\n",
    "\n",
    "# -----------------------\n",
    "# Embedding utils\n",
    "# -----------------------\n",
    "def load_embedder(model_name: str):\n",
    "    \"\"\"Load Sentence-Transformers model.\"\"\"\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "    except Exception as exc:\n",
    "        raise RuntimeError(\"Install with: pip install -U sentence-transformers\") from exc\n",
    "    device = \"cuda\" if os.environ.get(\"USE_CUDA\", \"1\") == \"1\" else \"cpu\"\n",
    "    return SentenceTransformer(model_name, device=device)\n",
    "\n",
    "def encode_texts(embedder, texts: List[str], batch_size: int, normalize_vecs: bool) -> np.ndarray:\n",
    "    \"\"\"Batched encode; returns float32.\"\"\"\n",
    "    return embedder.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=normalize_vecs\n",
    "    ).astype(np.float32, copy=False)\n",
    "\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Assumes L2-normalized rows; returns dot-product cosine.\"\"\"\n",
    "    return (a * b).sum(axis=1)\n",
    "\n",
    "# -----------------------\n",
    "# HNSW utils\n",
    "# -----------------------\n",
    "def build_hnsw(vectors: np.ndarray, m: int, ef_c: int, ef_q: int, topk: int):\n",
    "    index = hnswlib.Index(space=\"cosine\", dim=vectors.shape[1])\n",
    "    index.init_index(max_elements=vectors.shape[0], ef_construction=ef_c, M=m)\n",
    "    index.set_ef(max(ef_q, topk))\n",
    "    index.add_items(vectors, np.arange(vectors.shape[0], dtype=np.int32))\n",
    "    return index\n",
    "\n",
    "def knn_query_batched(index, vectors: np.ndarray, k: int, batch: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    n = vectors.shape[0]\n",
    "    all_idx = np.empty((n, k), dtype=np.int32)\n",
    "    all_dist = np.empty((n, k), dtype=np.float32)\n",
    "    s = 0\n",
    "    while s < n:\n",
    "        e = min(s + batch, n)\n",
    "        idx, dist = index.knn_query(vectors[s:e], k=k)\n",
    "        all_idx[s:e] = idx\n",
    "        all_dist[s:e] = dist\n",
    "        s = e\n",
    "    return all_idx, all_dist\n",
    "\n",
    "# -----------------------\n",
    "# Eval / threshold tuning\n",
    "# -----------------------\n",
    "def load_eval_pairs_from_csv(path: str) -> Tuple[List[Tuple[str,str]], List[Tuple[str,str]]]:\n",
    "    df = pd.read_csv(path)\n",
    "    # Flexible column names\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    a = cols.get(\"token_a\", cols.get(\"a\", None))\n",
    "    b = cols.get(\"token_b\", cols.get(\"b\", None))\n",
    "    y = cols.get(\"label\", cols.get(\"y\", None))\n",
    "    if not (a and b and y):\n",
    "        raise ValueError(\"CSV must have columns token_a, token_b, label (1/0).\")\n",
    "    pos = [(x, y_) for x, y_, lbl in zip(df[a], df[b], df[y]) if int(lbl) == 1]\n",
    "    neg = [(x, y_) for x, y_, lbl in zip(df[a], df[b], df[y]) if int(lbl) == 0]\n",
    "    return pos, neg\n",
    "\n",
    "def gather_eval_pairs_from_namespace() -> Tuple[List[Tuple[str,str]], List[Tuple[str,str]]]:\n",
    "    \"\"\"Pick up eval pairs if user defined `eval_positive_pairs` / `eval_negative_pairs` in notebook.\"\"\"\n",
    "    pos, neg = [], []\n",
    "    g = globals()\n",
    "    if \"eval_positive_pairs\" in g and isinstance(g[\"eval_positive_pairs\"], Iterable):\n",
    "        pos = [(str(a), str(b)) for a, b in g[\"eval_positive_pairs\"]]\n",
    "    if \"eval_negative_pairs\" in g and isinstance(g[\"eval_negative_pairs\"], Iterable):\n",
    "        neg = [(str(a), str(b)) for a, b in g[\"eval_negative_pairs\"]]\n",
    "    return pos, neg\n",
    "\n",
    "def embed_eval_pairs(embedder, pairs_pos: List[Tuple[str,str]], pairs_neg: List[Tuple[str,str]], batch_size: int, normalize_vecs: bool) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Embed unique strings once; return (scores, labels).\"\"\"\n",
    "    all_pairs = pairs_pos + pairs_neg\n",
    "    if not all_pairs:\n",
    "        return np.array([]), np.array([])\n",
    "    uniq: Dict[str, int] = {}\n",
    "    strings: List[str] = []\n",
    "    for a, b in all_pairs:\n",
    "        if a not in uniq:\n",
    "            uniq[a] = len(strings); strings.append(a)\n",
    "        if b not in uniq:\n",
    "            uniq[b] = len(strings); strings.append(b)\n",
    "    mat = encode_texts(embedder, strings, batch_size, normalize_vecs)  # normalized\n",
    "    scores = np.empty(len(all_pairs), dtype=np.float32)\n",
    "    labels = np.empty(len(all_pairs), dtype=np.int32)\n",
    "    for i, (a, b) in enumerate(all_pairs):\n",
    "        va = mat[uniq[a]][None, :]\n",
    "        vb = mat[uniq[b]][None, :]\n",
    "        scores[i] = cosine_sim(va, vb)[0]\n",
    "        labels[i] = 1 if i < len(pairs_pos) else 0\n",
    "    return scores, labels\n",
    "\n",
    "def precision_recall_f1(y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[float, float, float]:\n",
    "    tp = int(((y_true == 1) & (y_pred == 1)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred == 0)).sum())\n",
    "    prec = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    rec = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    f1 = 2 * prec * rec / (prec + rec) if (prec + rec) else 0.0\n",
    "    return prec, rec, f1\n",
    "\n",
    "def sweep_thresholds(scores: np.ndarray, labels: np.ndarray, start: float, end: float, step: float) -> pd.DataFrame:\n",
    "    \"\"\"Return metrics across thresholds.\"\"\"\n",
    "    if scores.size == 0:\n",
    "        return pd.DataFrame(columns=[\"threshold\",\"precision\",\"recall\",\"f1\",\"support_pos\",\"support_neg\"])\n",
    "    thrs = np.arange(start, end + 1e-9, step, dtype=np.float32)\n",
    "    rows = []\n",
    "    pos_cnt = int((labels == 1).sum())\n",
    "    neg_cnt = int((labels == 0).sum())\n",
    "    for t in thrs:\n",
    "        y_pred = (scores >= t).astype(np.int32)\n",
    "        p, r, f1 = precision_recall_f1(labels, y_pred)\n",
    "        rows.append((float(t), p, r, f1, pos_cnt, neg_cnt))\n",
    "    return pd.DataFrame(rows, columns=[\"threshold\",\"precision\",\"recall\",\"f1\",\"support_pos\",\"support_neg\"])\n",
    "\n",
    "def pick_threshold(metrics: pd.DataFrame,\n",
    "                   min_precision: Optional[float],\n",
    "                   min_recall: Optional[float]) -> Dict[str, float]:\n",
    "    \"\"\"Return chosen thresholds based on F1 and constraints.\"\"\"\n",
    "    if metrics.empty:\n",
    "        return {\"best_f1\": None, \"best_at_min_precision\": None, \"best_at_min_recall\": None}\n",
    "    # Best F1 (global)\n",
    "    best_f1_row = metrics.iloc[metrics[\"f1\"].values.argmax()]\n",
    "    best_f1 = float(best_f1_row[\"threshold\"])\n",
    "    # Best under min precision\n",
    "    best_at_min_p = None\n",
    "    if isinstance(min_precision, float):\n",
    "        sub = metrics[metrics[\"precision\"] >= min_precision]\n",
    "        if not sub.empty:\n",
    "            best_at_min_p = float(sub.iloc[sub[\"f1\"].values.argmax()][\"threshold\"])\n",
    "    # Best under min recall\n",
    "    best_at_min_r = None\n",
    "    if isinstance(min_recall, float):\n",
    "        sub = metrics[metrics[\"recall\"] >= min_recall]\n",
    "        if not sub.empty:\n",
    "            best_at_min_r = float(sub.iloc[sub[\"f1\"].values.argmax()][\"threshold\"])\n",
    "    return {\"best_f1\": best_f1, \"best_at_min_precision\": best_at_min_p, \"best_at_min_recall\": best_at_min_r}\n",
    "\n",
    "# -----------------------\n",
    "# Pair generation on index\n",
    "# -----------------------\n",
    "def process_row(i: int, neighbors_idx: np.ndarray, neighbors_dist: np.ndarray, tokens: List[str], thr: float) -> List[dict]:\n",
    "    res = []\n",
    "    dists = neighbors_dist[i][1:]\n",
    "    idxs = neighbors_idx[i][1:]\n",
    "    for rank, (d, j) in enumerate(zip(dists, idxs), start=1):\n",
    "        sim = 1.0 - float(d)\n",
    "        if sim >= thr:\n",
    "            res.append({\"token_a\": tokens[i], \"token_b\": tokens[int(j)], \"cosine_sim\": sim, \"rank\": rank})\n",
    "    return res\n",
    "\n",
    "def extra_metrics(pairs: List[dict]) -> None:\n",
    "    for p in pairs:\n",
    "        ratio = SequenceMatcher(None, p[\"token_a\"], p[\"token_b\"], autojunk=False).ratio()\n",
    "        p[\"seq_ratio\"] = round(ratio, 4)\n",
    "        p[\"len_a\"] = len(p[\"token_a\"])\n",
    "        p[\"len_b\"] = len(p[\"token_b\"])\n",
    "        p[\"len_diff\"] = abs(p[\"len_a\"] - p[\"len_b\"])\n",
    "        p[\"df_ratio\"] = 1.0\n",
    "\n",
    "# -----------------------\n",
    "# Main\n",
    "# -----------------------\n",
    "def main() -> None:\n",
    "    # Model\n",
    "    model = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" if USE_MULTILINGUAL else MODEL_NAME\n",
    "    log_info(\"üìã CONFIGURATION (Embeddings + Tuning)\")\n",
    "    log_info(f\"  Model: {model}\")\n",
    "    log_info(f\"  Batch size: {BATCH_SIZE}  | Normalize: {NORMALIZE}\")\n",
    "    log_info(f\"  HNSW: M={HNSW_M}, efC={HNSW_EF_CONSTRUCTION}, efQ={HNSW_EF}\")\n",
    "    log_info(f\"  Top-K neighbors: {TOP_K_NEIGHBORS}\")\n",
    "    log_info(f\"  Tokens: {len(canonical_tokens):,}\")\n",
    "    log_info(f\"  Tuning: {ENABLE_THRESHOLD_TUNING} | CSV: {EVAL_CSV_PATH or 'None'} | minP={EVAL_MIN_PRECISION} | minR={EVAL_MIN_RECALL}\")\n",
    "    log_memory()\n",
    "\n",
    "    # Load model\n",
    "    log_stage(\"LOADING EMBEDDING MODEL\")\n",
    "    t0 = time.time()\n",
    "    embedder = load_embedder(model)\n",
    "    dim = embedder.get_sentence_embedding_dimension()\n",
    "    log_info(f\"‚úÖ Model loaded. Embedding dim: {dim}\")\n",
    "    log_time(\"Model load\", t0)\n",
    "\n",
    "    # Optional: threshold tuning\n",
    "    tuned_threshold = None\n",
    "    eval_metrics_path = OUTPUTS_DIR / \"similarity_threshold_metrics.csv\"\n",
    "    eval_summary_path = OUTPUTS_DIR / \"similarity_threshold_summary.json\"\n",
    "    if ENABLE_THRESHOLD_TUNING:\n",
    "        log_stage(\"THRESHOLD TUNING (QUICK EVAL)\")\n",
    "        # Load eval pairs\n",
    "        pos_pairs: List[Tuple[str,str]] = []\n",
    "        neg_pairs: List[Tuple[str,str]] = []\n",
    "        if EVAL_CSV_PATH:\n",
    "            pos_pairs, neg_pairs = load_eval_pairs_from_csv(EVAL_CSV_PATH)\n",
    "        else:\n",
    "            pp, nn = gather_eval_pairs_from_namespace()\n",
    "            pos_pairs, neg_pairs = pp, nn\n",
    "\n",
    "        if (not pos_pairs) or (not neg_pairs):\n",
    "            log_info(\"‚ö†Ô∏è  No eval pairs found. Skipping tuning and keeping configured threshold.\")\n",
    "        else:\n",
    "            t_eval = time.time()\n",
    "            scores, labels = embed_eval_pairs(embedder, pos_pairs, neg_pairs, BATCH_SIZE, NORMALIZE)\n",
    "            metrics_df = sweep_thresholds(scores, labels, EVAL_GRID_START, EVAL_GRID_END, EVAL_GRID_STEP)\n",
    "            choices = pick_threshold(metrics_df, EVAL_MIN_PRECISION, EVAL_MIN_RECALL)\n",
    "\n",
    "            # Choose priority: min-precision -> min-recall -> best F1\n",
    "            tuned_threshold = (\n",
    "                choices[\"best_at_min_precision\"]\n",
    "                or choices[\"best_at_min_recall\"]\n",
    "                or choices[\"best_f1\"]\n",
    "            )\n",
    "            metrics_df.to_csv(eval_metrics_path, index=False)\n",
    "            with open(eval_summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(\n",
    "                    {\n",
    "                        \"grid\": [EVAL_GRID_START, EVAL_GRID_END, EVAL_GRID_STEP],\n",
    "                        \"min_precision\": EVAL_MIN_PRECISION,\n",
    "                        \"min_recall\": EVAL_MIN_RECALL,\n",
    "                        \"choices\": choices,\n",
    "                        \"picked_threshold\": tuned_threshold,\n",
    "                        \"pos_pairs\": len(pos_pairs),\n",
    "                        \"neg_pairs\": len(neg_pairs)\n",
    "                    },\n",
    "                    f,\n",
    "                    indent=2\n",
    "                )\n",
    "            log_time(\"Tuning\", t_eval)\n",
    "            log_info(f\"‚úÖ Tuned threshold = {tuned_threshold:.3f}  (choices={choices})\")\n",
    "\n",
    "    # Embeddings for full token set\n",
    "    log_stage(\"EMBEDDING CANONICAL TOKENS\")\n",
    "    t1 = time.time()\n",
    "    X = encode_texts(embedder, canonical_tokens, BATCH_SIZE, NORMALIZE)\n",
    "    log_time(\"Embedding\", t1)\n",
    "    log_info(f\"‚úÖ Embeddings: {X.shape[0]:,} √ó {X.shape[1]:,} (float32)\")\n",
    "    log_memory()\n",
    "\n",
    "    # Build & query ANN\n",
    "    log_stage(\"BUILDING ANN INDEX & RETRIEVING NEIGHBORS\")\n",
    "    k = min(TOP_K_NEIGHBORS + 1, X.shape[0])\n",
    "    t2 = time.time()\n",
    "    index = build_hnsw(X, HNSW_M, HNSW_EF_CONSTRUCTION, HNSW_EF, k)\n",
    "    idx, dist = knn_query_batched(index, X, k=k, batch=QUERY_BATCH_SIZE)\n",
    "    log_time(\"HNSW build + KNN\", t2)\n",
    "    log_info(\"‚úÖ ANN completed\")\n",
    "    log_memory()\n",
    "\n",
    "    # Use tuned threshold if available\n",
    "    threshold = float(tuned_threshold) if tuned_threshold is not None else float(SIMILARITY_THRESHOLD)\n",
    "    log_info(f\"üîé Using similarity threshold: {threshold:.3f}\")\n",
    "\n",
    "    # Generate candidate pairs\n",
    "    log_stage(\"GENERATING CANDIDATE PAIRS\")\n",
    "    t3 = time.time()\n",
    "    nested = Parallel(n_jobs=-1, verbose=1)(\n",
    "        delayed(process_row)(i, idx, dist, canonical_tokens, threshold) for i in range(X.shape[0])\n",
    "    )\n",
    "    pairs = [p for sub in nested for p in sub]\n",
    "    log_time(\"Candidate pair generation\", t3)\n",
    "    log_info(f\"üìä Candidate pairs: {len(pairs):,}\")\n",
    "    log_memory()\n",
    "\n",
    "    # Extra diagnostics\n",
    "    log_stage(\"CALCULATING ADDITIONAL METRICS\")\n",
    "    extra_metrics(pairs)\n",
    "    log_info(f\"‚úÖ Metrics enriched: {len(pairs):,}\")\n",
    "\n",
    "    # Save outputs\n",
    "    log_stage(\"SAVING RESULTS\")\n",
    "    pairs_df = pd.DataFrame(pairs)\n",
    "    pairs_path = OUTPUTS_DIR / \"candidate_similarity_pairs.parquet\"\n",
    "    sample_path = OUTPUTS_DIR / \"similarity_sample_inspection.parquet\"\n",
    "    meta_path = OUTPUTS_DIR / \"cell3_similarity_metadata.json\"\n",
    "\n",
    "    pairs_df.to_parquet(pairs_path, index=False)\n",
    "    sample_n = min(5000, len(pairs))\n",
    "    if sample_n > 0:\n",
    "        pairs_df.sample(n=sample_n, random_state=42).to_parquet(sample_path, index=False)\n",
    "\n",
    "    meta = {\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"config\": {\n",
    "            \"model\": model,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"normalize\": NORMALIZE,\n",
    "            \"top_k_neighbors\": TOP_K_NEIGHBORS,\n",
    "            \"similarity_threshold_initial\": SIMILARITY_THRESHOLD,\n",
    "            \"similarity_threshold_used\": threshold,\n",
    "            \"hnsw\": {\"M\": HNSW_M, \"ef_construction\": HNSW_EF_CONSTRUCTION, \"ef\": HNSW_EF},\n",
    "            \"query_batch_size\": QUERY_BATCH_SIZE,\n",
    "            \"tuning\": {\n",
    "                \"enabled\": ENABLE_THRESHOLD_TUNING,\n",
    "                \"csv\": EVAL_CSV_PATH,\n",
    "                \"min_precision\": EVAL_MIN_PRECISION,\n",
    "                \"min_recall\": EVAL_MIN_RECALL,\n",
    "                \"grid\": [EVAL_GRID_START, EVAL_GRID_END, EVAL_GRID_STEP],\n",
    "                \"metrics_csv\": str(eval_metrics_path) if ENABLE_THRESHOLD_TUNING else None,\n",
    "                \"summary_json\": str(eval_summary_path) if ENABLE_THRESHOLD_TUNING else None\n",
    "            }\n",
    "        },\n",
    "        \"stats\": {\n",
    "            \"total_tokens\": len(canonical_tokens),\n",
    "            \"embedding_dim\": int(X.shape[1]),\n",
    "            \"candidate_pairs_found\": len(pairs)\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"candidate_pairs_file\": str(pairs_path),\n",
    "            \"sample_file\": str(sample_path) if sample_n > 0 else None\n",
    "        }\n",
    "    }\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2, ensure_ascii=False)\n",
    "    log_info(f\"‚úÖ Saved pairs: {pairs_path}\")\n",
    "    if ENABLE_THRESHOLD_TUNING:\n",
    "        log_info(f\"‚úÖ Saved tuning metrics: {eval_metrics_path}\")\n",
    "        log_info(f\"‚úÖ Saved tuning summary: {eval_summary_path}\")\n",
    "    log_info(f\"‚úÖ Metadata: {meta_path}\")\n",
    "\n",
    "    # Summary\n",
    "    log_stage(\"SUMMARY STATISTICS\")\n",
    "    if pairs:\n",
    "        sims = [p[\"cosine_sim\"] for p in pairs]\n",
    "        ratios = [p[\"seq_ratio\"] for p in pairs]\n",
    "        log_info(f\"Cosine similarity: min={min(sims):.3f}, mean={np.mean(sims):.3f}, max={max(sims):.3f}\")\n",
    "        log_info(f\"SeqMatcher ratio: min={min(ratios):.3f}, mean={np.mean(ratios):.3f}, max={max(ratios):.3f}\")\n",
    "        covered = {p[\"token_a\"] for p in pairs} | {p[\"token_b\"] for p in pairs}\n",
    "        cov = len(covered) / len(canonical_tokens) * 100\n",
    "        log_info(f\"Token coverage: {len(covered):,}/{len(canonical_tokens):,} ({cov:.1f}%)\")\n",
    "    log_info(\"üéâ Cell 3 completed with pretrained embeddings + auto threshold tuning!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[22:01:11] üîß CELL 2: UNIVERSAL STRING CANONICALIZATION (v0)\n",
      "======================================================================\n",
      "üìã CANONICALIZATION CONFIG:\n",
      "  normalize_case: True\n",
      "  remove_extra_whitespace: True\n",
      "  remove_special_chars: False\n",
      "  standardize_separators: True\n",
      "  min_token_length: 1\n",
      "  max_token_length: 100\n",
      "\n",
      "üìö GENRE CANONICALIZATION\n",
      "----------------------------------------\n",
      "\n",
      "üîß PROCESSING GENRES:\n",
      "Canonicalizing genres from main_final dataset...\n",
      "Found 13 unique genres\n",
      "  ‚úÖ Processed 13 genres\n",
      "\n",
      "üìö SHELF CANONICALIZATION\n",
      "----------------------------------------\n",
      "\n",
      "üîß PROCESSING SHELVES:\n",
      "Canonicalizing shelves from main_final dataset...\n",
      "Found 255,664 unique shelves\n",
      "  ‚úÖ Processed 255,664 shelves\n",
      "\n",
      "üìä CANONICALIZATION RESULTS:\n",
      "----------------------------------------\n",
      "üìö GENRES:\n",
      "  Original count: 13\n",
      "  Canonical count: 13\n",
      "  Compression ratio: 1.000\n",
      "  Changes made: 0\n",
      "  Unchanged: 13\n",
      "\n",
      "üìö SHELVES:\n",
      "  Original count: 255,664\n",
      "  Canonical count: 254,778\n",
      "  Compression ratio: 0.997\n",
      "  Changes made: 230,934\n",
      "  Unchanged: 24,730\n",
      "\n",
      "üíæ SAVING CANONICAL MAPPINGS:\n",
      "----------------------------------------\n",
      "  ‚úÖ Saved genre mappings to: romance-novel-nlp-research/src/eda_analysis/outputs/genre_canonical_mappings.csv\n",
      "  ‚úÖ Saved shelf mappings to: romance-novel-nlp-research/src/eda_analysis/outputs/shelf_canonical_mappings.csv\n",
      "  ‚úÖ Saved metadata to: romance-novel-nlp-research/src/eda_analysis/outputs/canonicalization_metadata.json\n",
      "\n",
      "[22:01:38] ‚úÖ Cell 2: Universal String Canonicalization completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: UNIVERSAL STRING CANONICALIZATION (v0)\n",
    "# =============================================================================\n",
    "\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"\\n[{time.strftime('%H:%M:%S')}] üîß CELL 2: UNIVERSAL STRING CANONICALIZATION (v0)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "### ‚öôÔ∏è Configuration Setup\n",
    "\n",
    "# Configuration for canonicalization\n",
    "CANONICAL_CONFIG = {\n",
    "    'normalize_case': True,\n",
    "    'remove_extra_whitespace': True,\n",
    "    'remove_special_chars': False,  # Keep for genre/shelf analysis\n",
    "    'standardize_separators': True,\n",
    "    'min_token_length': 1,\n",
    "    'max_token_length': 100\n",
    "}\n",
    "\n",
    "print(f\"üìã CANONICALIZATION CONFIG:\")\n",
    "for key, value in CANONICAL_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "### üìö Genre Canonicalization\n",
    "\n",
    "print(f\"\\nüìö GENRE CANONICALIZATION\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def canonicalize_genre(genre_str):\n",
    "    \"\"\"\n",
    "    Canonicalize a single genre string.\n",
    "    \n",
    "    Args:\n",
    "        genre_str (str): Raw genre string\n",
    "        \n",
    "    Returns:\n",
    "        str: Canonicalized genre string\n",
    "    \"\"\"\n",
    "    if not isinstance(genre_str, str) or not genre_str.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    # Normalize case\n",
    "    canonical = genre_str.lower() if CANONICAL_CONFIG['normalize_case'] else genre_str\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    if CANONICAL_CONFIG['remove_extra_whitespace']:\n",
    "        canonical = ' '.join(canonical.split())\n",
    "    \n",
    "    # Standardize separators (hyphens to spaces for consistency)\n",
    "    if CANONICAL_CONFIG['standardize_separators']:\n",
    "        canonical = re.sub(r'[-_]+', ' ', canonical)\n",
    "        canonical = ' '.join(canonical.split())  # Clean up multiple spaces\n",
    "    \n",
    "    # Length validation\n",
    "    if len(canonical) < CANONICAL_CONFIG['min_token_length'] or len(canonical) > CANONICAL_CONFIG['max_token_length']:\n",
    "        return \"\"\n",
    "    \n",
    "    return canonical.strip()\n",
    "\n",
    "# Apply canonicalization to unique genres\n",
    "print(f\"\\nüîß PROCESSING GENRES:\")\n",
    "print(f\"Canonicalizing genres from main_final dataset...\")\n",
    "\n",
    "# Extract unique genres from the dataset\n",
    "unique_genres = set()\n",
    "for idx, row in main_final.iterrows():\n",
    "    if pd.notna(row.get('genres_str')) and row['genres_str'].strip():\n",
    "        genres_list = [g.strip() for g in row['genres_str'].split(',') if g.strip()]\n",
    "        unique_genres.update(genres_list)\n",
    "\n",
    "print(f\"Found {len(unique_genres):,} unique genres\")\n",
    "\n",
    "canonical_genres = {}\n",
    "genre_mapping_stats = defaultdict(list)\n",
    "\n",
    "for original_genre in unique_genres:\n",
    "    canonical = canonicalize_genre(original_genre)\n",
    "    canonical_genres[original_genre] = canonical\n",
    "    \n",
    "    # Track mapping for analysis\n",
    "    if canonical != original_genre.lower():\n",
    "        genre_mapping_stats['changed'].append((original_genre, canonical))\n",
    "    else:\n",
    "        genre_mapping_stats['unchanged'].append(original_genre)\n",
    "\n",
    "print(f\"  ‚úÖ Processed {len(canonical_genres):,} genres\")\n",
    "\n",
    "### üìö Shelf Canonicalization\n",
    "\n",
    "print(f\"\\nüìö SHELF CANONICALIZATION\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def canonicalize_shelf(shelf_str):\n",
    "    \"\"\"\n",
    "    Canonicalize a single shelf string.\n",
    "    \n",
    "    Args:\n",
    "        shelf_str (str): Raw shelf string\n",
    "        \n",
    "    Returns:\n",
    "        str: Canonicalized shelf string\n",
    "    \"\"\"\n",
    "    if not isinstance(shelf_str, str) or not shelf_str.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    # Normalize case\n",
    "    canonical = shelf_str.lower() if CANONICAL_CONFIG['normalize_case'] else shelf_str\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    if CANONICAL_CONFIG['remove_extra_whitespace']:\n",
    "        canonical = ' '.join(canonical.split())\n",
    "    \n",
    "    # Standardize separators (hyphens to spaces for consistency)\n",
    "    if CANONICAL_CONFIG['standardize_separators']:\n",
    "        canonical = re.sub(r'[-_]+', ' ', canonical)\n",
    "        canonical = ' '.join(canonical.split())  # Clean up multiple spaces\n",
    "    \n",
    "    # Length validation\n",
    "    if len(canonical) < CANONICAL_CONFIG['min_token_length'] or len(canonical) > CANONICAL_CONFIG['max_token_length']:\n",
    "        return \"\"\n",
    "    \n",
    "    return canonical.strip()\n",
    "\n",
    "# Apply canonicalization to unique shelves\n",
    "print(f\"\\nüîß PROCESSING SHELVES:\")\n",
    "print(f\"Canonicalizing shelves from main_final dataset...\")\n",
    "\n",
    "# Extract unique shelves from the dataset\n",
    "unique_shelves = set()\n",
    "for idx, row in main_final.iterrows():\n",
    "    if pd.notna(row.get('shelves_str')) and row['shelves_str'].strip():\n",
    "        shelves_list = [s.strip() for s in row['shelves_str'].split(',') if s.strip()]\n",
    "        unique_shelves.update(shelves_list)\n",
    "\n",
    "print(f\"Found {len(unique_shelves):,} unique shelves\")\n",
    "\n",
    "canonical_shelves = {}\n",
    "shelf_mapping_stats = defaultdict(list)\n",
    "\n",
    "for original_shelf in unique_shelves:\n",
    "    canonical = canonicalize_shelf(original_shelf)\n",
    "    canonical_shelves[original_shelf] = canonical\n",
    "    \n",
    "    # Track mapping for analysis\n",
    "    if canonical != original_shelf.lower():\n",
    "        shelf_mapping_stats['changed'].append((original_shelf, canonical))\n",
    "    else:\n",
    "        shelf_mapping_stats['unchanged'].append(original_shelf)\n",
    "\n",
    "print(f\"  ‚úÖ Processed {len(canonical_shelves):,} shelves\")\n",
    "\n",
    "### üìä Canonicalization Results\n",
    "\n",
    "print(f\"\\nüìä CANONICALIZATION RESULTS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Get unique canonical values\n",
    "unique_canonical_genres = set(canonical_genres.values())\n",
    "unique_canonical_shelves = set(canonical_shelves.values())\n",
    "\n",
    "# Calculate compression ratios\n",
    "genre_compression_ratio = len(unique_canonical_genres) / len(unique_genres) if len(unique_genres) > 0 else 0\n",
    "shelf_compression_ratio = len(unique_canonical_shelves) / len(unique_shelves) if len(unique_shelves) > 0 else 0\n",
    "\n",
    "print(f\"üìö GENRES:\")\n",
    "print(f\"  Original count: {len(unique_genres):,}\")\n",
    "print(f\"  Canonical count: {len(unique_canonical_genres):,}\")\n",
    "print(f\"  Compression ratio: {genre_compression_ratio:.3f}\")\n",
    "print(f\"  Changes made: {len(genre_mapping_stats['changed']):,}\")\n",
    "print(f\"  Unchanged: {len(genre_mapping_stats['unchanged']):,}\")\n",
    "\n",
    "print(f\"\\nüìö SHELVES:\")\n",
    "print(f\"  Original count: {len(unique_shelves):,}\")\n",
    "print(f\"  Canonical count: {len(unique_canonical_shelves):,}\")\n",
    "print(f\"  Compression ratio: {shelf_compression_ratio:.3f}\")\n",
    "print(f\"  Changes made: {len(shelf_mapping_stats['changed']):,}\")\n",
    "print(f\"  Unchanged: {len(shelf_mapping_stats['unchanged']):,}\")\n",
    "\n",
    "### üíæ Save Canonical Mappings\n",
    "\n",
    "print(f\"\\nüíæ SAVING CANONICAL MAPPINGS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create outputs directory\n",
    "outputs_dir = Path(\"romance-novel-nlp-research/src/eda_analysis/outputs\")\n",
    "outputs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save genre mappings\n",
    "genre_mappings_df = pd.DataFrame([\n",
    "    {'original': orig, 'canonical': canon} \n",
    "    for orig, canon in canonical_genres.items()\n",
    "])\n",
    "genre_mappings_path = outputs_dir / \"genre_canonical_mappings.csv\"\n",
    "genre_mappings_df.to_csv(genre_mappings_path, index=False)\n",
    "print(f\"  ‚úÖ Saved genre mappings to: {genre_mappings_path}\")\n",
    "\n",
    "# Save shelf mappings\n",
    "shelf_mappings_df = pd.DataFrame([\n",
    "    {'original': orig, 'canonical': canon} \n",
    "    for orig, canon in canonical_shelves.items()\n",
    "])\n",
    "shelf_mappings_path = outputs_dir / \"shelf_canonical_mappings.csv\"\n",
    "shelf_mappings_df.to_csv(shelf_mappings_path, index=False)\n",
    "print(f\"  ‚úÖ Saved shelf mappings to: {shelf_mappings_path}\")\n",
    "\n",
    "# Save canonicalization metadata\n",
    "canonical_meta = {\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': CANONICAL_CONFIG,\n",
    "    'stats': {\n",
    "        'genres': {\n",
    "            'original_count': len(unique_genres),\n",
    "            'canonical_count': len(unique_canonical_genres),\n",
    "            'compression_ratio': genre_compression_ratio,\n",
    "            'changes_count': len(genre_mapping_stats['changed']),\n",
    "            'duplicates_eliminated': len(unique_genres) - len(unique_canonical_genres)\n",
    "        },\n",
    "        'shelves': {\n",
    "            'original_count': len(unique_shelves),\n",
    "            'canonical_count': len(unique_canonical_shelves),\n",
    "            'compression_ratio': shelf_compression_ratio,\n",
    "            'changes_count': len(shelf_mapping_stats['changed']),\n",
    "            'duplicates_eliminated': len(unique_shelves) - len(unique_canonical_shelves)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = outputs_dir / \"canonicalization_metadata.json\"\n",
    "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(canonical_meta, f, indent=2, ensure_ascii=False)\n",
    "print(f\"  ‚úÖ Saved metadata to: {metadata_path}\")\n",
    "\n",
    "print(f\"\\n[{time.strftime('%H:%M:%S')}] ‚úÖ Cell 2: Universal String Canonicalization completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Character Similarity Index & Neighbor Retrieval\n",
    "\n",
    "This section builds a comprehensive character-based similarity index using TF-IDF vectorization and approximate nearest neighbor (ANN) search to identify potential duplicate shelf names.\n",
    "\n",
    "### What this section does:\n",
    "- Creates TF-IDF vectors from canonical shelf tokens using character n-grams\n",
    "- Builds an approximate nearest neighbor index for efficient similarity search\n",
    "- Retrieves candidate similar pairs based on cosine similarity thresholds\n",
    "- Generates comprehensive statistics on similarity patterns and coverage\n",
    "- Exports sample data for manual validation and quality assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install hnswlib\n",
    "! pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:üìã CONFIGURATION (Embeddings + Tuning + Fast Pairs)\n",
      "INFO:__main__:  Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:__main__:  Batch size: 1024 | Normalize: True\n",
      "INFO:__main__:  HNSW: M=16, efC=200, efQ=200\n",
      "INFO:__main__:  Top-K neighbors: 50\n",
      "INFO:__main__:  Tokens: 254,778\n",
      "INFO:__main__:  Tuning: True | CSV: None | minP=0.9 | minR=None\n",
      "INFO:__main__:  Autogen eval: True (pos=1500, neg=1500)\n",
      "INFO:__main__:  Extra metrics: False | RapidFuzz: True | sample=None\n",
      "INFO:__main__:üíæ Memory usage: 6.18 GB\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:üöÄ LOADING EMBEDDING MODEL\n",
      "INFO:__main__:============================================================\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã CONFIGURATION (Embeddings + Tuning + Fast Pairs)\n",
      "  Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "  Batch size: 1024 | Normalize: True\n",
      "  HNSW: M=16, efC=200, efQ=200\n",
      "  Top-K neighbors: 50\n",
      "  Tokens: 254,778\n",
      "  Tuning: True | CSV: None | minP=0.9 | minR=None\n",
      "  Autogen eval: True (pos=1500, neg=1500)\n",
      "  Extra metrics: False | RapidFuzz: True | sample=None\n",
      "üíæ Memory usage: 6.18 GB\n",
      "============================================================\n",
      "üöÄ LOADING EMBEDDING MODEL\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:‚úÖ Model loaded. Embedding dim: 384\n",
      "INFO:__main__:Model load: 4.08s\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:üöÄ THRESHOLD TUNING (QUICK EVAL)\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:‚ÑπÔ∏è No eval pairs supplied. Auto-generating a small eval set from tokens...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded. Embedding dim: 384\n",
      "‚è±Ô∏è Model load took 4.08 s\n",
      "============================================================\n",
      "üöÄ THRESHOLD TUNING (QUICK EVAL)\n",
      "============================================================\n",
      "‚ÑπÔ∏è No eval pairs supplied. Auto-generating a small eval set from tokens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:‚úÖ Auto-generated eval pairs: pos=633, neg=1500 (neg iters=7070)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Auto-generated eval pairs: pos=633, neg=1500 (neg iters=7070)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a08bfc7db847eeaab8f121a536f410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Tuning: 0.71s\n",
      "INFO:__main__:‚úÖ Tuned threshold = 0.450  (choices={'best_f1': 0.44999992847442627, 'best_at_min_precision': 0.44999992847442627, 'best_at_min_recall': None})\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:üöÄ EMBEDDING CANONICAL TOKENS\n",
      "INFO:__main__:============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Tuning took 0.71 s\n",
      "‚úÖ Tuned threshold = 0.450  (choices={'best_f1': 0.44999992847442627, 'best_at_min_precision': 0.44999992847442627, 'best_at_min_recall': None})\n",
      "============================================================\n",
      "üöÄ EMBEDDING CANONICAL TOKENS\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4816dd3679481bbfb54e119584c656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Embedding: 80.62s\n",
      "INFO:__main__:‚úÖ Embeddings: 254,778 √ó 384 (float32)\n",
      "INFO:__main__:üíæ Memory usage: 6.60 GB\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:üöÄ BUILDING ANN INDEX & RETRIEVING NEIGHBORS\n",
      "INFO:__main__:============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Embedding took 80.62 s\n",
      "‚úÖ Embeddings: 254,778 √ó 384 (float32)\n",
      "üíæ Memory usage: 6.60 GB\n",
      "============================================================\n",
      "üöÄ BUILDING ANN INDEX & RETRIEVING NEIGHBORS\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:HNSW build + KNN: 136.21s\n",
      "INFO:__main__:‚úÖ ANN completed\n",
      "INFO:__main__:üíæ Memory usage: 7.06 GB\n",
      "INFO:__main__:üîé Using similarity threshold: 0.450\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:üöÄ GENERATING CANDIDATE PAIRS (FAST, VECTORIZED)\n",
      "INFO:__main__:============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è HNSW build + KNN took 136.21 s\n",
      "‚úÖ ANN completed\n",
      "üíæ Memory usage: 7.06 GB\n",
      "üîé Using similarity threshold: 0.450\n",
      "============================================================\n",
      "üöÄ GENERATING CANDIDATE PAIRS (FAST, VECTORIZED)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Candidate pair generation (vectorized): 3.97s\n",
      "INFO:__main__:üìä Candidate pairs: 12,502,616\n",
      "INFO:__main__:üíæ Memory usage: 7.34 GB\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:üöÄ SAVING RESULTS\n",
      "INFO:__main__:============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Candidate pair generation (vectorized) took 3.97 s\n",
      "üìä Candidate pairs: 12,502,616\n",
      "üíæ Memory usage: 7.34 GB\n",
      "============================================================\n",
      "üöÄ SAVING RESULTS\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:‚úÖ Saved pairs: romance-novel-nlp-research/src/eda_analysis/outputs/candidate_similarity_pairs.parquet\n",
      "INFO:__main__:‚úÖ Saved sample: romance-novel-nlp-research/src/eda_analysis/outputs/similarity_sample_inspection.parquet\n",
      "INFO:__main__:‚úÖ Saved tuning metrics: romance-novel-nlp-research/src/eda_analysis/outputs/similarity_threshold_metrics.csv (auto-generated eval set)\n",
      "INFO:__main__:‚úÖ Saved tuning summary: romance-novel-nlp-research/src/eda_analysis/outputs/similarity_threshold_summary.json\n",
      "INFO:__main__:‚úÖ Metadata: romance-novel-nlp-research/src/eda_analysis/outputs/cell3_similarity_metadata.json\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:üöÄ SUMMARY STATISTICS\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:Cosine similarity: min=0.450, mean=0.678, max=1.000\n",
      "INFO:__main__:Token coverage: 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved pairs: romance-novel-nlp-research/src/eda_analysis/outputs/candidate_similarity_pairs.parquet\n",
      "‚úÖ Saved sample: romance-novel-nlp-research/src/eda_analysis/outputs/similarity_sample_inspection.parquet\n",
      "‚úÖ Saved tuning metrics: romance-novel-nlp-research/src/eda_analysis/outputs/similarity_threshold_metrics.csv (auto-generated eval set)\n",
      "‚úÖ Saved tuning summary: romance-novel-nlp-research/src/eda_analysis/outputs/similarity_threshold_summary.json\n",
      "‚úÖ Metadata: romance-novel-nlp-research/src/eda_analysis/outputs/cell3_similarity_metadata.json\n",
      "============================================================\n",
      "üöÄ SUMMARY STATISTICS\n",
      "============================================================\n",
      "Cosine similarity: min=0.450, mean=0.678, max=1.000\n",
      "Token coverage: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:üéâ Cell 3 completed with pretrained embeddings, auto-tuning, and fast pair generation!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Cell 3 completed with pretrained embeddings, auto-tuning, and fast pair generation!\n"
     ]
    }
   ],
   "source": [
    "# romance-novel-nlp-research/src/eda_analysis/cell3_similarity_embeddings_tuned.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Iterable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import hnswlib\n",
    "\n",
    "# ======================\n",
    "# Logging\n",
    "# ======================\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.FileHandler(\"cell3_pipeline.log\", mode=\"w\"), logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def log_info(msg: str) -> None:\n",
    "    print(msg)\n",
    "    logger.info(msg)\n",
    "\n",
    "def log_stage(stage: str) -> None:\n",
    "    sep = \"=\" * 60\n",
    "    print(sep); print(f\"üöÄ {stage}\"); print(sep)\n",
    "    logger.info(sep); logger.info(f\"üöÄ {stage}\"); logger.info(sep)\n",
    "\n",
    "def log_time(label: str, start: float) -> None:\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"‚è±Ô∏è {label} took {elapsed:.2f} s\")\n",
    "    logger.info(f\"{label}: {elapsed:.2f}s\")\n",
    "\n",
    "def log_memory() -> None:\n",
    "    mem_gb = psutil.Process(os.getpid()).memory_info().rss / (1024**3)\n",
    "    msg = f\"üíæ Memory usage: {mem_gb:.2f} GB\"\n",
    "    print(msg); logger.info(msg)\n",
    "\n",
    "# ======================\n",
    "# Config\n",
    "# ======================\n",
    "MODEL_NAME: str = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384-d, fast\n",
    "USE_MULTILINGUAL: bool = False                               # True ‚Üí paraphrase-multilingual-MiniLM-L12-v2\n",
    "BATCH_SIZE: int = 1024\n",
    "NORMALIZE: bool = True\n",
    "\n",
    "TOP_K_NEIGHBORS: int = 50\n",
    "SIMILARITY_THRESHOLD: float = 0.30  # initial; may be overridden by tuning\n",
    "\n",
    "# HNSW\n",
    "HNSW_M: int = 16\n",
    "HNSW_EF_CONSTRUCTION: int = 200\n",
    "HNSW_EF: int = 200\n",
    "QUERY_BATCH_SIZE: int = 25000\n",
    "\n",
    "# Threshold tuning\n",
    "ENABLE_THRESHOLD_TUNING: bool = True\n",
    "EVAL_CSV_PATH: Optional[str] = None   # CSV columns: token_a, token_b, label (1/0)\n",
    "EVAL_MIN_PRECISION: Optional[float] = 0.90\n",
    "EVAL_MIN_RECALL: Optional[float] = None\n",
    "EVAL_GRID_START: float = 0.05\n",
    "EVAL_GRID_END: float = 0.95\n",
    "EVAL_GRID_STEP: float = 0.01\n",
    "\n",
    "# Auto-generate eval pairs if none are supplied\n",
    "AUTOGEN_EVAL_IF_MISSING: bool = True\n",
    "AUTOGEN_TARGET_POS: int = 1500\n",
    "AUTOGEN_TARGET_NEG: int = 1500\n",
    "AUTOGEN_MAX_PAIRS_PER_GROUP: int = 3\n",
    "AUTOGEN_BUCKET_PREFIX_LEN: int = 2\n",
    "AUTOGEN_POS_MIN_RATIO: int = 90    # %\n",
    "AUTOGEN_NEG_MAX_RATIO: int = 20    # %\n",
    "AUTOGEN_RANDOM_SEED: int = 42\n",
    "AUTOGEN_MAX_NEG_ITERS: int = 1_000_000\n",
    "\n",
    "# Optional extra string metrics on the final pairs\n",
    "ADD_STRING_METRICS: bool = False          # default off for speed\n",
    "USE_RAPIDFUZZ: bool = True                # prefer RapidFuzz if available\n",
    "METRICS_SAMPLE_N: Optional[int] = None    # e.g., 200_000 to cap; None = all\n",
    "\n",
    "OUTPUTS_DIR = Path(\"romance-novel-nlp-research/src/eda_analysis/outputs\")\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ======================\n",
    "# Inputs (from previous cells)\n",
    "# ======================\n",
    "try:\n",
    "    canonical_tokens: List[str] = list(unique_canonical_shelves)  # noqa: F821\n",
    "except NameError as e:\n",
    "    raise RuntimeError(\"Expected `unique_canonical_shelves` to be defined in previous cells.\") from e\n",
    "\n",
    "# ======================\n",
    "# Embedding utils\n",
    "# ======================\n",
    "def load_embedder(model_name: str):\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "    except Exception as exc:\n",
    "        raise RuntimeError(\"Install with: pip install -U sentence-transformers\") from exc\n",
    "    device = \"cuda\" if os.environ.get(\"USE_CUDA\", \"1\") == \"1\" else \"cpu\"\n",
    "    return SentenceTransformer(model_name, device=device)\n",
    "\n",
    "def encode_texts(embedder, texts: List[str], batch_size: int, normalize_vecs: bool) -> np.ndarray:\n",
    "    return embedder.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=normalize_vecs\n",
    "    ).astype(np.float32, copy=False)\n",
    "\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    return (a * b).sum(axis=1)\n",
    "\n",
    "# ======================\n",
    "# HNSW utils\n",
    "# ======================\n",
    "def build_hnsw(vectors: np.ndarray, m: int, ef_c: int, ef_q: int, topk: int):\n",
    "    index = hnswlib.Index(space=\"cosine\", dim=vectors.shape[1])\n",
    "    index.init_index(max_elements=vectors.shape[0], ef_construction=ef_c, M=m)\n",
    "    index.set_ef(max(ef_q, topk))\n",
    "    index.add_items(vectors, np.arange(vectors.shape[0], dtype=np.int32))\n",
    "    return index\n",
    "\n",
    "def knn_query_batched(index, vectors: np.ndarray, k: int, batch: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    n = vectors.shape[0]\n",
    "    all_idx = np.empty((n, k), dtype=np.int32)\n",
    "    all_dist = np.empty((n, k), dtype=np.float32)\n",
    "    s = 0\n",
    "    while s < n:\n",
    "        e = min(s + batch, n)\n",
    "        idx, dist = index.knn_query(vectors[s:e], k=k)\n",
    "        all_idx[s:e] = idx\n",
    "        all_dist[s:e] = dist\n",
    "        s = e\n",
    "    return all_idx, all_dist\n",
    "\n",
    "# ======================\n",
    "# Eval / threshold tuning\n",
    "# ======================\n",
    "def load_eval_pairs_from_csv(path: str) -> Tuple[List[Tuple[str, str]], List[Tuple[str, str]]]:\n",
    "    df = pd.read_csv(path)\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    a = cols.get(\"token_a\", cols.get(\"a\"))\n",
    "    b = cols.get(\"token_b\", cols.get(\"b\"))\n",
    "    y = cols.get(\"label\", cols.get(\"y\"))\n",
    "    if not (a and b and y):\n",
    "        raise ValueError(\"CSV must have columns token_a, token_b, label (1/0).\")\n",
    "    pos = [(str(x), str(y_)) for x, y_, lbl in zip(df[a], df[b], df[y]) if int(lbl) == 1]\n",
    "    neg = [(str(x), str(y_)) for x, y_, lbl in zip(df[a], df[b], df[y]) if int(lbl) == 0]\n",
    "    return pos, neg\n",
    "\n",
    "def gather_eval_pairs_from_namespace() -> Tuple[List[Tuple[str, str]], List[Tuple[str, str]]]:\n",
    "    pos, neg = [], []\n",
    "    g = globals()\n",
    "    if \"eval_positive_pairs\" in g and isinstance(g[\"eval_positive_pairs\"], Iterable):\n",
    "        pos = [(str(a), str(b)) for a, b in g[\"eval_positive_pairs\"]]\n",
    "    if \"eval_negative_pairs\" in g and isinstance(g[\"eval_negative_pairs\"], Iterable):\n",
    "        neg = [(str(a), str(b)) for a, b in g[\"eval_negative_pairs\"]]\n",
    "    return pos, neg\n",
    "\n",
    "def embed_eval_pairs(embedder, pairs_pos: List[Tuple[str, str]], pairs_neg: List[Tuple[str, str]],\n",
    "                     batch_size: int, normalize_vecs: bool) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    all_pairs = pairs_pos + pairs_neg\n",
    "    if not all_pairs:\n",
    "        return np.array([]), np.array([])\n",
    "    uniq: Dict[str, int] = {}\n",
    "    strings: List[str] = []\n",
    "    for a, b in all_pairs:\n",
    "        if a not in uniq:\n",
    "            uniq[a] = len(strings); strings.append(a)\n",
    "        if b not in uniq:\n",
    "            uniq[b] = len(strings); strings.append(b)\n",
    "    mat = encode_texts(embedder, strings, batch_size, normalize_vecs)\n",
    "    scores = np.empty(len(all_pairs), dtype=np.float32)\n",
    "    labels = np.empty(len(all_pairs), dtype=np.int32)\n",
    "    for i, (a, b) in enumerate(all_pairs):\n",
    "        va = mat[uniq[a]][None, :]\n",
    "        vb = mat[uniq[b]][None, :]\n",
    "        scores[i] = cosine_sim(va, vb)[0]\n",
    "        labels[i] = 1 if i < len(pairs_pos) else 0\n",
    "    return scores, labels\n",
    "\n",
    "def precision_recall_f1(y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[float, float, float]:\n",
    "    tp = int(((y_true == 1) & (y_pred == 1)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred == 0)).sum())\n",
    "    prec = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    rec = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    f1 = 2 * prec * rec / (prec + rec) if (prec + rec) else 0.0\n",
    "    return prec, rec, f1\n",
    "\n",
    "def sweep_thresholds(scores: np.ndarray, labels: np.ndarray, start: float, end: float, step: float) -> pd.DataFrame:\n",
    "    if scores.size == 0:\n",
    "        return pd.DataFrame(columns=[\"threshold\", \"precision\", \"recall\", \"f1\", \"support_pos\", \"support_neg\"])\n",
    "    thrs = np.arange(start, end + 1e-9, step, dtype=np.float32)\n",
    "    rows = []\n",
    "    pos_cnt = int((labels == 1).sum())\n",
    "    neg_cnt = int((labels == 0).sum())\n",
    "    for t in thrs:\n",
    "        y_pred = (scores >= t).astype(np.int32)\n",
    "        p, r, f1 = precision_recall_f1(labels, y_pred)\n",
    "        rows.append((float(t), p, r, f1, pos_cnt, neg_cnt))\n",
    "    return pd.DataFrame(rows, columns=[\"threshold\", \"precision\", \"recall\", \"f1\", \"support_pos\", \"support_neg\"])\n",
    "\n",
    "def pick_threshold(metrics: pd.DataFrame,\n",
    "                   min_precision: Optional[float],\n",
    "                   min_recall: Optional[float]) -> Dict[str, Optional[float]]:\n",
    "    if metrics.empty:\n",
    "        return {\"best_f1\": None, \"best_at_min_precision\": None, \"best_at_min_recall\": None}\n",
    "    best_f1_row = metrics.iloc[metrics[\"f1\"].values.argmax()]\n",
    "    best_f1 = float(best_f1_row[\"threshold\"])\n",
    "    best_at_min_p = None\n",
    "    if isinstance(min_precision, float):\n",
    "        sub = metrics[metrics[\"precision\"] >= min_precision]\n",
    "        if not sub.empty:\n",
    "            best_at_min_p = float(sub.iloc[sub[\"f1\"].values.argmax()][\"threshold\"])\n",
    "    best_at_min_r = None\n",
    "    if isinstance(min_recall, float):\n",
    "        sub = metrics[metrics[\"recall\"] >= min_recall]\n",
    "        if not sub.empty:\n",
    "            best_at_min_r = float(sub.iloc[sub[\"f1\"].values.argmax()][\"threshold\"])\n",
    "    return {\"best_f1\": best_f1, \"best_at_min_precision\": best_at_min_p, \"best_at_min_recall\": best_at_min_r}\n",
    "\n",
    "# ======================\n",
    "# Autogen eval pairs (when missing)\n",
    "# ======================\n",
    "def _norm_key(s: str) -> str:\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = re.sub(r\"[^a-z0-9 ]+\", \"\", s)\n",
    "    return s\n",
    "\n",
    "def _ratio(a: str, b: str) -> int:\n",
    "    if USE_RAPIDFUZZ:\n",
    "        try:\n",
    "            from rapidfuzz.fuzz import ratio as fuzz_ratio  # type: ignore\n",
    "            return int(fuzz_ratio(a, b))\n",
    "        except Exception:\n",
    "            pass\n",
    "    from difflib import SequenceMatcher\n",
    "    return int(SequenceMatcher(None, a, b, autojunk=False).ratio() * 100)\n",
    "\n",
    "def autogen_eval_pairs(tokens: List[str]) -> Tuple[List[Tuple[str, str]], List[Tuple[str, str]], Dict[str, int]]:\n",
    "    random.seed(AUTOGEN_RANDOM_SEED)\n",
    "    # Positives from normalized-key groups\n",
    "    pos: List[Tuple[str, str]] = []\n",
    "    groups: Dict[str, List[str]] = {}\n",
    "    for t in tokens:\n",
    "        groups.setdefault(_norm_key(t), []).append(t)\n",
    "    for key, arr in groups.items():\n",
    "        if len(arr) < 2 or not key:\n",
    "            continue\n",
    "        arr = list(dict.fromkeys(arr))\n",
    "        for other in arr[1:1 + AUTOGEN_MAX_PAIRS_PER_GROUP]:\n",
    "            pos.append((arr[0], other))\n",
    "        if len(pos) >= AUTOGEN_TARGET_POS:\n",
    "            break\n",
    "    # Top-up positives within small prefix buckets\n",
    "    if len(pos) < AUTOGEN_TARGET_POS:\n",
    "        buckets: Dict[str, List[str]] = {}\n",
    "        for t in tokens:\n",
    "            buckets.setdefault(t[:AUTOGEN_BUCKET_PREFIX_LEN].lower(), []).append(t)\n",
    "        for arr in buckets.values():\n",
    "            if len(pos) >= AUTOGEN_TARGET_POS:\n",
    "                break\n",
    "            if len(arr) < 2:\n",
    "                continue\n",
    "            sample = arr[:256]\n",
    "            for i in range(min(len(sample), 32)):\n",
    "                for j in range(i + 1, min(len(sample), 32)):\n",
    "                    a, b = sample[i], sample[j]\n",
    "                    if abs(len(a) - len(b)) > max(5, 0.4 * max(len(a), len(b))):\n",
    "                        continue\n",
    "                    if _ratio(a, b) >= AUTOGEN_POS_MIN_RATIO:\n",
    "                        pos.append((a, b))\n",
    "                        if len(pos) >= AUTOGEN_TARGET_POS:\n",
    "                            break\n",
    "                if len(pos) >= AUTOGEN_TARGET_POS:\n",
    "                    break\n",
    "    # Dedup positives\n",
    "    pos = list(dict.fromkeys(tuple(sorted(p)) for p in pos))\n",
    "    pos = [(a, b) for a, b in pos][:AUTOGEN_TARGET_POS]\n",
    "\n",
    "    # Negatives: random dissimilar pairs\n",
    "    neg: List[Tuple[str, str]] = []\n",
    "    uniq_tokens = list(dict.fromkeys(tokens))\n",
    "    n = len(uniq_tokens)\n",
    "    seen = set()\n",
    "    iters = 0\n",
    "    while len(neg) < AUTOGEN_TARGET_NEG and iters < AUTOGEN_MAX_NEG_ITERS:\n",
    "        iters += 1\n",
    "        i, j = random.randrange(n), random.randrange(n)\n",
    "        if i == j:\n",
    "            continue\n",
    "        a, b = uniq_tokens[i], uniq_tokens[j]\n",
    "        key = (a, b) if a < b else (b, a)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        if a[:AUTOGEN_BUCKET_PREFIX_LEN].lower() == b[:AUTOGEN_BUCKET_PREFIX_LEN].lower():\n",
    "            continue\n",
    "        if _norm_key(a) == _norm_key(b):\n",
    "            continue\n",
    "        if _ratio(a, b) <= AUTOGEN_NEG_MAX_RATIO:\n",
    "            seen.add(key)\n",
    "            neg.append((a, b))\n",
    "    stats = {\"neg_iters\": iters}\n",
    "    return pos, neg, stats\n",
    "\n",
    "# ======================\n",
    "# Vectorized candidate pair generation\n",
    "# ======================\n",
    "def generate_pairs_fast(\n",
    "    neighbors_idx: np.ndarray,\n",
    "    neighbors_dist: np.ndarray,\n",
    "    tokens: List[str],\n",
    "    sim_threshold: float\n",
    ") -> pd.DataFrame:\n",
    "    n, k = neighbors_idx.shape\n",
    "    if k < 2:\n",
    "        return pd.DataFrame(columns=[\"token_a\", \"token_b\", \"cosine_sim\", \"rank\"])\n",
    "    idx_sub = neighbors_idx[:, 1:]\n",
    "    sims = 1.0 - neighbors_dist[:, 1:]\n",
    "    mask = sims >= sim_threshold\n",
    "    counts = mask.sum(axis=1).astype(np.int64)\n",
    "    total = int(counts.sum())\n",
    "    if total == 0:\n",
    "        return pd.DataFrame(columns=[\"token_a\", \"token_b\", \"cosine_sim\", \"rank\"])\n",
    "    row_ids = np.repeat(np.arange(n, dtype=np.int32), counts)\n",
    "    col_ids = idx_sub[mask].astype(np.int32, copy=False)\n",
    "    sim_vals = sims[mask].astype(np.float32, copy=False)\n",
    "    offsets = np.empty(n + 1, dtype=np.int64)\n",
    "    offsets[0] = 0\n",
    "    np.cumsum(counts, out=offsets[1:])\n",
    "    ranks = np.empty(total, dtype=np.int32)\n",
    "    for i in range(n):  # intentional tiny loop; assigning ranks per row\n",
    "        s, e = offsets[i], offsets[i + 1]\n",
    "        if e > s:\n",
    "            ranks[s:e] = np.arange(1, e - s + 1, dtype=np.int32)\n",
    "    tok = np.asarray(tokens, dtype=object)\n",
    "    return pd.DataFrame({\n",
    "        \"token_a\": tok[row_ids],\n",
    "        \"token_b\": tok[col_ids],\n",
    "        \"cosine_sim\": sim_vals,\n",
    "        \"rank\": ranks\n",
    "    })\n",
    "\n",
    "def maybe_add_string_metrics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    target = df if METRICS_SAMPLE_N is None else df.sample(n=min(METRICS_SAMPLE_N, len(df)), random_state=42).copy()\n",
    "    if USE_RAPIDFUZZ:\n",
    "        try:\n",
    "            from rapidfuzz.fuzz import ratio as fuzz_ratio  # type: ignore\n",
    "            target[\"seq_ratio\"] = [\n",
    "                round(fuzz_ratio(a, b) / 100.0, 4) for a, b in zip(target[\"token_a\"], target[\"token_b\"])\n",
    "            ]\n",
    "        except Exception:\n",
    "            from difflib import SequenceMatcher\n",
    "            target[\"seq_ratio\"] = [\n",
    "                round(SequenceMatcher(None, a, b, autojunk=False).ratio(), 4)\n",
    "                for a, b in zip(target[\"token_a\"], target[\"token_b\"])\n",
    "            ]\n",
    "    else:\n",
    "        from difflib import SequenceMatcher\n",
    "        target[\"seq_ratio\"] = [\n",
    "            round(SequenceMatcher(None, a, b, autojunk=False).ratio(), 4)\n",
    "            for a, b in zip(target[\"token_a\"], target[\"token_b\"])\n",
    "        ]\n",
    "    target[\"len_a\"] = target[\"token_a\"].str.len()\n",
    "    target[\"len_b\"] = target[\"token_b\"].str.len()\n",
    "    target[\"len_diff\"] = (target[\"len_a\"] - target[\"len_b\"]).abs()\n",
    "    return df.merge(\n",
    "        target[[\"token_a\", \"token_b\", \"seq_ratio\", \"len_a\", \"len_b\", \"len_diff\"]],\n",
    "        on=[\"token_a\", \"token_b\"], how=\"left\"\n",
    "    )\n",
    "\n",
    "# ======================\n",
    "# Main\n",
    "# ======================\n",
    "def main() -> None:\n",
    "    model = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" if USE_MULTILINGUAL else MODEL_NAME\n",
    "\n",
    "    log_info(\"üìã CONFIGURATION (Embeddings + Tuning + Fast Pairs)\")\n",
    "    log_info(f\"  Model: {model}\")\n",
    "    log_info(f\"  Batch size: {BATCH_SIZE} | Normalize: {NORMALIZE}\")\n",
    "    log_info(f\"  HNSW: M={HNSW_M}, efC={HNSW_EF_CONSTRUCTION}, efQ={HNSW_EF}\")\n",
    "    log_info(f\"  Top-K neighbors: {TOP_K_NEIGHBORS}\")\n",
    "    log_info(f\"  Tokens: {len(canonical_tokens):,}\")\n",
    "    log_info(f\"  Tuning: {ENABLE_THRESHOLD_TUNING} | CSV: {EVAL_CSV_PATH or 'None'} | minP={EVAL_MIN_PRECISION} | minR={EVAL_MIN_RECALL}\")\n",
    "    log_info(f\"  Autogen eval: {AUTOGEN_EVAL_IF_MISSING} (pos={AUTOGEN_TARGET_POS}, neg={AUTOGEN_TARGET_NEG})\")\n",
    "    log_info(f\"  Extra metrics: {ADD_STRING_METRICS} | RapidFuzz: {USE_RAPIDFUZZ} | sample={METRICS_SAMPLE_N}\")\n",
    "    log_memory()\n",
    "\n",
    "    # Load model\n",
    "    log_stage(\"LOADING EMBEDDING MODEL\")\n",
    "    t0 = time.time()\n",
    "    embedder = load_embedder(model)\n",
    "    dim = embedder.get_sentence_embedding_dimension()\n",
    "    log_info(f\"‚úÖ Model loaded. Embedding dim: {dim}\")\n",
    "    log_time(\"Model load\", t0)\n",
    "\n",
    "    # Threshold tuning\n",
    "    tuned_threshold = None\n",
    "    eval_used_autogen = False\n",
    "    eval_metrics_path = OUTPUTS_DIR / \"similarity_threshold_metrics.csv\"\n",
    "    eval_summary_path = OUTPUTS_DIR / \"similarity_threshold_summary.json\"\n",
    "\n",
    "    if ENABLE_THRESHOLD_TUNING:\n",
    "        log_stage(\"THRESHOLD TUNING (QUICK EVAL)\")\n",
    "        pos_pairs: List[Tuple[str, str]] = []\n",
    "        neg_pairs: List[Tuple[str, str]] = []\n",
    "\n",
    "        if EVAL_CSV_PATH:\n",
    "            pos_pairs, neg_pairs = load_eval_pairs_from_csv(EVAL_CSV_PATH)\n",
    "        else:\n",
    "            pp, nn = gather_eval_pairs_from_namespace()\n",
    "            pos_pairs, neg_pairs = pp, nn\n",
    "\n",
    "        if (not pos_pairs or not neg_pairs) and AUTOGEN_EVAL_IF_MISSING:\n",
    "            log_info(\"‚ÑπÔ∏è No eval pairs supplied. Auto-generating a small eval set from tokens...\")\n",
    "            pos_pairs, neg_pairs, stats = autogen_eval_pairs([t for t in map(str, canonical_tokens) if t])\n",
    "            eval_used_autogen = True\n",
    "            log_info(f\"‚úÖ Auto-generated eval pairs: pos={len(pos_pairs)}, neg={len(neg_pairs)} (neg iters={stats['neg_iters']})\")\n",
    "\n",
    "        if (not pos_pairs) or (not neg_pairs):\n",
    "            log_info(\"‚ö†Ô∏è  No eval pairs found. Skipping tuning and keeping configured threshold.\")\n",
    "        else:\n",
    "            t_eval = time.time()\n",
    "            scores, labels = embed_eval_pairs(embedder, pos_pairs, neg_pairs, BATCH_SIZE, NORMALIZE)\n",
    "            metrics_df = sweep_thresholds(scores, labels, EVAL_GRID_START, EVAL_GRID_END, EVAL_GRID_STEP)\n",
    "            choices = pick_threshold(metrics_df, EVAL_MIN_PRECISION, EVAL_MIN_RECALL)\n",
    "            tuned_threshold = choices[\"best_at_min_precision\"] or choices[\"best_at_min_recall\"] or choices[\"best_f1\"]\n",
    "            metrics_df.to_csv(eval_metrics_path, index=False)\n",
    "            with open(eval_summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump({\n",
    "                    \"grid\": [EVAL_GRID_START, EVAL_GRID_END, EVAL_GRID_STEP],\n",
    "                    \"min_precision\": EVAL_MIN_PRECISION,\n",
    "                    \"min_recall\": EVAL_MIN_RECALL,\n",
    "                    \"choices\": choices,\n",
    "                    \"picked_threshold\": tuned_threshold,\n",
    "                    \"pos_pairs\": len(pos_pairs),\n",
    "                    \"neg_pairs\": len(neg_pairs),\n",
    "                    \"autogen_used\": eval_used_autogen\n",
    "                }, f, indent=2)\n",
    "            log_time(\"Tuning\", t_eval)\n",
    "            log_info(f\"‚úÖ Tuned threshold = {tuned_threshold:.3f}  (choices={choices})\")\n",
    "\n",
    "    # Embeddings for full token set\n",
    "    log_stage(\"EMBEDDING CANONICAL TOKENS\")\n",
    "    t1 = time.time()\n",
    "    X = encode_texts(embedder, canonical_tokens, BATCH_SIZE, NORMALIZE)  # shape: (n, dim)\n",
    "    log_time(\"Embedding\", t1)\n",
    "    log_info(f\"‚úÖ Embeddings: {X.shape[0]:,} √ó {X.shape[1]:,} (float32)\")\n",
    "    log_memory()\n",
    "\n",
    "    # Build & query ANN\n",
    "    log_stage(\"BUILDING ANN INDEX & RETRIEVING NEIGHBORS\")\n",
    "    k = min(TOP_K_NEIGHBORS + 1, X.shape[0])\n",
    "    t2 = time.time()\n",
    "    index = build_hnsw(X, HNSW_M, HNSW_EF_CONSTRUCTION, HNSW_EF, k)\n",
    "    idx, dist = knn_query_batched(index, X, k=k, batch=QUERY_BATCH_SIZE)\n",
    "    log_time(\"HNSW build + KNN\", t2)\n",
    "    log_info(\"‚úÖ ANN completed\")\n",
    "    log_memory()\n",
    "\n",
    "    # Use tuned threshold if available\n",
    "    threshold = float(tuned_threshold) if tuned_threshold is not None else float(SIMILARITY_THRESHOLD)\n",
    "    log_info(f\"üîé Using similarity threshold: {threshold:.3f}\")\n",
    "\n",
    "    # Generate candidate pairs (vectorized)\n",
    "    log_stage(\"GENERATING CANDIDATE PAIRS (FAST, VECTORIZED)\")\n",
    "    t3 = time.time()\n",
    "    pairs_df = generate_pairs_fast(neighbors_idx=idx, neighbors_dist=dist,\n",
    "                                   tokens=list(canonical_tokens), sim_threshold=threshold)\n",
    "    log_time(\"Candidate pair generation (vectorized)\", t3)\n",
    "    log_info(f\"üìä Candidate pairs: {len(pairs_df):,}\")\n",
    "    log_memory()\n",
    "\n",
    "    # Optional extra metrics\n",
    "    if ADD_STRING_METRICS and not pairs_df.empty:\n",
    "        log_stage(\"ADDING STRING METRICS (OPTIONAL)\")\n",
    "        t4 = time.time()\n",
    "        pairs_df = maybe_add_string_metrics(pairs_df)\n",
    "        log_time(\"Extra string metrics\", t4)\n",
    "        log_info(\"‚úÖ Metrics added\")\n",
    "\n",
    "    # Save outputs\n",
    "    log_stage(\"SAVING RESULTS\")\n",
    "    pairs_path = OUTPUTS_DIR / \"candidate_similarity_pairs.parquet\"\n",
    "    sample_path = OUTPUTS_DIR / \"similarity_sample_inspection.parquet\"\n",
    "    meta_path = OUTPUTS_DIR / \"cell3_similarity_metadata.json\"\n",
    "\n",
    "    pairs_df.to_parquet(pairs_path, index=False)\n",
    "    sample_n = min(5000, len(pairs_df))\n",
    "    if sample_n > 0:\n",
    "        pairs_df.sample(n=sample_n, random_state=42).to_parquet(sample_path, index=False)\n",
    "\n",
    "    meta = {\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"config\": {\n",
    "            \"model\": model,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"normalize\": NORMALIZE,\n",
    "            \"top_k_neighbors\": TOP_K_NEIGHBORS,\n",
    "            \"similarity_threshold_initial\": SIMILARITY_THRESHOLD,\n",
    "            \"similarity_threshold_used\": threshold,\n",
    "            \"hnsw\": {\"M\": HNSW_M, \"ef_construction\": HNSW_EF_CONSTRUCTION, \"ef\": HNSW_EF},\n",
    "            \"query_batch_size\": QUERY_BATCH_SIZE,\n",
    "            \"tuning\": {\n",
    "                \"enabled\": ENABLE_THRESHOLD_TUNING,\n",
    "                \"csv\": EVAL_CSV_PATH,\n",
    "                \"min_precision\": EVAL_MIN_PRECISION,\n",
    "                \"min_recall\": EVAL_MIN_RECALL,\n",
    "                \"grid\": [EVAL_GRID_START, EVAL_GRID_END, EVAL_GRID_STEP],\n",
    "                \"metrics_csv\": str(eval_metrics_path) if ENABLE_THRESHOLD_TUNING else None,\n",
    "                \"summary_json\": str(eval_summary_path) if ENABLE_THRESHOLD_TUNING else None,\n",
    "                \"autogen_used\": eval_used_autogen\n",
    "            },\n",
    "            \"extras\": {\n",
    "                \"add_string_metrics\": ADD_STRING_METRICS,\n",
    "                \"use_rapidfuzz\": USE_RAPIDFUZZ,\n",
    "                \"metrics_sample_n\": METRICS_SAMPLE_N\n",
    "            }\n",
    "        },\n",
    "        \"stats\": {\n",
    "            \"total_tokens\": len(canonical_tokens),\n",
    "            \"embedding_dim\": int(X.shape[1]),\n",
    "            \"candidate_pairs_found\": len(pairs_df)\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"candidate_pairs_file\": str(pairs_path),\n",
    "            \"sample_file\": str(sample_path) if sample_n > 0 else None\n",
    "        }\n",
    "    }\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2, ensure_ascii=False)\n",
    "    log_info(f\"‚úÖ Saved pairs: {pairs_path}\")\n",
    "    if sample_n > 0:\n",
    "        log_info(f\"‚úÖ Saved sample: {sample_path}\")\n",
    "    if ENABLE_THRESHOLD_TUNING:\n",
    "        if eval_used_autogen:\n",
    "            log_info(f\"‚úÖ Saved tuning metrics: {eval_metrics_path} (auto-generated eval set)\")\n",
    "            log_info(f\"‚úÖ Saved tuning summary: {eval_summary_path}\")\n",
    "        else:\n",
    "            log_info(f\"‚úÖ Saved tuning metrics: {eval_metrics_path}\")\n",
    "            log_info(f\"‚úÖ Saved tuning summary: {eval_summary_path}\")\n",
    "    log_info(f\"‚úÖ Metadata: {meta_path}\")\n",
    "\n",
    "    # Summary\n",
    "    log_stage(\"SUMMARY STATISTICS\")\n",
    "    if not pairs_df.empty:\n",
    "        sims = pairs_df[\"cosine_sim\"].to_numpy()\n",
    "        log_info(f\"Cosine similarity: min={float(sims.min()):.3f}, mean={float(sims.mean()):.3f}, max={float(sims.max()):.3f}\")\n",
    "        covered = len(set(pairs_df[\"token_a\"]).union(pairs_df[\"token_b\"])) / len(canonical_tokens) * 100\n",
    "        log_info(f\"Token coverage: {covered:.1f}%\")\n",
    "    log_info(\"üéâ Cell 3 completed with pretrained embeddings, auto-tuning, and fast pair generation!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîé OUTPUT FILES INVENTORY\n",
      "================================================================================\n",
      "romance-novel-nlp-research/src/eda_analysis/outputs/candidate_similarity_pairs.parquet | exists=True | size=176.77 MB\n",
      "romance-novel-nlp-research/src/eda_analysis/outputs/similarity_sample_inspection.parquet | exists=True | size=181.64 KB\n",
      "romance-novel-nlp-research/src/eda_analysis/outputs/similarity_threshold_metrics.csv | exists=True | size=6.67 KB\n",
      "romance-novel-nlp-research/src/eda_analysis/outputs/similarity_threshold_summary.json | exists=True | size=339.00 B\n",
      "romance-novel-nlp-research/src/eda_analysis/outputs/cell3_similarity_metadata.json | exists=True | size=1.27 KB\n",
      "================================================================================\n",
      "üîé METADATA + TUNING SUMMARY (QUICK VIEW)\n",
      "================================================================================\n",
      "{\n",
      "  \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
      "  \"batch_size\": 1024,\n",
      "  \"normalize\": true,\n",
      "  \"top_k_neighbors\": 50,\n",
      "  \"similarity_threshold_initial\": 0.3,\n",
      "  \"similarity_threshold_used\": 0.44999992847442627,\n",
      "  \"hnsw\": {\n",
      "    \"M\": 16,\n",
      "    \"ef_construction\": 200,\n",
      "    \"ef\": 200\n",
      "  },\n",
      "  \"query_batch_size\": 25000,\n",
      "  \"tuning\": {\n",
      "    \"enabled\": true,\n",
      "    \"csv\": null,\n",
      "    \"min_precision\": 0.9,\n",
      "    \"min_recall\": null,\n",
      "    \"grid\": [\n",
      "      0.05,\n",
      "      0.95,\n",
      "      0.01\n",
      "    ],\n",
      "    \"metrics_csv\": \"romance-novel-nlp-research/src/eda_analysis/outputs/similarity_threshold_metrics.csv\",\n",
      "    \"summary_json\": \"romance-novel-nlp-research/src/eda_analysis/outputs/similarity_threshold_summary.json\",\n",
      "    \"autogen_used\": true\n",
      "  },\n",
      "  \"extras\": {\n",
      "    \"add_string_metrics\": false,\n",
      "    \"use_rapidfuzz\": true,\n",
      "    \"metrics_sample_n\": null\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"total_tokens\": 254778,\n",
      "  \"embedding_dim\": 384,\n",
      "  \"candidate_pairs_found\": 12502616\n",
      "}\n",
      "Tuning summary:\n",
      "{\n",
      "  \"grid\": [\n",
      "    0.05,\n",
      "    0.95,\n",
      "    0.01\n",
      "  ],\n",
      "  \"min_precision\": 0.9,\n",
      "  \"min_recall\": null,\n",
      "  \"choices\": {\n",
      "    \"best_f1\": 0.44999992847442627,\n",
      "    \"best_at_min_precision\": 0.44999992847442627,\n",
      "    \"best_at_min_recall\": null\n",
      "  },\n",
      "  \"picked_threshold\": 0.44999992847442627,\n",
      "  \"pos_pairs\": 633,\n",
      "  \"neg_pairs\": 1500,\n",
      "  \"autogen_used\": true\n",
      "}\n",
      "\n",
      "Metrics CSV (first 10 rows):\n",
      " threshold  precision  recall       f1  support_pos  support_neg\n",
      "      0.05   0.322301     1.0 0.487486          633         1500\n",
      "      0.06   0.330203     1.0 0.496471          633         1500\n",
      "      0.07   0.338865     1.0 0.506198          633         1500\n",
      "      0.08   0.348376     1.0 0.516735          633         1500\n",
      "      0.09   0.360684     1.0 0.530151          633         1500\n",
      "      0.10   0.374778     1.0 0.545220          633         1500\n",
      "      0.11   0.394638     1.0 0.565937          633         1500\n",
      "      0.12   0.414267     1.0 0.585840          633         1500\n",
      "      0.13   0.435351     1.0 0.606612          633         1500\n",
      "      0.14   0.462381     1.0 0.632368          633         1500\n",
      "\n",
      "Best F1 threshold: 0.450 | P=0.998 R=0.989\n",
      "Best @P>=0.90: thr=0.450 | P=0.998 R=0.989 F1=0.994\n",
      "================================================================================\n",
      "üîé SAMPLE PARQUET QUICK PEEK\n",
      "================================================================================\n",
      "Sample rows: 5,000\n",
      "\n",
      "Head(5):\n",
      "                 token_a                 token_b  cosine_sim  rank\n",
      "           santina crown  he who wears the crown    0.524262    20\n",
      "             no cheaters cheater cheater cheater    0.729189    13\n",
      "men in uniforms military   military commando men    0.741850    49\n",
      "       honestly loved it        fucking loved it    0.757635    13\n",
      "              the us pnw             us new york    0.497495    24\n",
      "\n",
      "Tail(5):\n",
      "             token_a                 token_b  cosine_sim  rank\n",
      "     readinglist2012       reading list 2013    0.781855    10\n",
      "           a01 p q r               a01 m n o    0.703537     1\n",
      "     ill try romance romance recommendations    0.777703     3\n",
      "     serie y o sagas          serie en cours    0.580652    47\n",
      "set in north america            set in china    0.730287    28\n",
      "\n",
      "Random(5):\n",
      "                          token_a                     token_b  cosine_sim  rank\n",
      "                        poly voce                 pan bi poly    0.548202    19\n",
      "ride em cowboys ranchers westerns             ride em cowboys    0.800737    25\n",
      "                     other novels     other books in a series    0.774916    26\n",
      " under reading challenge pressure 2017 reading challenge read    0.749680    43\n",
      "                   north caralina                 nova scotia    0.583393    25\n",
      "\n",
      "Sample cosine_sim stats:\n",
      "count    5000.000000\n",
      "mean        0.678531\n",
      "std         0.112927\n",
      "min         0.450445\n",
      "25%         0.591874\n",
      "50%         0.676621\n",
      "75%         0.758738\n",
      "90%         0.831412\n",
      "95%         0.871575\n",
      "99%         0.935160\n",
      "max         0.986127\n",
      "\n",
      "Sample rows with short tokens (<=3 chars): 111\n",
      "          token_a       token_b  cosine_sim  rank  _len_a  _len_b\n",
      "           pub 13           pub    0.786807    20       6       3\n",
      "                z           m z    0.774189    45       1       3\n",
      "           z17 03           z14    0.759807    27       6       3\n",
      "              aer      aer hist    0.757716     4       3       8\n",
      "               04 04 april 2017    0.752407    18       2      13\n",
      "didnt like or dnf           dnf    0.749796    36      17       3\n",
      "              7th           6 7    0.749518     6       3       3\n",
      "              s d             s    0.749025     9       3       1\n",
      "              sea    sea travel    0.748443    11       3      10\n",
      "        ace books           ace    0.745586     6       9       3\n",
      "\n",
      "Low RapidFuzz (<0.4) but high cosine (>=0.8) in 1k sample: 5\n",
      "                    token_a                          token_b  cosine_sim  rf_ratio\n",
      "     cop pi law enforcement                 cop detective pi    0.871884  0.368421\n",
      "physical illness disability disability disorder non physical    0.826748  0.338983\n",
      "               utc mar 2017              05 2017 mm utc june    0.825825  0.322581\n",
      "giveaway entered didn t win                won in a giveaway    0.822898  0.363636\n",
      "   disabled or scarred hero           hero with a disability    0.822356  0.260870\n",
      "================================================================================\n",
      "üîé FULL PAIRS EXPLORATION (DuckDB/Arrow)\n",
      "================================================================================\n",
      "DuckDB not available. Using PyArrow dataset scan (chunked).\n",
      "[GLOBAL] rows=12,502,616 min=0.450 mean=0.678 max=1.000\n",
      "[HISTOGRAM 0.45..1.0 step 0.05]\n",
      "0.45‚Äì0.50: 645,846\n",
      "0.50‚Äì0.55: 1,193,308\n",
      "0.55‚Äì0.60: 1,597,857\n",
      "0.60‚Äì0.65: 1,868,188\n",
      "0.65‚Äì0.70: 1,951,008\n",
      "0.70‚Äì0.75: 1,792,148\n",
      "0.75‚Äì0.80: 1,478,108\n",
      "0.80‚Äì0.85: 1,061,053\n",
      "0.85‚Äì0.90: 598,069\n",
      "0.90‚Äì0.95: 245,451\n",
      "0.95‚Äì1.00: 71,580\n",
      "[SHORT TOKENS] rows_short = 291,445 (of 12,502,616)\n",
      "Scan time: 51.64s\n",
      "================================================================================\n",
      "üîé ACTION FLAGS (NEXT FEATURE-ENGINEERING STEPS)\n",
      "================================================================================\n",
      "- Threshold in use: 0.44999992847442627\n",
      "- Inspect histogram & borderline pairs (0.45‚Äì0.50). If many questionable, raise threshold (e.g., +0.05).\n",
      "- Check TOP HUBS. If heavy tails (p99 deg >> p50), cap neighbors per token (e.g., keep top-10 by sim per token).\n",
      "- If short tokens appear frequently, add a short-token guard: require len>=4 or higher threshold for len<=3.\n",
      "- Consider symmetry filter: keep A‚ÜîB only if both sides rank <= R (e.g., R=10) to drop one-sided noise.\n",
      "- Optionally add RapidFuzz gate on short strings: rf_ratio>=0.6 when min(len_a,len_b)<=4.\n",
      "- Next build clusters (connected components on pairs) to produce canonical groups & representatives.\n",
      "================================================================================\n",
      "üîé DONE ‚Äî Review above prints to pick concrete thresholds/filters.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# romance-novel-nlp-research/src/eda_analysis/cell3a_explore_outputs.py\n",
    "# Heavy-print exploratory audit of Cell 3 outputs. Safe on large parquet via DuckDB/Arrow.\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, json, math, time, shutil, sys\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- Paths ----------\n",
    "OUT = Path(\"romance-novel-nlp-research/src/eda_analysis/outputs\")\n",
    "PAIRS = OUT / \"candidate_similarity_pairs.parquet\"\n",
    "SAMPLE = OUT / \"similarity_sample_inspection.parquet\"\n",
    "METRICS_CSV = OUT / \"similarity_threshold_metrics.csv\"\n",
    "SUMMARY_JSON = OUT / \"similarity_threshold_summary.json\"\n",
    "META_JSON = OUT / \"cell3_similarity_metadata.json\"\n",
    "\n",
    "# ---------- Utils ----------\n",
    "def human(n: int) -> str:\n",
    "    units = [\"B\",\"KB\",\"MB\",\"GB\",\"TB\"]\n",
    "    i = 0\n",
    "    x = float(n)\n",
    "    while x >= 1024 and i < len(units)-1:\n",
    "        x /= 1024; i += 1\n",
    "    return f\"{x:.2f} {units[i]}\"\n",
    "\n",
    "def p(s: str) -> None:\n",
    "    print(s); sys.stdout.flush()\n",
    "\n",
    "def print_header(title: str) -> None:\n",
    "    sep = \"=\" * 80\n",
    "    p(sep); p(f\"üîé {title}\"); p(sep)\n",
    "\n",
    "def file_info(path: Path) -> str:\n",
    "    return f\"{path} | exists={path.exists()} | size={human(path.stat().st_size) if path.exists() else '‚Äî'}\"\n",
    "\n",
    "def try_import(mod: str):\n",
    "    try:\n",
    "        return __import__(mod)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "duckdb = try_import(\"duckdb\")\n",
    "pyarrow = try_import(\"pyarrow\")\n",
    "pa_ds = None\n",
    "if pyarrow:\n",
    "    try:\n",
    "        import pyarrow.dataset as pa_ds\n",
    "    except Exception:\n",
    "        pa_ds = None\n",
    "\n",
    "try:\n",
    "    from rapidfuzz.fuzz import ratio as fuzz_ratio\n",
    "    HAS_RAPIDFUZZ = True\n",
    "except Exception:\n",
    "    HAS_RAPIDFUZZ = False\n",
    "\n",
    "# ---------- 0) Print file inventory ----------\n",
    "print_header(\"OUTPUT FILES INVENTORY\")\n",
    "p(file_info(PAIRS))\n",
    "p(file_info(SAMPLE))\n",
    "p(file_info(METRICS_CSV))\n",
    "p(file_info(SUMMARY_JSON))\n",
    "p(file_info(META_JSON))\n",
    "\n",
    "# ---------- 1) Load and print metadata + tuning summary ----------\n",
    "print_header(\"METADATA + TUNING SUMMARY (QUICK VIEW)\")\n",
    "meta = {}\n",
    "if META_JSON.exists():\n",
    "    meta = json.loads(META_JSON.read_text())\n",
    "    p(json.dumps(meta.get(\"config\", {}), indent=2))\n",
    "    p(json.dumps(meta.get(\"stats\", {}), indent=2))\n",
    "else:\n",
    "    p(\"No metadata JSON found.\")\n",
    "\n",
    "if SUMMARY_JSON.exists():\n",
    "    summ = json.loads(SUMMARY_JSON.read_text())\n",
    "    p(\"Tuning summary:\")\n",
    "    p(json.dumps(summ, indent=2))\n",
    "else:\n",
    "    p(\"No tuning summary JSON found.\")\n",
    "\n",
    "if METRICS_CSV.exists():\n",
    "    p(\"\\nMetrics CSV (first 10 rows):\")\n",
    "    try:\n",
    "        mdf = pd.read_csv(METRICS_CSV)\n",
    "        p(mdf.head(10).to_string(index=False))\n",
    "        # Print best rows\n",
    "        best_f1_row = mdf.iloc[mdf[\"f1\"].values.argmax()]\n",
    "        p(f\"\\nBest F1 threshold: {best_f1_row['threshold']:.3f} | P={best_f1_row['precision']:.3f} R={best_f1_row['recall']:.3f}\")\n",
    "        if \"precision\" in mdf.columns:\n",
    "            hi_p = mdf[mdf[\"precision\"] >= 0.90]\n",
    "            if not hi_p.empty:\n",
    "                r = hi_p.iloc[hi_p[\"f1\"].values.argmax()]\n",
    "                p(f\"Best @P>=0.90: thr={r['threshold']:.3f} | P={r['precision']:.3f} R={r['recall']:.3f} F1={r['f1']:.3f}\")\n",
    "    except Exception as e:\n",
    "        p(f\"‚ö†Ô∏è Failed reading metrics CSV: {e}\")\n",
    "else:\n",
    "    p(\"No metrics CSV found.\")\n",
    "\n",
    "# ---------- 2) Peek at SAMPLE parquet ----------\n",
    "print_header(\"SAMPLE PARQUET QUICK PEEK\")\n",
    "if SAMPLE.exists():\n",
    "    sdf = pd.read_parquet(SAMPLE)\n",
    "    p(f\"Sample rows: {len(sdf):,}\")\n",
    "    p(\"\\nHead(5):\"); p(sdf.head(5).to_string(index=False))\n",
    "    p(\"\\nTail(5):\"); p(sdf.tail(5).to_string(index=False))\n",
    "    p(\"\\nRandom(5):\"); p(sdf.sample(min(5, len(sdf)), random_state=42).to_string(index=False))\n",
    "    # Basic stats on sample\n",
    "    if \"cosine_sim\" in sdf.columns:\n",
    "        p(\"\\nSample cosine_sim stats:\")\n",
    "        p(sdf[\"cosine_sim\"].describe(percentiles=[.25,.5,.75,.9,.95,.99]).to_string())\n",
    "    # Short tokens diagnostics on sample\n",
    "    sdf[\"_len_a\"] = sdf[\"token_a\"].str.len()\n",
    "    sdf[\"_len_b\"] = sdf[\"token_b\"].str.len()\n",
    "    short_cut = sdf[(sdf[\"_len_a\"] <= 3) | (sdf[\"_len_b\"] <= 3)]\n",
    "    p(f\"\\nSample rows with short tokens (<=3 chars): {len(short_cut):,}\")\n",
    "    if not short_cut.empty:\n",
    "        p(short_cut.nlargest(10, \"cosine_sim\")[[\"token_a\",\"token_b\",\"cosine_sim\",\"rank\",\"_len_a\",\"_len_b\"]].to_string(index=False))\n",
    "    # RapidFuzz spot-check (sample) to detect semantic vs surface mismatch\n",
    "    if HAS_RAPIDFUZZ:\n",
    "        tmp = sdf.sample(min(1000, len(sdf)), random_state=123).copy()\n",
    "        tmp[\"rf_ratio\"] = [fuzz_ratio(a,b)/100.0 for a,b in zip(tmp[\"token_a\"], tmp[\"token_b\"])]\n",
    "        low_char_high_sem = tmp[(tmp[\"rf_ratio\"] < 0.4) & (tmp[\"cosine_sim\"] >= 0.8)]\n",
    "        p(f\"\\nLow RapidFuzz (<0.4) but high cosine (>=0.8) in 1k sample: {len(low_char_high_sem):,}\")\n",
    "        if not low_char_high_sem.empty:\n",
    "            p(low_char_high_sem.nlargest(20, \"cosine_sim\")[[\"token_a\",\"token_b\",\"cosine_sim\",\"rf_ratio\"]].to_string(index=False))\n",
    "else:\n",
    "    p(\"No sample parquet found.\")\n",
    "\n",
    "# ---------- 3) FULL PAIRS exploration (DuckDB preferred) ----------\n",
    "print_header(\"FULL PAIRS EXPLORATION (DuckDB/Arrow)\")\n",
    "\n",
    "if duckdb and PAIRS.exists():\n",
    "    p(\"Using DuckDB for SQL on parquet (no full load).\")\n",
    "    con = duckdb.connect()\n",
    "    con.execute(\"PRAGMA threads=%d\" % max(1, os.cpu_count() or 4))\n",
    "    con.execute(\"SET memory_limit='80%';\")\n",
    "    con.execute(\"INSTALL json; LOAD json;\")  # harmless if already loaded\n",
    "\n",
    "    # Global stats\n",
    "    p(\"\\n[GLOBAL]\")\n",
    "    q = f\"\"\"\n",
    "        SELECT\n",
    "          COUNT(*) AS rows,\n",
    "          MIN(cosine_sim) AS min_sim,\n",
    "          AVG(cosine_sim) AS mean_sim,\n",
    "          MAX(cosine_sim) AS max_sim\n",
    "        FROM read_parquet('{PAIRS.as_posix()}')\n",
    "    \"\"\"\n",
    "    p(con.execute(q).df().to_string(index=False))\n",
    "\n",
    "    # Histogram bins\n",
    "    p(\"\\n[HISTOGRAM 0.45..1.0, bin=0.05]\")\n",
    "    q = f\"\"\"\n",
    "        WITH binned AS (\n",
    "          SELECT\n",
    "            CAST(FLOOR((cosine_sim - 0.45) / 0.05) AS INTEGER) AS bin_id\n",
    "          FROM read_parquet('{PAIRS.as_posix()}')\n",
    "          WHERE cosine_sim >= 0.45\n",
    "        )\n",
    "        SELECT bin_id,\n",
    "               0.45 + bin_id*0.05 AS bin_start,\n",
    "               0.45 + (bin_id+1)*0.05 AS bin_end,\n",
    "               COUNT(*) AS cnt\n",
    "        FROM binned\n",
    "        GROUP BY 1\n",
    "        ORDER BY 1\n",
    "    \"\"\"\n",
    "    p(con.execute(q).df().to_string(index=False))\n",
    "\n",
    "    # Top hubs (many neighbors per token_a)\n",
    "    p(\"\\n[TOP HUBS by degree (token_a count)]\")\n",
    "    q = f\"\"\"\n",
    "        SELECT token_a, COUNT(*) AS deg\n",
    "        FROM read_parquet('{PAIRS.as_posix()}')\n",
    "        GROUP BY token_a\n",
    "        ORDER BY deg DESC\n",
    "        LIMIT 30\n",
    "    \"\"\"\n",
    "    p(con.execute(q).df().to_string(index=False))\n",
    "\n",
    "    # Very short tokens involvement\n",
    "    p(\"\\n[SHORT TOKENS involvement (<=3 chars)]\")\n",
    "    q = f\"\"\"\n",
    "        SELECT COUNT(*) AS rows_short\n",
    "        FROM read_parquet('{PAIRS.as_posix()}')\n",
    "        WHERE length(token_a) <= 3 OR length(token_b) <= 3\n",
    "    \"\"\"\n",
    "    p(con.execute(q).df().to_string(index=False))\n",
    "\n",
    "    # Extremes for manual inspection\n",
    "    p(\"\\n[TOP 20 pairs by similarity]\")\n",
    "    q = f\"\"\"\n",
    "        SELECT token_a, token_b, cosine_sim, rank\n",
    "        FROM read_parquet('{PAIRS.as_posix()}')\n",
    "        ORDER BY cosine_sim DESC\n",
    "        LIMIT 20\n",
    "    \"\"\"\n",
    "    p(con.execute(q).df().to_string(index=False))\n",
    "\n",
    "    p(\"\\n[BORDERLINE near threshold 0.45..0.47 (20 rows)]\")\n",
    "    q = f\"\"\"\n",
    "        SELECT token_a, token_b, cosine_sim, rank\n",
    "        FROM read_parquet('{PAIRS.as_posix()}')\n",
    "        WHERE cosine_sim BETWEEN 0.45 AND 0.47\n",
    "        ORDER BY cosine_sim ASC\n",
    "        LIMIT 20\n",
    "    \"\"\"\n",
    "    p(con.execute(q).df().to_string(index=False))\n",
    "\n",
    "    # Optional: cap per-token neighbors gauge\n",
    "    p(\"\\n[DEGREE QUANTILES for token_a]\")\n",
    "    q = f\"\"\"\n",
    "        SELECT\n",
    "          MIN(deg) AS min_deg,\n",
    "          AVG(deg) AS mean_deg,\n",
    "          MAX(deg) AS max_deg,\n",
    "          QUANTILE_CONT(deg, 0.50) AS p50,\n",
    "          QUANTILE_CONT(deg, 0.90) AS p90,\n",
    "          QUANTILE_CONT(deg, 0.99) AS p99\n",
    "        FROM (\n",
    "          SELECT token_a, COUNT(*) AS deg\n",
    "          FROM read_parquet('{PAIRS.as_posix()}')\n",
    "          GROUP BY token_a\n",
    "        )\n",
    "    \"\"\"\n",
    "    p(con.execute(q).df().to_string(index=False))\n",
    "\n",
    "    con.close()\n",
    "\n",
    "elif pa_ds and PAIRS.exists():\n",
    "    p(\"DuckDB not available. Using PyArrow dataset scan (chunked).\")\n",
    "    import pyarrow.dataset as ds\n",
    "    dsobj = ds.dataset(PAIRS.as_posix(), format=\"parquet\")\n",
    "    scanner = dsobj.scanner(columns=[\"token_a\",\"token_b\",\"cosine_sim\",\"rank\"], batch_size=250_000)\n",
    "\n",
    "    n_rows = 0\n",
    "    min_sim, max_sim, sum_sim = 1.0, 0.0, 0.0\n",
    "    bins = np.zeros(11, dtype=np.int64)  # 0.45..1.0 step 0.05 + overflow/underflow bucket\n",
    "    short_rows = 0\n",
    "    t0 = time.time()\n",
    "    for batch in scanner.to_batches():\n",
    "        df = batch.to_pandas(types_mapper=None)\n",
    "        n = len(df); n_rows += n\n",
    "        cs = df[\"cosine_sim\"].to_numpy(np.float32, copy=False)\n",
    "        min_sim = min(min_sim, float(cs.min()))\n",
    "        max_sim = max(max_sim, float(cs.max()))\n",
    "        sum_sim += float(cs.sum())\n",
    "        # bins\n",
    "        idx = ((cs - 0.45) / 0.05).astype(np.int32)\n",
    "        for b in idx[(idx >= 0) & (idx < 11)]:\n",
    "            bins[int(b)] += 1\n",
    "        # short tokens\n",
    "        short_rows += int(((df[\"token_a\"].str.len() <= 3) | (df[\"token_b\"].str.len() <= 3)).sum())\n",
    "    mean_sim = sum_sim / max(1, n_rows)\n",
    "    p(f\"[GLOBAL] rows={n_rows:,} min={min_sim:.3f} mean={mean_sim:.3f} max={max_sim:.3f}\")\n",
    "    p(\"[HISTOGRAM 0.45..1.0 step 0.05]\")\n",
    "    for i, c in enumerate(bins):\n",
    "        p(f\"{0.45 + i*0.05:.2f}‚Äì{0.45 + (i+1)*0.05:.2f}: {int(c):,}\")\n",
    "    p(f\"[SHORT TOKENS] rows_short = {short_rows:,} (of {n_rows:,})\")\n",
    "    p(f\"Scan time: {time.time()-t0:.2f}s\")\n",
    "\n",
    "else:\n",
    "    p(\"‚ö†Ô∏è Neither DuckDB nor PyArrow available; using SAMPLE only. Install duckdb or pyarrow for full-file stats: `pip install duckdb pyarrow`.\")\n",
    "\n",
    "# ---------- 4) Action flags (what to tweak next) ----------\n",
    "print_header(\"ACTION FLAGS (NEXT FEATURE-ENGINEERING STEPS)\")\n",
    "# Heuristics based on observed stats from metadata/prints\n",
    "thr_used = None\n",
    "try:\n",
    "    thr_used = float(meta.get(\"config\", {}).get(\"similarity_threshold_used\", None))\n",
    "except Exception:\n",
    "    thr_used = None\n",
    "\n",
    "# Print deterministic guidance; adjust as needed after reading console outputs.\n",
    "p(f\"- Threshold in use: {thr_used if thr_used is not None else 'unknown'}\")\n",
    "p(\"- Inspect histogram & borderline pairs (0.45‚Äì0.50). If many questionable, raise threshold (e.g., +0.05).\")\n",
    "p(\"- Check TOP HUBS. If heavy tails (p99 deg >> p50), cap neighbors per token (e.g., keep top-10 by sim per token).\")\n",
    "p(\"- If short tokens appear frequently, add a short-token guard: require len>=4 or higher threshold for len<=3.\")\n",
    "p(\"- Consider symmetry filter: keep A‚ÜîB only if both sides rank <= R (e.g., R=10) to drop one-sided noise.\")\n",
    "p(\"- Optionally add RapidFuzz gate on short strings: rf_ratio>=0.6 when min(len_a,len_b)<=4.\")\n",
    "p(\"- Next build clusters (connected components on pairs) to produce canonical groups & representatives.\")\n",
    "\n",
    "print_header(\"DONE ‚Äî Review above prints to pick concrete thresholds/filters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "üîé INPUTS\n",
      "==========================================================================================\n",
      "romance-novel-nlp-research/src/eda_analysis/outputs/candidate_similarity_pairs.parquet | exists=True | size=176.77 MB\n",
      "romance-novel-nlp-research/src/eda_analysis/outputs/cell3_similarity_metadata.json | exists=True | size=1.27 KB\n",
      "\n",
      "Meta (config.stats excerpt):\n",
      "{\n",
      "  \"config\": {\n",
      "    \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
      "    \"batch_size\": 1024,\n",
      "    \"normalize\": true,\n",
      "    \"top_k_neighbors\": 50,\n",
      "    \"similarity_threshold_initial\": 0.3,\n",
      "    \"similarity_threshold_used\": 0.44999992847442627,\n",
      "    \"hnsw\": {\n",
      "      \"M\": 16,\n",
      "      \"ef_construction\": 200,\n",
      "      \"ef\": 200\n",
      "    },\n",
      "    \"query_batch_size\": 25000,\n",
      "    \"tuning\": {\n",
      "      \"enabled\": true,\n",
      "      \"csv\": null,\n",
      "      \"min_precision\": 0.9,\n",
      "      \"min_recall\": null,\n",
      "      \"grid\": [\n",
      "        0.05,\n",
      "        0.95,\n",
      "        0.01\n",
      "      ],\n",
      "      \"metrics_csv\": \"romance-novel-nlp-research/src/eda_analysis/outputs/similarity_threshold_metrics.csv\",\n",
      "      \"summary_json\": \"romance-novel-nlp-research/src/eda_analysis/outputs/similarity_threshold_summary.json\",\n",
      "      \"autogen_used\": true\n",
      "    },\n",
      "    \"extras\": {\n",
      "      \"add_string_metrics\": false,\n",
      "      \"use_rapidfuzz\": true,\n",
      "      \"metrics_sample_n\": null\n",
      "    }\n",
      "  },\n",
      "  \"stats\": {\n",
      "    \"total_tokens\": 254778,\n",
      "    \"embedding_dim\": 384,\n",
      "    \"candidate_pairs_found\": 12502616\n",
      "  }\n",
      "}\n",
      "==========================================================================================\n",
      "üîé BEFORE STATS (STREAMING)\n",
      "==========================================================================================\n",
      "Rows=12,502,616 | min=0.450 mean=0.678 max=1.000\n",
      "Histogram (pre-filter):\n",
      "0.45‚Äì0.50: 645,846\n",
      "0.50‚Äì0.55: 1,193,308\n",
      "0.55‚Äì0.60: 1,597,857\n",
      "0.60‚Äì0.65: 1,868,188\n",
      "0.65‚Äì0.70: 1,951,008\n",
      "0.70‚Äì0.75: 1,792,148\n",
      "0.75‚Äì0.80: 1,478,108\n",
      "0.80‚Äì0.85: 1,061,053\n",
      "0.85‚Äì0.90: 598,069\n",
      "0.90‚Äì0.95: 245,451\n",
      "0.95‚Äì1.00: 71,580\n",
      "Short-token rows (<= 3 chars): 291,445 (2.33%)\n",
      "Scan time: 27.61s\n",
      "==========================================================================================\n",
      "üîé BUILD REVERSE RANK MAP (rank‚â§10)\n",
      "==========================================================================================\n",
      "Reverse entries stored: 2,543,337 | time: 11.82s\n",
      "==========================================================================================\n",
      "üîé APPLY FILTERS (streaming) + PER-TOKEN TOP-K\n",
      "==========================================================================================\n",
      "Streamed rows: 12,502,616 | time: 69.07s\n",
      "==========================================================================================\n",
      "üîé MATERIALIZE FILTERED EDGES\n",
      "==========================================================================================\n",
      "Filtered pairs: 1,198,346  | tokens covered: 232,918\n",
      "==========================================================================================\n",
      "üîé AFTER STATS\n",
      "==========================================================================================\n",
      "Rows=1,198,346 | min=0.500 mean=0.810 max=1.000\n",
      "Histogram (post-filter):\n",
      "0.45‚Äì0.50: 0\n",
      "0.50‚Äì0.55: 5,234\n",
      "0.55‚Äì0.60: 22,792\n",
      "0.60‚Äì0.65: 55,410\n",
      "0.65‚Äì0.70: 100,058\n",
      "0.70‚Äì0.75: 148,834\n",
      "0.75‚Äì0.80: 186,558\n",
      "0.80‚Äì0.85: 213,350\n",
      "0.85‚Äì0.90: 221,596\n",
      "0.90‚Äì0.95: 173,916\n",
      "0.95‚Äì1.00: 70,598\n",
      "\n",
      "Top hubs (post-filter):\n",
      "token_a\n",
      "zzz 2014 books              10\n",
      "0 0 0 1                     10\n",
      "zzpurchased 05 2012         10\n",
      "zzcovercolor white          10\n",
      "zzcovercolor blue           10\n",
      "book covers body head       10\n",
      "zzzzzz                      10\n",
      "0 0 1                       10\n",
      "0 0 0 a                     10\n",
      "zzz bdsm challenge          10\n",
      "book group read             10\n",
      "book group books            10\n",
      "book group                  10\n",
      "book gifts                  10\n",
      "zz wc 40                    10\n",
      "zz wc 140                   10\n",
      "zz wc 110                   10\n",
      "zz wc 100                   10\n",
      "zz to read scribd mmm       10\n",
      "zz to read scribd mmf       10\n",
      "book covers red             10\n",
      "book covers i love          10\n",
      "book covers colorful        10\n",
      "book covers color red       10\n",
      "book covers color purple    10\n",
      "book covers color grey      10\n",
      "zz series i am reading      10\n",
      "zz scribd                   10\n",
      "zz romance                  10\n",
      "zz reviewed                 10\n",
      "\n",
      "Degree quantiles (post-filter):\n",
      "p50=5.0  p90=9.0  p99=10.0  max=10\n",
      "\n",
      "Short-token rows (post-filter): 4,114 (0.34%)\n",
      "\n",
      "Borderline examples [0.50‚Äì0.52) (up to 20):\n",
      "                            token_a                            token_b  cosine_sim  rank\n",
      "                       0 sk logsdon                          logarithm    0.507021     7\n",
      "                0 spurs and saddles             hats boots spurs stars    0.503664     9\n",
      "              1 gatheringforcharity                           entirity    0.510059     2\n",
      "                       1 jelly gila                           1 nutmeg    0.511322     5\n",
      "                           1 nutmeg                       1 jelly gila    0.511322     6\n",
      "                            1 stern               ein milliarde sterne    0.513204     7\n",
      "                          1060 1070     10720 bestgayromancewithacrimi    0.514810     3\n",
      "     10720 bestgayromancewithacrimi                          1060 1070    0.514810     9\n",
      "            1bw cool unique premise                            1bw seg    0.501918     9\n",
      "                            1bw seg        1bw taken against your will    0.508534     7\n",
      "                            1bw seg            1bw cool unique premise    0.501918    10\n",
      "        1bw taken against your will                            1bw seg    0.508534     9\n",
      "                       2 quadrilogy                   quads of galafax    0.517429     8\n",
      "                       2 quadrilogy              the quads of galafrax    0.501876    10\n",
      " 2016 nbrc cyoa astronomy spell out  2016 nbrc cyoa circus carnival sp    0.513442     8\n",
      "  2016 nbrc cyoa circus carnival sp 2016 nbrc cyoa astronomy spell out    0.513442    10\n",
      "                     2march madness                     2much bullshit    0.516852    10\n",
      "                     2much bullshit                     2march madness    0.516852     4\n",
      "                         2steampunk                         gsteampunk    0.517285     4\n",
      "38833 damsels in shining armor dude     72007 badasswomenwhogetrevenge    0.504828     2\n",
      "==========================================================================================\n",
      "üîé DROP BREAKDOWN\n",
      "==========================================================================================\n",
      "Before=12,502,616  After=1,198,346  Dropped=11,304,270 (90.42%)\n",
      "By reason (approx, non-exclusive across stages except base filter):\n",
      "- below_base_threshold: 645,846\n",
      "- failed_symmetry: 10,397,500\n",
      "- short_below_rapidfuzz: 5,622\n",
      "- short_below_short_min_sim: 255,302\n",
      "==========================================================================================\n",
      "üîé SAVE FILTERED OUTPUTS\n",
      "==========================================================================================\n",
      "‚úÖ Saved filtered pairs ‚Üí romance-novel-nlp-research/src/eda_analysis/outputs/candidate_similarity_pairs.filtered.parquet\n",
      "‚úÖ Saved filtered sample (5000) ‚Üí romance-novel-nlp-research/src/eda_analysis/outputs/similarity_sample_inspection.filtered.parquet\n",
      "‚úÖ Saved filter metadata ‚Üí romance-novel-nlp-research/src/eda_analysis/outputs/cell3_filters_metadata.json\n",
      "==========================================================================================\n",
      "üîé NEXT STEP SUGGESTION\n",
      "==========================================================================================\n",
      "- If hubs remain spiky (p99>>p50), consider lowering DEGREE_CAP or raising SYMMETRY_R stringency.\n",
      "- If borderline noise persists, move BASE_THRESHOLD to 0.55 and re-run this cell.\n",
      "- Then cluster filtered graph (union-find) to derive canonical groups.\n"
     ]
    }
   ],
   "source": [
    "# romance-novel-nlp-research/src/eda_analysis/cell3b_filter_and_audit.py\n",
    "\"\"\"\n",
    "Why: Decide next feature-engineering steps with concrete evidence.\n",
    "This streams the full pairs parquet, applies proposed filters, prints exhaustive stats, and writes filtered outputs.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, sys, json, time, heapq\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- Config (tweak here) ----------\n",
    "BASE_THRESHOLD: float = 0.50      # raise from 0.45 ‚Üí trims ~646k rows\n",
    "SHORT_LEN_MAX: int = 3\n",
    "SHORT_MIN_SIM: float = 0.80\n",
    "SHORT_MIN_RF: float = 0.60        # requires rapidfuzz; otherwise skip this gate\n",
    "SYMMETRY_R: int = 10              # keep only if A->B rank<=R AND B->A rank<=R\n",
    "DEGREE_CAP: int = 25              # top-K per token_a after filters\n",
    "\n",
    "SAMPLE_SAVE_MAX: int = 5000\n",
    "PRINT_TOP_HUBS: int = 30\n",
    "HIST_BIN_START: float = 0.45\n",
    "HIST_BIN_STEP: float = 0.05\n",
    "HIST_BIN_END: float = 1.00\n",
    "\n",
    "OUT_DIR = Path(\"romance-novel-nlp-research/src/eda_analysis/outputs\")\n",
    "IN_PAIRS = OUT_DIR / \"candidate_similarity_pairs.parquet\"\n",
    "IN_META  = OUT_DIR / \"cell3_similarity_metadata.json\"\n",
    "\n",
    "OUT_PAIRS_FILTERED = OUT_DIR / \"candidate_similarity_pairs.filtered.parquet\"\n",
    "OUT_SAMPLE_FILTERED = OUT_DIR / \"similarity_sample_inspection.filtered.parquet\"\n",
    "OUT_FILTER_META = OUT_DIR / \"cell3_filters_metadata.json\"\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def p(x: str) -> None:\n",
    "    print(x); sys.stdout.flush()\n",
    "\n",
    "def head(title: str) -> None:\n",
    "    sep = \"=\" * 90\n",
    "    p(sep); p(f\"üîé {title}\"); p(sep)\n",
    "\n",
    "def human(n: int) -> str:\n",
    "    units = [\"B\",\"KB\",\"MB\",\"GB\",\"TB\"]; i=0; x=float(n)\n",
    "    while x>=1024 and i<len(units)-1: x/=1024; i+=1\n",
    "    return f\"{x:.2f} {units[i]}\"\n",
    "\n",
    "def file_info(path: Path) -> str:\n",
    "    return f\"{path} | exists={path.exists()} | size={human(path.stat().st_size) if path.exists() else '‚Äî'}\"\n",
    "\n",
    "# ---------- Imports (optional) ----------\n",
    "try:\n",
    "    import pyarrow.dataset as ds\n",
    "    import pyarrow as pa\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Requires pyarrow. Install: pip install pyarrow\") from e\n",
    "\n",
    "try:\n",
    "    from rapidfuzz.fuzz import ratio as fuzz_ratio\n",
    "    HAS_RAPIDFUZZ = True\n",
    "except Exception:\n",
    "    HAS_RAPIDFUZZ = False\n",
    "\n",
    "# ---------- 0) Inventory ----------\n",
    "head(\"INPUTS\")\n",
    "p(file_info(IN_PAIRS))\n",
    "p(file_info(IN_META))\n",
    "if not IN_PAIRS.exists():\n",
    "    raise FileNotFoundError(\"pairs parquet not found.\")\n",
    "meta = json.loads(IN_META.read_text()) if IN_META.exists() else {}\n",
    "p(\"\\nMeta (config.stats excerpt):\")\n",
    "p(json.dumps({\"config\": meta.get(\"config\", {}), \"stats\": meta.get(\"stats\", {})}, indent=2))\n",
    "\n",
    "# ---------- 1) Before-stats (global + histogram + short rate, streaming) ----------\n",
    "head(\"BEFORE STATS (STREAMING)\")\n",
    "dsobj = ds.dataset(IN_PAIRS.as_posix(), format=\"parquet\")\n",
    "scanner = dsobj.scanner(columns=[\"token_a\",\"token_b\",\"cosine_sim\",\"rank\"], batch_size=250_000)\n",
    "\n",
    "total_rows = 0\n",
    "short_rows = 0\n",
    "min_sim = 1.0\n",
    "max_sim = 0.0\n",
    "sum_sim = 0.0\n",
    "\n",
    "# 0.45..1.00 bins\n",
    "nbins = int(np.ceil((HIST_BIN_END - HIST_BIN_START) / HIST_BIN_STEP))\n",
    "bins = np.zeros(nbins, dtype=np.int64)\n",
    "\n",
    "t0 = time.time()\n",
    "for batch in scanner.to_batches():\n",
    "    df = batch.to_pandas()\n",
    "    n = len(df); total_rows += n\n",
    "    cs = df[\"cosine_sim\"].to_numpy(np.float32, copy=False)\n",
    "    min_sim = min(min_sim, float(cs.min()))\n",
    "    max_sim = max(max_sim, float(cs.max()))\n",
    "    sum_sim += float(cs.sum())\n",
    "    idx = ((cs - HIST_BIN_START) / HIST_BIN_STEP).astype(np.int32)\n",
    "    m = (idx >= 0) & (idx < nbins)\n",
    "    if m.any():\n",
    "        bincount = np.bincount(idx[m], minlength=nbins)\n",
    "        bins[:len(bincount)] += bincount\n",
    "    short_rows += int(((df[\"token_a\"].str.len() <= SHORT_LEN_MAX) | (df[\"token_b\"].str.len() <= SHORT_LEN_MAX)).sum())\n",
    "\n",
    "mean_sim = sum_sim / max(1, total_rows)\n",
    "p(f\"Rows={total_rows:,} | min={min_sim:.3f} mean={mean_sim:.3f} max={max_sim:.3f}\")\n",
    "p(\"Histogram (pre-filter):\")\n",
    "for i in range(nbins):\n",
    "    start = HIST_BIN_START + i*HIST_BIN_STEP\n",
    "    end = start + HIST_BIN_STEP\n",
    "    p(f\"{start:.2f}‚Äì{end:.2f}: {int(bins[i]):,}\")\n",
    "p(f\"Short-token rows (<= {SHORT_LEN_MAX} chars): {short_rows:,} ({short_rows/total_rows*100:.2f}%)\")\n",
    "p(f\"Scan time: {time.time()-t0:.2f}s\")\n",
    "\n",
    "# ---------- 2) Pass-0: reverse_rank for symmetry (only ranks ‚â§ R) ----------\n",
    "head(f\"BUILD REVERSE RANK MAP (rank‚â§{SYMMETRY_R})\")\n",
    "rev_map: Dict[Tuple[str,str], int] = {}\n",
    "t1 = time.time()\n",
    "scanner_r = dsobj.scanner(columns=[\"token_a\",\"token_b\",\"rank\"], batch_size=250_000)\n",
    "kept_rev = 0\n",
    "for batch in scanner_r.to_batches():\n",
    "    df = batch.to_pandas()\n",
    "    small = df[df[\"rank\"] <= SYMMETRY_R]\n",
    "    if small.empty:\n",
    "        continue\n",
    "    for a, b, r in zip(small[\"token_a\"], small[\"token_b\"], small[\"rank\"]):\n",
    "        rev_map[(a, b)] = int(r)\n",
    "    kept_rev += len(small)\n",
    "p(f\"Reverse entries stored: {kept_rev:,} | time: {time.time()-t1:.2f}s\")\n",
    "\n",
    "# ---------- 3) Pass-1: apply filters + per-token top-K ----------\n",
    "head(\"APPLY FILTERS (streaming) + PER-TOKEN TOP-K\")\n",
    "drop_stats = defaultdict(int)\n",
    "heaps: Dict[str, List[Tuple[float,int,str,float]]] = defaultdict(list)  # token_a -> min-heap of (sim, rank, token_b, sim) ; sim duplicated for clarity\n",
    "\n",
    "def maybe_push(a: str, b: str, sim: float, rank: int) -> None:\n",
    "    h = heaps[a]\n",
    "    item = (sim, rank, b, sim)\n",
    "    if len(h) < DEGREE_CAP:\n",
    "        heapq.heappush(h, item)\n",
    "    else:\n",
    "        if sim > h[0][0]:  # replace worst\n",
    "            heapq.heapreplace(h, item)\n",
    "\n",
    "def short_pair_gate(a: str, b: str, sim: float) -> bool:\n",
    "    la = len(a); lb = len(b)\n",
    "    if min(la, lb) > SHORT_LEN_MAX:\n",
    "        return True\n",
    "    if sim < SHORT_MIN_SIM:\n",
    "        return False\n",
    "    if HAS_RAPIDFUZZ:\n",
    "        return (fuzz_ratio(a, b) / 100.0) >= SHORT_MIN_RF\n",
    "    return True  # no RapidFuzz ‚Üí skip ratio gate\n",
    "\n",
    "def sym_gate(a: str, b: str, rank_ab: int) -> bool:\n",
    "    if rank_ab > SYMMETRY_R:\n",
    "        return False\n",
    "    rb = rev_map.get((b, a), None)\n",
    "    return (rb is not None) and (rb <= SYMMETRY_R)\n",
    "\n",
    "t2 = time.time()\n",
    "scanner_f = dsobj.scanner(columns=[\"token_a\",\"token_b\",\"cosine_sim\",\"rank\"], batch_size=200_000)\n",
    "rows_seen = 0\n",
    "for batch in scanner_f.to_batches():\n",
    "    df = batch.to_pandas()\n",
    "    rows_seen += len(df)\n",
    "    for a, b, sim, r in zip(df[\"token_a\"], df[\"token_b\"], df[\"cosine_sim\"], df[\"rank\"]):\n",
    "        # base threshold\n",
    "        if sim < BASE_THRESHOLD:\n",
    "            drop_stats[\"below_base_threshold\"] += 1\n",
    "            continue\n",
    "        # short gate\n",
    "        if not short_pair_gate(a, b, float(sim)):\n",
    "            if min(len(a), len(b)) <= SHORT_LEN_MAX and sim < SHORT_MIN_SIM:\n",
    "                drop_stats[\"short_below_short_min_sim\"] += 1\n",
    "            else:\n",
    "                drop_stats[\"short_below_rapidfuzz\"] += 1\n",
    "            continue\n",
    "        # symmetry\n",
    "        if not sym_gate(a, b, int(r)):\n",
    "            drop_stats[\"failed_symmetry\"] += 1\n",
    "            continue\n",
    "        maybe_push(a, b, float(sim), int(r))\n",
    "p(f\"Streamed rows: {rows_seen:,} | time: {time.time()-t2:.2f}s\")\n",
    "\n",
    "# ---------- 4) Materialize filtered edges ----------\n",
    "head(\"MATERIALIZE FILTERED EDGES\")\n",
    "rows = []\n",
    "for a, h in heaps.items():\n",
    "    for sim, r, b, sim_copy in sorted(h, key=lambda x: (-x[0], x[1])):  # highest sim first, then rank\n",
    "        rows.append((a, b, float(sim), int(r)))\n",
    "filtered_df = pd.DataFrame(rows, columns=[\"token_a\",\"token_b\",\"cosine_sim\",\"rank\"])\n",
    "filtered_df.sort_values([\"token_a\",\"rank\",\"cosine_sim\"], ascending=[True, True, False], inplace=True)\n",
    "p(f\"Filtered pairs: {len(filtered_df):,}  | tokens covered: {filtered_df['token_a'].nunique():,}\")\n",
    "\n",
    "# ---------- 5) After-stats ----------\n",
    "head(\"AFTER STATS\")\n",
    "def hist_of(series: pd.Series, start: float, end: float, step: float) -> List[Tuple[float,float,int]]:\n",
    "    nb = int(np.ceil((end-start)/step)); out=[]\n",
    "    arr = series.to_numpy(np.float32, copy=False)\n",
    "    idx = ((arr - start) / step).astype(np.int32)\n",
    "    counts = np.zeros(nb, dtype=np.int64)\n",
    "    m = (idx >= 0) & (idx < nb)\n",
    "    if m.any():\n",
    "        bincount = np.bincount(idx[m], minlength=nb); counts[:len(bincount)] += bincount\n",
    "    for i in range(nb):\n",
    "        s = start + i*step; e = s + step\n",
    "        out.append((s, e, int(counts[i])))\n",
    "    return out\n",
    "\n",
    "p(f\"Rows={len(filtered_df):,} | min={filtered_df['cosine_sim'].min():.3f} mean={filtered_df['cosine_sim'].mean():.3f} max={filtered_df['cosine_sim'].max():.3f}\")\n",
    "p(\"Histogram (post-filter):\")\n",
    "for s,e,c in hist_of(filtered_df[\"cosine_sim\"], HIST_BIN_START, HIST_BIN_END, HIST_BIN_STEP):\n",
    "    p(f\"{s:.2f}‚Äì{e:.2f}: {c:,}\")\n",
    "\n",
    "# hubs\n",
    "deg = filtered_df.groupby(\"token_a\", observed=True).size().sort_values(ascending=False)\n",
    "p(\"\\nTop hubs (post-filter):\")\n",
    "p(deg.head(PRINT_TOP_HUBS).to_string())\n",
    "p(\"\\nDegree quantiles (post-filter):\")\n",
    "q = deg.quantile([0.5,0.9,0.99]).to_numpy()\n",
    "p(f\"p50={q[0]:.1f}  p90={q[1]:.1f}  p99={q[2]:.1f}  max={deg.max()}\")\n",
    "\n",
    "# short-token share\n",
    "short_mask = (filtered_df[\"token_a\"].str.len() <= SHORT_LEN_MAX) | (filtered_df[\"token_b\"].str.len() <= SHORT_LEN_MAX)\n",
    "p(f\"\\nShort-token rows (post-filter): {int(short_mask.sum()):,} ({short_mask.mean()*100:.2f}%)\")\n",
    "\n",
    "# borderline examples\n",
    "border = filtered_df[(filtered_df[\"cosine_sim\"] >= BASE_THRESHOLD) & (filtered_df[\"cosine_sim\"] < BASE_THRESHOLD + 0.02)]\n",
    "p(f\"\\nBorderline examples [{BASE_THRESHOLD:.2f}‚Äì{BASE_THRESHOLD+0.02:.2f}) (up to 20):\")\n",
    "p(border.head(20).to_string(index=False))\n",
    "\n",
    "# ---------- 6) Drop breakdown ----------\n",
    "head(\"DROP BREAKDOWN\")\n",
    "total_before = total_rows\n",
    "kept_after = len(filtered_df)\n",
    "dropped_total = total_before - kept_after\n",
    "p(f\"Before={total_before:,}  After={kept_after:,}  Dropped={dropped_total:,} ({dropped_total/max(1,total_before)*100:.2f}%)\")\n",
    "p(\"By reason (approx, non-exclusive across stages except base filter):\")\n",
    "for k in sorted(drop_stats.keys()):\n",
    "    p(f\"- {k}: {drop_stats[k]:,}\")\n",
    "\n",
    "# ---------- 7) Save outputs ----------\n",
    "head(\"SAVE FILTERED OUTPUTS\")\n",
    "filtered_df.to_parquet(OUT_PAIRS_FILTERED, index=False)\n",
    "samp_n = min(SAMPLE_SAVE_MAX, len(filtered_df))\n",
    "if samp_n > 0:\n",
    "    filtered_df.sample(n=samp_n, random_state=42).to_parquet(OUT_SAMPLE_FILTERED, index=False)\n",
    "\n",
    "filters_meta = {\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"input_pairs_file\": str(IN_PAIRS),\n",
    "    \"filters\": {\n",
    "        \"base_threshold\": BASE_THRESHOLD,\n",
    "        \"short_len_max\": SHORT_LEN_MAX,\n",
    "        \"short_min_sim\": SHORT_MIN_SIM,\n",
    "        \"short_min_rapidfuzz\": SHORT_MIN_RF if HAS_RAPIDFUZZ else None,\n",
    "        \"symmetry_rank_max\": SYMMETRY_R,\n",
    "        \"degree_cap\": DEGREE_CAP,\n",
    "        \"rapidfuzz_available\": HAS_RAPIDFUZZ\n",
    "    },\n",
    "    \"before\": {\n",
    "        \"rows\": int(total_rows),\n",
    "        \"hist_bins\": {f\"{HIST_BIN_START+i*HIST_BIN_STEP:.2f}-{HIST_BIN_START+(i+1)*HIST_BIN_STEP:.2f}\": int(bins[i]) for i in range(nbins)},\n",
    "        \"short_rows\": int(short_rows)\n",
    "    },\n",
    "    \"after\": {\n",
    "        \"rows\": int(kept_after),\n",
    "        \"short_rows\": int(short_mask.sum()),\n",
    "        \"degree_quantiles\": {\"p50\": float(q[0]), \"p90\": float(q[1]), \"p99\": float(q[2]), \"max\": int(deg.max())}\n",
    "    },\n",
    "}\n",
    "with open(OUT_FILTER_META, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filters_meta, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "p(f\"‚úÖ Saved filtered pairs ‚Üí {OUT_PAIRS_FILTERED}\")\n",
    "if samp_n > 0:\n",
    "    p(f\"‚úÖ Saved filtered sample ({samp_n}) ‚Üí {OUT_SAMPLE_FILTERED}\")\n",
    "p(f\"‚úÖ Saved filter metadata ‚Üí {OUT_FILTER_META}\")\n",
    "\n",
    "# ---------- 8) Clear next-step guidance ----------\n",
    "head(\"NEXT STEP SUGGESTION\")\n",
    "p(\"- If hubs remain spiky (p99>>p50), consider lowering DEGREE_CAP or raising SYMMETRY_R stringency.\")\n",
    "p(\"- If borderline noise persists, move BASE_THRESHOLD to 0.55 and re-run this cell.\")\n",
    "p(\"- Then cluster filtered graph (union-find) to derive canonical groups.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "üîé INPUTS\n",
      "====================================================================================================\n",
      "romance-novel-nlp-research/src/eda_analysis/outputs/candidate_similarity_pairs.filtered.parquet | exists=True | size=19.02 MB\n",
      "romance-novel-nlp-research/src/eda_analysis/outputs/cell3_similarity_metadata.json | exists=True\n",
      "romance-novel-nlp-research/src/eda_analysis/outputs/cell3_filters_metadata.json | exists=True\n",
      "\n",
      "Meta excerpt:\n",
      "{\n",
      "  \"stats\": {\n",
      "    \"total_tokens\": 254778,\n",
      "    \"embedding_dim\": 384,\n",
      "    \"candidate_pairs_found\": 12502616\n",
      "  },\n",
      "  \"config\": {\n",
      "    \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
      "    \"batch_size\": 1024,\n",
      "    \"normalize\": true,\n",
      "    \"top_k_neighbors\": 50,\n",
      "    \"similarity_threshold_initial\": 0.3,\n",
      "    \"similarity_threshold_used\": 0.44999992847442627,\n",
      "    \"hnsw\": {\n",
      "      \"M\": 16,\n",
      "      \"ef_construction\": 200,\n",
      "      \"ef\": 200\n",
      "    },\n",
      "    \"query_batch_size\": 25000,\n",
      "    \"tuning\": {\n",
      "      \"enabled\": true,\n",
      "      \"csv\": null,\n",
      "      \"min_precision\": 0.9,\n",
      "      \"min_recall\": null,\n",
      "      \"grid\": [\n",
      "        0.05,\n",
      "        0.95,\n",
      "        0.01\n",
      "      ],\n",
      "      \"metrics_csv\": \"romance-novel-nlp-research/src/eda_analysis/outputs/similarity_threshold_metrics.csv\",\n",
      "      \"summary_json\": \"romance-novel-nlp-research/src/eda_analysis/outputs/similarity_threshold_summary.json\",\n",
      "      \"autogen_used\": true\n",
      "    },\n",
      "    \"extras\": {\n",
      "      \"add_string_metrics\": false,\n",
      "      \"use_rapidfuzz\": true,\n",
      "      \"metrics_sample_n\": null\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "Filter meta excerpt:\n",
      "{\n",
      "  \"base_threshold\": 0.5,\n",
      "  \"short_len_max\": 3,\n",
      "  \"short_min_sim\": 0.8,\n",
      "  \"short_min_rapidfuzz\": 0.6,\n",
      "  \"symmetry_rank_max\": 10,\n",
      "  \"degree_cap\": 25,\n",
      "  \"rapidfuzz_available\": true\n",
      "}\n",
      "====================================================================================================\n",
      "üîé BUILD GRAPH (UNION-FIND) + DEGREE FEATURES (STREAMING)\n",
      "====================================================================================================\n",
      "Nodes=232,918 | Edges=1,198,346 | time=14.52s\n",
      "Token coverage vs meta.total_tokens: 91.42%\n",
      "====================================================================================================\n",
      "üîé COMPUTE CONNECTED COMPONENTS\n",
      "====================================================================================================\n",
      "Components=4,223 | mean size=55.15 | median=2 | max=218,379\n",
      "Size buckets:\n",
      "  1-1  : 0\n",
      "  2-2  : 2,633\n",
      "  3-5  : 1,239\n",
      "  6-10 : 200\n",
      " 11-20 : 105\n",
      " 21-50 : 36\n",
      " 51-100: 7\n",
      "101-99999999: 3\n",
      "time=0.65s\n",
      "====================================================================================================\n",
      "üîé BUILD CLUSTER TABLES\n",
      "====================================================================================================\n",
      "map_df: 232,918 rows | clusters: 4,223 | time=4.14s\n",
      "====================================================================================================\n",
      "üîé CLUSTER DIAGNOSTICS (PRINTS)\n",
      "====================================================================================================\n",
      "\n",
      "Top 20 clusters (size, medoid, deg, flags):\n",
      " cluster_id   size                         medoid  medoid_deg  medoid_wdeg  short_rate  digit_rate  zz_rate\n",
      "          0 218379    manga graphic novels comics          20    19.753629     0.00463    0.134148 0.002757\n",
      "          1    328               150 to 200 pages          20    19.306986     0.00000    0.993902 0.000000\n",
      "          2    140      meet n greet 2015 dec jan          20    18.484929     0.00000    0.685714 0.014286\n",
      "          3     96           erotica bdsm romance          20    19.272889     0.00000    0.031250 0.000000\n",
      "          4     77              01 june utc bonus          20    19.072018     0.00000    0.623377 0.000000\n",
      "          6     60 2015 read a thon challenge jan          20    19.012063     0.00000    0.866667 0.016667\n",
      "          5     60                      1 on deck          20    16.895066     0.00000    0.416667 0.000000\n",
      "          7     59           00 feb ult challenge          20    19.234684     0.00000    0.983051 0.000000\n",
      "          8     53 pride and prejudice variations          20    18.590283     0.00000    0.000000 0.000000\n",
      "          9     51                  wattpad books          20    18.838457     0.00000    0.058824 0.000000\n",
      "         10     46    debut author challenge 2013          20    19.180490     0.00000    0.826087 0.000000\n",
      "         12     45          fairy tale retellings          20    19.345152     0.00000    0.000000 0.000000\n",
      "         11     45                 sfrc fall 2016          20    18.244408     0.00000    0.644444 0.000000\n",
      "         13     44                      abandoned          20    17.771411     0.00000    0.000000 0.000000\n",
      "         14     40                     4018 03 01          20    19.216676     0.00000    1.000000 0.000000\n",
      "         16     38                         zboxb1          20    18.235952     0.00000    1.000000 0.000000\n",
      "         15     38                        samhain          20    17.560153     0.00000    0.052632 0.000000\n",
      "         17     36                 mills and boon          20    18.709940     0.00000    0.000000 0.000000\n",
      "         18     35                     300 400pgs          20    18.829105     0.00000    1.000000 0.000000\n",
      "         19     34            2014 rita finalists          20    19.010934     0.00000    0.558824 0.000000\n",
      "\n",
      "Top flagged clusters (potential garbage):\n",
      " cluster_id  size                         medoid  medoid_deg  medoid_wdeg  mean_deg  mean_wdeg  short_rate  digit_rate  zz_rate\n",
      "          1   328               150 to 200 pages          20    19.306986 12.146341  11.178399         0.0    0.993902 0.000000\n",
      "          2   140      meet n greet 2015 dec jan          20    18.484929 14.428571  12.805877         0.0    0.685714 0.014286\n",
      "          4    77              01 june utc bonus          20    19.072018 11.480519  10.598638         0.0    0.623377 0.000000\n",
      "          6    60 2015 read a thon challenge jan          20    19.012063 14.200000  12.836556         0.0    0.866667 0.016667\n",
      "          7    59           00 feb ult challenge          20    19.234684 11.389831  10.681322         0.0    0.983051 0.000000\n",
      "         10    46    debut author challenge 2013          20    19.180490 14.695652  13.777565         0.0    0.826087 0.000000\n",
      "         11    45                 sfrc fall 2016          20    18.244408 12.800000  10.828341         0.0    0.644444 0.000000\n",
      "         14    40                     4018 03 01          20    19.216676 16.000000  15.092120         0.0    1.000000 0.000000\n",
      "         16    38                         zboxb1          20    18.235952 15.578947  13.902230         0.0    1.000000 0.000000\n",
      "         18    35                     300 400pgs          20    18.829105 14.742857  13.438922         0.0    1.000000 0.000000\n",
      "         19    34            2014 rita finalists          20    19.010934 14.470588  13.293220         0.0    0.558824 0.000000\n",
      "         20    33               ed pub month aug          20    18.769808 11.878788  10.782680         0.0    0.666667 0.000000\n",
      "         23    29         2016 botb bonus 01 jan          20    19.046519 15.034483  13.975225         0.0    0.862069 0.000000\n",
      "         25    28         whitney finalists 2014          20    19.080361 14.571429  13.741109         0.0    0.785714 0.000000\n",
      "         27    27                       bea 2013          20    17.780761 14.370370  12.456968         0.0    1.000000 0.000000\n",
      "         31    26           manga challenge 2016          20    19.226413 13.692308  12.976913         0.0    0.961538 0.000000\n",
      "         30    26          07 2017 spellapalooza          20    18.473184 13.230769  11.541078         0.0    0.653846 0.000000\n",
      "         36    23                       locmed17          20    18.228083 14.260870  12.763625         0.0    1.000000 0.000000\n",
      "         45    21  meet greet challenge round 19          20    19.426205 13.333333  12.713176         0.0    0.666667 0.000000\n",
      "         43    21                     01 h a r 4          20    17.670881 14.095238  12.079558         0.0    1.000000 0.000000\n",
      "\n",
      "Cluster size histogram:\n",
      "1-1: 0\n",
      "2-2: 2,633\n",
      "3-4: 1,083\n",
      "5-9: 342\n",
      "10-19: 116\n",
      "20-49: 39\n",
      "50-99: 7\n",
      "100-199: 1\n",
      "200-499: 1\n",
      "500-999: 0\n",
      "1000-999998: 1\n",
      "\n",
      "Examples from a few largest clusters:\n",
      "\n",
      "[Cluster 0] size=218,379  medoid='manga graphic novels comics'\n",
      "                          token  degree   wdegree  is_short  has_digit  starts_zz\n",
      "    manga graphic novels comics      20 19.753629     False      False      False\n",
      "    graphic novels manga comics      20 19.729990     False      False      False\n",
      "    comics manga graphic novels      20 19.718242     False      False      False\n",
      "    comics graphic novels manga      20 19.699776     False      False      False\n",
      "    manga comics graphic novels      20 19.690819     False      False      False\n",
      "         novellas short stories      20 19.678731     False      False      False\n",
      "mystery crime thriller suspense      20 19.674843     False      False      False\n",
      "     manga comic graphic novels      20 19.671915     False      False      False\n",
      "crime thriller mystery suspense      20 19.663854     False      False      False\n",
      "      sci fi paranormal fantasy      20 19.662551     False      False      False\n",
      "         short stories novellas      20 19.662418     False      False      False\n",
      "crime mystery thriller suspense      20 19.658270     False      False      False\n",
      "      paranormal sci fi fantasy      20 19.658045     False      False      False\n",
      "mystery suspense crime thriller      20 19.654997     False      False      False\n",
      "         short novellas stories      20 19.646349     False      False      False\n",
      "\n",
      "[Cluster 1] size=328  medoid='150 to 200 pages'\n",
      "              token  degree   wdegree  is_short  has_digit  starts_zz\n",
      "   150 to 200 pages      20 19.306986     False       True      False\n",
      "   250 to 300 pages      20 19.306888     False       True      False\n",
      "   200 to 300 pages      20 19.271230     False       True      False\n",
      "   300 to 400 pages      20 19.249509     False       True      False\n",
      "   200 to 400 pages      20 19.242629     False       True      False\n",
      "   200 to 250 pages      20 19.240225     False       True      False\n",
      "less than 250 pages      20 19.199620     False       True      False\n",
      "      300 400 pages      20 19.138381     False       True      False\n",
      "less than 150 pages      20 19.115009     False       True      False\n",
      "less than 300 pages      20 19.104457     False       True      False\n",
      "less than 400 pages      20 19.099887     False       True      False\n",
      "      200 300 pages      20 19.070574     False       True      False\n",
      "   250 to 600 pages      20 19.031709     False       True      False\n",
      "  300 pages or more      20 19.020064     False       True      False\n",
      "less than 160 pages      20 19.016197     False       True      False\n",
      "\n",
      "[Cluster 2] size=140  medoid='meet n greet 2015 dec jan'\n",
      "                         token  degree   wdegree  is_short  has_digit  starts_zz\n",
      "     meet n greet 2015 dec jan      20 18.484929     False       True      False\n",
      "    meet n greet 2015 aug sept      20 18.360795     False       True      False\n",
      "             meet n greet pick      20 18.355090     False      False      False\n",
      "            meet greet 21 read      20 18.348849     False       True      False\n",
      "            meet n greet picks      20 18.302583     False      False      False\n",
      "meet n greet dec 2016 jan 2017      20 18.243182     False       True      False\n",
      "     2015 meet n greet dec jan      20 18.237914     False       True      False\n",
      "     meet and greet 21 to read      20 18.221347     False       True      False\n",
      "              meet greet picks      20 18.206493     False      False      False\n",
      "           meet and greet pick      20 18.195864     False      False      False\n",
      "            meet greet 19 read      20 18.188090     False       True      False\n",
      "             meet n greet 2015      20 18.167164     False       True      False\n",
      "         meet n greet round 19      20 18.137235     False       True      False\n",
      "     meet and greet 23 to read      20 18.128178     False       True      False\n",
      "            meet greet to pick      20 18.095919     False      False      False\n",
      "\n",
      "[Cluster 3] size=96  medoid='erotica bdsm romance'\n",
      "                            token  degree   wdegree  is_short  has_digit  starts_zz\n",
      "             erotica bdsm romance      20 19.272889     False      False      False\n",
      "             bdsm erotica romance      20 19.241008     False      False      False\n",
      "                        bdsm kink      20 19.234374     False      False      False\n",
      "                        kink bdsm      20 19.226376     False      False      False\n",
      "              erotic romance bdsm      20 19.205822     False      False      False\n",
      "                    bdsm and kink      20 19.182086     False      False      False\n",
      "                     bdsm erotica      20 19.178757     False      False      False\n",
      "                     erotica bdsm      20 19.156519     False      False      False\n",
      "                    kink and bdsm      20 19.156063     False      False      False\n",
      "              romance erotic bdsm      20 19.151192     False      False      False\n",
      "                 sex erotica bdsm      20 19.048186     False      False      False\n",
      "                   kinks and bdsm      20 18.983869     False      False      False\n",
      "         bdsm erotica and romance      20 18.967978     False      False      False\n",
      "                       bdsm kinks      20 18.946510     False      False      False\n",
      "contemporary romance erotica bdsm      20 18.926745     False      False      False\n",
      "\n",
      "[Cluster 4] size=77  medoid='01 june utc bonus'\n",
      "             token  degree   wdegree  is_short  has_digit  starts_zz\n",
      " 01 june utc bonus      20 19.072018     False       True      False\n",
      " 01 july utc bonus      20 19.054014     False       True      False\n",
      " 01 utc june bonus      20 18.954345     False       True      False\n",
      " 07 july utc bonus      20 18.885596     False       True      False\n",
      " 01 sept utc bonus      20 18.844991     False       True      False\n",
      "  01 may utc bonus      20 18.788271     False       True      False\n",
      " 01 utc july bonus      18 17.051130     False       True      False\n",
      " 06 june utc bonus      18 16.929294     False       True      False\n",
      "    july utc bonus      18 16.918317     False      False      False\n",
      "  08 utc sep bonus      18 16.836265     False       True      False\n",
      "    utc june bonus      18 16.802889     False      False      False\n",
      "  04 utc may bonus      18 16.763342     False       True      False\n",
      "     utc may bonus      18 16.625339     False      False      False\n",
      "  utc august bonus      18 16.518639     False      False      False\n",
      "01 utc april bonus      16 15.040193     False       True      False\n",
      "====================================================================================================\n",
      "üîé EDGE SAMPLES FROM TOP CLUSTERS\n",
      "====================================================================================================\n",
      " cluster_id           token_a           token_b  cosine_sim  rank\n",
      "          0               0 0             0 0 0    0.975303     1\n",
      "          0               0 0             0 0 1    0.906423     3\n",
      "          0               0 0           0 0 0 1    0.885735     4\n",
      "          0               0 0               0 1    0.877480     5\n",
      "          0               0 0             0 0 2    0.850747     9\n",
      "          0               0 0           0 0 0 a    0.846050    10\n",
      "          0             0 0 0               0 0    0.975303     1\n",
      "          0             0 0 0           0 0 0 1    0.919496     2\n",
      "          0             0 0 0             0 0 1    0.908444     3\n",
      "          0             0 0 0    0 0 0 0over it    0.899378     4\n",
      "          0             0 0 0           0 0 0 a    0.874364     5\n",
      "          0             0 0 0        0 0 0 next    0.857385     6\n",
      "          0             0 0 0    0 0 eventually    0.855615     7\n",
      "          0             0 0 0          0 0 asap    0.852846     8\n",
      "          0             0 0 0             0 0 2    0.837812     9\n",
      "          0      0 0 0 0check           0 check    0.793832     1\n",
      "          0      0 0 0 0check          0 checkf    0.769414     2\n",
      "          0      0 0 0 0check           01check    0.727421     5\n",
      "          0      0 0 0 0check    0 to check out    0.683171     9\n",
      "          0    0 0 0 0over it             0 0 0    0.899378     1\n",
      "          0    0 0 0 0over it           0 0 0 1    0.840687     3\n",
      "          0    0 0 0 0over it           0 0 0 a    0.815835     4\n",
      "          0    0 0 0 0over it             0 0 1    0.807630     5\n",
      "          0    0 0 0 0over it    0 0 eventually    0.777979     7\n",
      "          0    0 0 0 0over it           0 0 0 b    0.771831     8\n",
      "          0    0 0 0 0over it          0 0 asap    0.769464     9\n",
      "          0    0 0 0 0over it   0 0 0 this year    0.749866    10\n",
      "          0           0 0 0 1             0 0 1    0.983803     1\n",
      "          0           0 0 0 1             0 0 0    0.919496     2\n",
      "          0           0 0 0 1               0 1    0.900497     3\n",
      "          0           0 0 0 1               0 0    0.885735     4\n",
      "          0           0 0 0 1      0 0 0 next 1    0.852676     5\n",
      "          0           0 0 0 1        0 0 0 next    0.840824     6\n",
      "          0           0 0 0 1    0 0 0 0over it    0.840687     7\n",
      "          0           0 0 0 1           0 0 0 a    0.815703     8\n",
      "          0           0 0 0 1             0 0 2    0.814457     9\n",
      "          0           0 0 0 1           0 0 0 b    0.792719    10\n",
      "          0 0 0 0 300 in 2015   0 0 250 in 2014    0.804723     1\n",
      "          0 0 0 0 300 in 2015            0 2015    0.790982     2\n",
      "          0 0 0 0 300 in 2015 0 0 2 try in 2014    0.679918     5\n",
      "\n",
      "Borderline edges in sampled top clusters (0.50‚Äì0.55): 0\n",
      "====================================================================================================\n",
      "üîé SAVE CLUSTER ARTIFACTS\n",
      "====================================================================================================\n",
      "‚úÖ Saved token‚Üícluster map ‚Üí romance-novel-nlp-research/src/eda_analysis/outputs/clusters_token_map.parquet\n",
      "‚úÖ Saved cluster summary ‚Üí romance-novel-nlp-research/src/eda_analysis/outputs/clusters_summary.parquet\n",
      "‚úÖ Saved edge samples ‚Üí romance-novel-nlp-research/src/eda_analysis/outputs/clusters_edges_samples.parquet\n",
      "====================================================================================================\n",
      "üîé NEXT-STEP SUGGESTIONS (ACTIONABLE)\n",
      "====================================================================================================\n",
      "- If flagged clusters dominate (short/digits/zz), add a **cluster-level gate**: drop clusters with flag_rate>0.7 or size<3 and high flag rates.\n",
      "- Promote medoid tokens as **canonical labels**; export token‚Üícanonical map (medoid).\n",
      "- If many borderline edges within large clusters, raise BASE_THRESHOLD to 0.55 and re-run filter.\n",
      "- If hubiness resurfaces in clusters, consider **per-length thresholds** (len‚â§4 ‚Üí sim‚â•0.85) and **token stoplist** (e.g., exact 'zz*').\n",
      "- Proceed to **merge clusters with high medoid similarity** (medoid‚Äìmedoid sim‚â•0.85) to collapse near-dup clusters.\n"
     ]
    }
   ],
   "source": [
    "# romance-novel-nlp-research/src/eda_analysis/cell3c_cluster_and_inspect.py\n",
    "\n",
    "from __future__ import annotations\n",
    "import sys, os, time, json, math, re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Paths ---\n",
    "OUT = Path(\"romance-novel-nlp-research/src/eda_analysis/outputs\")\n",
    "PAIRS = OUT / \"candidate_similarity_pairs.filtered.parquet\"\n",
    "META_JSON = OUT / \"cell3_similarity_metadata.json\"            # from Cell 3\n",
    "FILTER_META_JSON = OUT / \"cell3_filters_metadata.json\"        # from Cell 3b\n",
    "CLUSTERS_MAP = OUT / \"clusters_token_map.parquet\"\n",
    "CLUSTERS_SUM = OUT / \"clusters_summary.parquet\"\n",
    "CLUSTERS_EDGES_SAMPLE = OUT / \"clusters_edges_samples.parquet\"\n",
    "\n",
    "# --- Printing helpers ---\n",
    "def p(x: str) -> None:\n",
    "    print(x); sys.stdout.flush()\n",
    "\n",
    "def head(title: str) -> None:\n",
    "    sep = \"=\" * 100\n",
    "    p(sep); p(f\"üîé {title}\"); p(sep)\n",
    "\n",
    "def human(n: int) -> str:\n",
    "    units = [\"B\",\"KB\",\"MB\",\"GB\",\"TB\"]; i=0; x=float(n)\n",
    "    while x>=1024 and i<len(units)-1: x/=1024; i+=1\n",
    "    return f\"{x:.2f} {units[i]}\"\n",
    "\n",
    "# --- Require PyArrow Dataset for streaming ---\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.dataset as ds\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Requires pyarrow. Install: pip install pyarrow\") from e\n",
    "\n",
    "# --- Tiny utils ---\n",
    "DIGIT_RE = re.compile(r\"\\d\")\n",
    "def flag_short(s: str) -> bool: return len(s) <= 3\n",
    "def flag_digit(s: str) -> bool: return bool(DIGIT_RE.search(s))\n",
    "def flag_zz(s: str) -> bool: return s.strip().lower().startswith(\"zz\")\n",
    "\n",
    "# --- Disjoint Set (Union-Find) with dynamic add (why: 230k+ nodes) ---\n",
    "class DSU:\n",
    "    def __init__(self):\n",
    "        self.parent: Dict[int,int] = {}\n",
    "        self.size: Dict[int,int] = {}\n",
    "\n",
    "    def _add(self, x: int):\n",
    "        if x not in self.parent:\n",
    "            self.parent[x] = x\n",
    "            self.size[x] = 1\n",
    "\n",
    "    def find(self, x: int) -> int:\n",
    "        self._add(x)\n",
    "        # Path compression\n",
    "        while self.parent[x] != x:\n",
    "            self.parent[x] = self.parent[self.parent[x]]\n",
    "            x = self.parent[x]\n",
    "        return x\n",
    "\n",
    "    def union(self, a: int, b: int):\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra == rb: return\n",
    "        if self.size[ra] < self.size[rb]:\n",
    "            ra, rb = rb, ra\n",
    "        self.parent[rb] = ra\n",
    "        self.size[ra] += self.size[rb]\n",
    "\n",
    "# --- 0) Inventory ---\n",
    "head(\"INPUTS\")\n",
    "p(f\"{PAIRS} | exists={PAIRS.exists()} | size={human(PAIRS.stat().st_size) if PAIRS.exists() else '‚Äî'}\")\n",
    "p(f\"{META_JSON} | exists={META_JSON.exists()}\")\n",
    "p(f\"{FILTER_META_JSON} | exists={FILTER_META_JSON.exists()}\")\n",
    "\n",
    "meta = json.loads(META_JSON.read_text()) if META_JSON.exists() else {}\n",
    "filter_meta = json.loads(FILTER_META_JSON.read_text()) if FILTER_META_JSON.exists() else {}\n",
    "total_tokens = meta.get(\"stats\", {}).get(\"total_tokens\", None)\n",
    "\n",
    "p(\"\\nMeta excerpt:\")\n",
    "p(json.dumps({\"stats\": meta.get(\"stats\", {}), \"config\": meta.get(\"config\", {})}, indent=2))\n",
    "p(\"\\nFilter meta excerpt:\")\n",
    "p(json.dumps(filter_meta.get(\"filters\", {}), indent=2) if filter_meta else \"‚Äî\")\n",
    "\n",
    "if not PAIRS.exists():\n",
    "    raise FileNotFoundError(\"Filtered pairs parquet not found. Run cell3b_filter_and_audit.py first.\")\n",
    "\n",
    "# --- 1) STREAM EDGES ‚Üí BUILD DSU + DEG FEATURES ---\n",
    "head(\"BUILD GRAPH (UNION-FIND) + DEGREE FEATURES (STREAMING)\")\n",
    "\n",
    "dsobj = ds.dataset(PAIRS.as_posix(), format=\"parquet\")\n",
    "scanner = dsobj.scanner(columns=[\"token_a\",\"token_b\",\"cosine_sim\",\"rank\"], batch_size=250_000)\n",
    "\n",
    "tok2id: Dict[str,int] = {}\n",
    "id2tok: List[str] = []\n",
    "def get_id(s: str) -> int:\n",
    "    i = tok2id.get(s)\n",
    "    if i is None:\n",
    "        i = len(id2tok)\n",
    "        tok2id[s] = i\n",
    "        id2tok.append(s)\n",
    "    return i\n",
    "\n",
    "dsu = DSU()\n",
    "deg = Counter()       # degree count\n",
    "wdeg = Counter()      # weighted degree (sum cos)\n",
    "maxsim = Counter()    # max neighbor sim\n",
    "\n",
    "n_edges = 0\n",
    "t0 = time.time()\n",
    "for batch in scanner.to_batches():\n",
    "    df = batch.to_pandas()\n",
    "    for a, b, sim in zip(df[\"token_a\"], df[\"token_b\"], df[\"cosine_sim\"]):\n",
    "        ia, ib = get_id(a), get_id(b)\n",
    "        dsu.union(ia, ib)\n",
    "        deg[ia] += 1; deg[ib] += 1\n",
    "        wdeg[ia] += float(sim); wdeg[ib] += float(sim)\n",
    "        if float(sim) > maxsim[ia]: maxsim[ia] = float(sim)\n",
    "        if float(sim) > maxsim[ib]: maxsim[ib] = float(sim)\n",
    "        n_edges += 1\n",
    "\n",
    "n_nodes = len(id2tok)\n",
    "elapsed = time.time() - t0\n",
    "p(f\"Nodes={n_nodes:,} | Edges={n_edges:,} | time={elapsed:.2f}s\")\n",
    "\n",
    "coverage = (n_nodes / total_tokens * 100.0) if total_tokens else None\n",
    "if coverage is not None:\n",
    "    p(f\"Token coverage vs meta.total_tokens: {coverage:.2f}%\")\n",
    "\n",
    "# --- 2) COMPONENTS ---\n",
    "head(\"COMPUTE CONNECTED COMPONENTS\")\n",
    "t1 = time.time()\n",
    "root_of = [dsu.find(i) for i in range(n_nodes)]\n",
    "comp2nodes: Dict[int, List[int]] = defaultdict(list)\n",
    "for i, r in enumerate(root_of):\n",
    "    comp2nodes[r].append(i)\n",
    "n_comps = len(comp2nodes)\n",
    "sizes = [len(v) for v in comp2nodes.values()]\n",
    "p(f\"Components={n_comps:,} | mean size={np.mean(sizes):.2f} | median={np.median(sizes):.0f} | max={np.max(sizes):,}\")\n",
    "# Size buckets\n",
    "bins = [(1,1),(2,2),(3,5),(6,10),(11,20),(21,50),(51,100),(101,99999999)]\n",
    "bucket_counts = []\n",
    "for lo,hi in bins:\n",
    "    c = sum(1 for s in sizes if lo <= s <= hi)\n",
    "    bucket_counts.append(((lo,hi), c))\n",
    "p(\"Size buckets:\")\n",
    "for (lo,hi), c in bucket_counts:\n",
    "    p(f\"{lo:>3}-{hi:<3}: {c:,}\")\n",
    "p(f\"time={time.time()-t1:.2f}s\")\n",
    "\n",
    "# --- 3) BUILD DATAFRAMES: token‚Üícluster, summary per cluster ---\n",
    "head(\"BUILD CLUSTER TABLES\")\n",
    "t2 = time.time()\n",
    "# cluster id: compact 0..C-1\n",
    "roots_sorted = sorted(comp2nodes.keys(), key=lambda r: (-len(comp2nodes[r]), r))\n",
    "root_to_cid = {r:i for i,r in enumerate(roots_sorted)}\n",
    "\n",
    "rows_map = []\n",
    "for r, nodes in comp2nodes.items():\n",
    "    cid = root_to_cid[r]\n",
    "    for i in nodes:\n",
    "        s = id2tok[i]\n",
    "        rows_map.append((\n",
    "            s, cid, int(deg[i]), float(wdeg[i]),\n",
    "            flag_short(s), flag_digit(s), flag_zz(s)\n",
    "        ))\n",
    "\n",
    "map_df = pd.DataFrame(rows_map, columns=[\"token\",\"cluster_id\",\"degree\",\"wdegree\",\"is_short\",\"has_digit\",\"starts_zz\"])\n",
    "\n",
    "# cluster summary\n",
    "summ_rows = []\n",
    "for r, nodes in comp2nodes.items():\n",
    "    cid = root_to_cid[r]\n",
    "    size = len(nodes)\n",
    "    # medoid = max wdegree (why: most connected/central)\n",
    "    med_i = max(nodes, key=lambda i: (wdeg[i], deg[i]))\n",
    "    med_token = id2tok[med_i]\n",
    "    med_deg = int(deg[med_i]); med_wdeg = float(wdeg[med_i])\n",
    "    # flags\n",
    "    short_rate = np.mean([flag_short(id2tok[i]) for i in nodes])\n",
    "    digit_rate = np.mean([flag_digit(id2tok[i]) for i in nodes])\n",
    "    zz_rate = np.mean([flag_zz(id2tok[i]) for i in nodes])\n",
    "    mean_deg = float(np.mean([deg[i] for i in nodes]))\n",
    "    mean_wdeg = float(np.mean([wdeg[i] for i in nodes]))\n",
    "    summ_rows.append((\n",
    "        cid, size, med_token, med_deg, med_wdeg, mean_deg, mean_wdeg, short_rate, digit_rate, zz_rate\n",
    "    ))\n",
    "\n",
    "sum_df = pd.DataFrame(\n",
    "    summ_rows,\n",
    "    columns=[\"cluster_id\",\"size\",\"medoid\",\"medoid_deg\",\"medoid_wdeg\",\"mean_deg\",\"mean_wdeg\",\"short_rate\",\"digit_rate\",\"zz_rate\"]\n",
    ").sort_values([\"size\",\"medoid_wdeg\"], ascending=[False, False])\n",
    "\n",
    "p(f\"map_df: {len(map_df):,} rows | clusters: {len(sum_df):,} | time={time.time()-t2:.2f}s\")\n",
    "\n",
    "# --- 4) PRINT EXCESSIVE DIAGNOSTICS ---\n",
    "head(\"CLUSTER DIAGNOSTICS (PRINTS)\")\n",
    "\n",
    "# Top 20 largest clusters\n",
    "topk = sum_df.head(20)\n",
    "p(\"\\nTop 20 clusters (size, medoid, deg, flags):\")\n",
    "p(topk[[\"cluster_id\",\"size\",\"medoid\",\"medoid_deg\",\"medoid_wdeg\",\"short_rate\",\"digit_rate\",\"zz_rate\"]].to_string(index=False))\n",
    "\n",
    "# Problematic clusters by flags\n",
    "flaggy = sum_df.query(\"short_rate>0.50 or digit_rate>0.50 or zz_rate>0.50\").head(20)\n",
    "p(\"\\nTop flagged clusters (potential garbage):\")\n",
    "p(flaggy.to_string(index=False) if not flaggy.empty else \"(none)\")\n",
    "\n",
    "# Cluster size histogram\n",
    "hist_bins = [1,2,3,5,10,20,50,100,200,500,1000,999999]\n",
    "hist_counts = {}\n",
    "sizes_arr = sum_df[\"size\"].to_numpy()\n",
    "for i in range(len(hist_bins)-1):\n",
    "    lo, hi = hist_bins[i], hist_bins[i+1]\n",
    "    cnt = int(((sizes_arr >= lo) & (sizes_arr < hi)).sum())\n",
    "    hist_counts[f\"{lo}-{hi-1}\"] = cnt\n",
    "p(\"\\nCluster size histogram:\")\n",
    "for k in hist_counts:\n",
    "    p(f\"{k}: {hist_counts[k]:,}\")\n",
    "\n",
    "# Show a few clusters with examples\n",
    "def show_cluster(cid: int, n_tokens: int = 15):\n",
    "    sub = map_df[map_df[\"cluster_id\"] == cid].copy()\n",
    "    sub.sort_values([\"degree\",\"wdegree\"], ascending=False, inplace=True)\n",
    "    p(f\"\\n[Cluster {cid}] size={len(sub):,}  medoid='{sum_df.loc[sum_df['cluster_id']==cid,'medoid'].values[0]}'\")\n",
    "    p(sub.head(n_tokens)[[\"token\",\"degree\",\"wdegree\",\"is_short\",\"has_digit\",\"starts_zz\"]].to_string(index=False))\n",
    "\n",
    "p(\"\\nExamples from a few largest clusters:\")\n",
    "for cid in topk[\"cluster_id\"].head(5).tolist():\n",
    "    show_cluster(cid)\n",
    "\n",
    "# Sample edges for those clusters (for eyeballing)\n",
    "head(\"EDGE SAMPLES FROM TOP CLUSTERS\")\n",
    "need_cids = set(topk[\"cluster_id\"].head(5).tolist())\n",
    "cid_of_token = dict(zip(map_df[\"token\"], map_df[\"cluster_id\"]))\n",
    "samples = []\n",
    "scan2 = dsobj.scanner(columns=[\"token_a\",\"token_b\",\"cosine_sim\",\"rank\"], batch_size=250_000)\n",
    "take_per_cid = 200  # cap prints\n",
    "kept_per_cid = Counter()\n",
    "for batch in scan2.to_batches():\n",
    "    df = batch.to_pandas()\n",
    "    for a, b, sim, r in zip(df[\"token_a\"], df[\"token_b\"], df[\"cosine_sim\"], df[\"rank\"]):\n",
    "        ca = cid_of_token.get(a); cb = cid_of_token.get(b)\n",
    "        if ca is None or cb is None or ca != cb or ca not in need_cids:\n",
    "            continue\n",
    "        if kept_per_cid[ca] >= take_per_cid:\n",
    "            continue\n",
    "        samples.append((ca, a, b, float(sim), int(r)))\n",
    "        kept_per_cid[ca] += 1\n",
    "        if all(kept_per_cid[c] >= take_per_cid for c in need_cids):\n",
    "            break\n",
    "    if all(kept_per_cid[c] >= take_per_cid for c in need_cids):\n",
    "        break\n",
    "\n",
    "edge_samples_df = pd.DataFrame(samples, columns=[\"cluster_id\",\"token_a\",\"token_b\",\"cosine_sim\",\"rank\"])\n",
    "p(edge_samples_df.head(40).to_string(index=False))\n",
    "\n",
    "# Borderline per-cluster (0.50‚Äì0.55) to spot weak links\n",
    "border = edge_samples_df[(edge_samples_df[\"cosine_sim\"] >= 0.50) & (edge_samples_df[\"cosine_sim\"] < 0.55)]\n",
    "p(f\"\\nBorderline edges in sampled top clusters (0.50‚Äì0.55): {len(border):,}\")\n",
    "if not border.empty:\n",
    "    p(border.sort_values([\"cosine_sim\"]).head(30).to_string(index=False))\n",
    "\n",
    "# --- 5) SAVE ARTIFACTS ---\n",
    "head(\"SAVE CLUSTER ARTIFACTS\")\n",
    "map_df.to_parquet(CLUSTERS_MAP, index=False)\n",
    "sum_df.to_parquet(CLUSTERS_SUM, index=False)\n",
    "edge_samples_df.to_parquet(CLUSTERS_EDGES_SAMPLE, index=False)\n",
    "p(f\"‚úÖ Saved token‚Üícluster map ‚Üí {CLUSTERS_MAP}\")\n",
    "p(f\"‚úÖ Saved cluster summary ‚Üí {CLUSTERS_SUM}\")\n",
    "p(f\"‚úÖ Saved edge samples ‚Üí {CLUSTERS_EDGES_SAMPLE}\")\n",
    "\n",
    "# --- 6) NEXT-STEP GUIDANCE (printed) ---\n",
    "head(\"NEXT-STEP SUGGESTIONS (ACTIONABLE)\")\n",
    "p(\"- If flagged clusters dominate (short/digits/zz), add a **cluster-level gate**: drop clusters with flag_rate>0.7 or size<3 and high flag rates.\")\n",
    "p(\"- Promote medoid tokens as **canonical labels**; export token‚Üícanonical map (medoid).\")\n",
    "p(\"- If many borderline edges within large clusters, raise BASE_THRESHOLD to 0.55 and re-run filter.\")\n",
    "p(\"- If hubiness resurfaces in clusters, consider **per-length thresholds** (len‚â§4 ‚Üí sim‚â•0.85) and **token stoplist** (e.g., exact 'zz*').\")\n",
    "p(\"- Proceed to **merge clusters with high medoid similarity** (medoid‚Äìmedoid sim‚â•0.85) to collapse near-dup clusters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "üîé INPUTS\n",
      "========================================================================================================================\n",
      "romance-novel-nlp-research/src/eda_analysis/outputs/candidate_similarity_pairs.filtered.parquet | exists=True | size=19.02 MB\n",
      "romance-novel-nlp-research/src/eda_analysis/outputs/clusters_token_map.parquet | exists=True | size=4.22 MB\n",
      "romance-novel-nlp-research/src/eda_analysis/outputs/clusters_summary.parquet | exists=True | size=0.16 MB\n",
      "clusters: 4,223 | tokens mapped: 232,918\n",
      "========================================================================================================================\n",
      "üîé SELECT LARGEST CLUSTER\n",
      "========================================================================================================================\n",
      "Picked cluster_id=0 | size=218,379 | medoid='manga graphic novels comics'\n",
      "Collected tokens for giant cluster: 218,379\n",
      "========================================================================================================================\n",
      "üîé LOAD STOPWORDS & BUILD CONTENT WORDS\n",
      "========================================================================================================================\n",
      "‚úÖ Stopwords backend: sklearn | langs=['en'] | count=318\n",
      "========================================================================================================================\n",
      "üîé BUILD CONTENT-WORD SETS (GIANT CLUSTER)\n",
      "========================================================================================================================\n",
      "Built content sets: 218,379 | time=1.70s\n",
      "========================================================================================================================\n",
      "üîé STREAM & FILTER EDGES WITH LEXICAL OVERLAP\n",
      "========================================================================================================================\n",
      "Scanned in-giant edges: 1,155,628 | kept=1,135,066 | time=9.67s\n",
      "Drop reasons:\n",
      "- short_guard_fail: 1,946\n",
      "- no_word_overlap: 18,616\n",
      "========================================================================================================================\n",
      "üîé BUILD SUBGRAPH & SUBCLUSTERS\n",
      "========================================================================================================================\n",
      "Subclusters=2,797 | max=213,904 | median=1 | mean=78.08\n",
      "\n",
      "Top 15 subclusters:\n",
      " sub_id   size                         medoid  medoid_deg  medoid_mean_sim\n",
      "      0 213904    manga graphic novels comics          20         0.987681\n",
      "    746     50                 tissues needed          20         0.861505\n",
      "    276     30                      m m f m m          20         0.976353\n",
      "     22     28                      hp hp ish          18         0.820120\n",
      "    503     20 fuck logic this makes no sense          18         0.629125\n",
      "    463     16              star trek voyager          12         0.718812\n",
      "    905     16                      postponed          12         0.643317\n",
      "    433     14            want to cuddle with          18         0.778714\n",
      "    364     14                       contempt          12         0.790552\n",
      "   1429     13              fiction indie spa          16         0.810587\n",
      "    642     13                          qubec          10         0.630647\n",
      "   2088     12             the lords of satyr          18         0.823284\n",
      "    864     12                          zebra          10         0.678015\n",
      "    967     11      roxie rivera lolita lopez          18         0.763837\n",
      "   1787     11                      travis tm          16         0.633024\n",
      "========================================================================================================================\n",
      "üîé EDGE SAMPLES FROM BIG SUBCLUSTERS\n",
      "========================================================================================================================\n",
      " sub_id                           token_a                           token_b  cosine_sim  rank\n",
      "      0                               0 0                             0 0 0    0.975303     1\n",
      "      0                               0 0                             0 0 1    0.906423     3\n",
      "      0                               0 0                             0 0 2    0.850747     9\n",
      "      0                             0 0 0                               0 0    0.975303     1\n",
      "      0                             0 0 0                           0 0 0 1    0.919496     2\n",
      "      0                             0 0 0                             0 0 1    0.908444     3\n",
      "      0                             0 0 0                    0 0 0 0over it    0.899378     4\n",
      "      0                             0 0 0                           0 0 0 a    0.874364     5\n",
      "      0                             0 0 0                        0 0 0 next    0.857385     6\n",
      "      0                             0 0 0                    0 0 eventually    0.855615     7\n",
      "      0                             0 0 0                          0 0 asap    0.852846     8\n",
      "      0                             0 0 0                             0 0 2    0.837812     9\n",
      "      0                      0 0 0 0check                           0 check    0.793832     1\n",
      "      0                      0 0 0 0check                          0 checkf    0.769414     2\n",
      "      0                      0 0 0 0check                           01check    0.727421     5\n",
      "      0                      0 0 0 0check                    0 to check out    0.683171     9\n",
      "      0                    0 0 0 0over it                             0 0 0    0.899378     1\n",
      "      0                    0 0 0 0over it                           0 0 0 1    0.840687     3\n",
      "      0                    0 0 0 0over it                           0 0 0 a    0.815835     4\n",
      "      0                    0 0 0 0over it                             0 0 1    0.807630     5\n",
      "      0                    0 0 0 0over it                    0 0 eventually    0.777979     7\n",
      "      0                    0 0 0 0over it                           0 0 0 b    0.771831     8\n",
      "      0                    0 0 0 0over it                          0 0 asap    0.769464     9\n",
      "      0                    0 0 0 0over it                   0 0 0 this year    0.749866    10\n",
      "      0                           0 0 0 1                             0 0 1    0.983803     1\n",
      "      0                           0 0 0 1                             0 0 0    0.919496     2\n",
      "      0                           0 0 0 1                      0 0 0 next 1    0.852676     5\n",
      "      0                           0 0 0 1                        0 0 0 next    0.840824     6\n",
      "      0                           0 0 0 1                    0 0 0 0over it    0.840687     7\n",
      "      0                           0 0 0 1                           0 0 0 a    0.815703     8\n",
      "      0                           0 0 0 1                             0 0 2    0.814457     9\n",
      "      0                           0 0 0 1                           0 0 0 b    0.792719    10\n",
      "      0                 0 0 0 300 in 2015                   0 0 250 in 2014    0.804723     1\n",
      "      0                 0 0 0 300 in 2015                            0 2015    0.790982     2\n",
      "      0                 0 0 0 300 in 2015                 0 0 2 try in 2014    0.679918     5\n",
      "      0                           0 0 0 a                             0 a 0    0.973835     1\n",
      "      0                           0 0 0 a                             0 0 0    0.874364     2\n",
      "      0                           0 0 0 a                             0 a 1    0.850235     3\n",
      "      0                           0 0 0 a                    0 0 0 0over it    0.815835     5\n",
      "      0                           0 0 0 a                           0 0 0 1    0.815703     6\n",
      "     22                           2017 hp                           hp 2017    0.961697     1\n",
      "     22                           2017 hp                           hp 2016    0.848191     2\n",
      "     22                           2017 hp                           hp 2015    0.811461     4\n",
      "     22                           2017 hp                            old hp    0.807967     5\n",
      "     22                           2017 hp                            new hp    0.802401     6\n",
      "     22                           2017 hp                         hp hp ish    0.787189     8\n",
      "     22                           2017 hp                          hpb 2017    0.784654     9\n",
      "     22                           2017 hp                           hp 2014    0.777468    10\n",
      "     22                  all time fav hps                      favorite hps    0.846406     1\n",
      "     22                  all time fav hps                       favorite hp    0.789659     2\n",
      "     22                  all time fav hps                      favorites hp    0.747452     3\n",
      "     22                  all time fav hps                   hp s most hated    0.708674     4\n",
      "    503 annoying gaps in logic ridiculous    writing annoying gaps in logic    0.878809     1\n",
      "    503 annoying gaps in logic ridiculous       logic thrown out the window    0.616120     2\n",
      "    503 annoying gaps in logic ridiculous                        logic fail    0.603382     3\n",
      "    503 annoying gaps in logic ridiculous                no logic available    0.560892     4\n",
      "    503 annoying gaps in logic ridiculous    we don t need no stinkin logic    0.512816     9\n",
      "    503 annoying gaps in logic ridiculous        because logic is overrated    0.512342    10\n",
      "    503        because logic is overrated    we don t need no stinkin logic    0.528591     3\n",
      "    503        because logic is overrated annoying gaps in logic ridiculous    0.512342     6\n",
      "========================================================================================================================\n",
      "üîé REASSIGN CLUSTER IDS (REFINE GIANT ONLY)\n",
      "========================================================================================================================\n",
      "Refined clusters: 7,019 (giant ‚Üí 2,797 subclusters)\n",
      "\n",
      "Refined top 15 clusters:\n",
      " cluster_id   size                         medoid  medoid_deg  medoid_wdeg  mean_deg  mean_wdeg  short_rate  digit_rate  zz_rate\n",
      "          0 213904    manga graphic novels comics          20     0.987681 10.563804   0.796383    0.002473    0.134883 0.002786\n",
      "          1    328               150 to 200 pages          20    19.306986 12.146341  11.178399    0.000000    0.993902 0.000000\n",
      "          2    140      meet n greet 2015 dec jan          20    18.484929 14.428571  12.805877    0.000000    0.685714 0.014286\n",
      "          3     96           erotica bdsm romance          20    19.272889 11.708333  11.020980    0.000000    0.031250 0.000000\n",
      "          4     77              01 june utc bonus          20    19.072018 11.480519  10.598638    0.000000    0.623377 0.000000\n",
      "          6     60 2015 read a thon challenge jan          20    19.012063 14.200000  12.836556    0.000000    0.866667 0.016667\n",
      "          5     60                      1 on deck          20    16.895066 11.866667   9.702792    0.000000    0.416667 0.000000\n",
      "          7     59           00 feb ult challenge          20    19.234684 11.389831  10.681322    0.000000    0.983051 0.000000\n",
      "          8     53 pride and prejudice variations          20    18.590283 12.226415  11.013900    0.000000    0.000000 0.000000\n",
      "          9     51                  wattpad books          20    18.838457 12.627451  11.489220    0.000000    0.058824 0.000000\n",
      "        746     50                 tissues needed          20     0.861505 12.960000   0.801729    0.000000    0.000000 0.000000\n",
      "         10     46    debut author challenge 2013          20    19.180490 14.695652  13.777565    0.000000    0.826087 0.000000\n",
      "         12     45          fairy tale retellings          20    19.345152 12.888889  12.308549    0.000000    0.000000 0.000000\n",
      "         11     45                 sfrc fall 2016          20    18.244408 12.800000  10.828341    0.000000    0.644444 0.000000\n",
      "         13     44                      abandoned          20    17.771411 10.909091   9.078963    0.000000    0.000000 0.000000\n",
      "========================================================================================================================\n",
      "üîé SAVE ARTIFACTS\n",
      "========================================================================================================================\n",
      "‚úÖ Saved refined token‚Üícluster map ‚Üí romance-novel-nlp-research/src/eda_analysis/outputs/clusters_token_map.refined.parquet\n",
      "‚úÖ Saved refined cluster summary ‚Üí romance-novel-nlp-research/src/eda_analysis/outputs/clusters_summary.refined.parquet\n",
      "‚úÖ Saved refined giant-cluster edges ‚Üí romance-novel-nlp-research/src/eda_analysis/outputs/candidate_similarity_pairs.cluster0_refined.parquet\n",
      "========================================================================================================================\n",
      "üîé NEXT STEP GUIDANCE\n",
      "========================================================================================================================\n",
      "- If overlap is too strict, reduce MIN_SHARED_WORDS or lower BASE_SIM_STRONG (e.g., 0.58).\n",
      "- For multilingual data, set STOP_LANGS=['en','es','de',...] to broaden stopword removal.\n",
      "- Export canonical map (token ‚Üí medoid) and apply to shelves next.\n"
     ]
    }
   ],
   "source": [
    "# romance-novel-nlp-research/src/eda_analysis/cell3d_refine_giant_cluster.py\n",
    "from __future__ import annotations\n",
    "import re, sys, time, json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple\n",
    "from collections import defaultdict, Counter, deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- Paths ----------\n",
    "OUT = Path(\"romance-novel-nlp-research/src/eda_analysis/outputs\")\n",
    "PAIR_F = OUT / \"candidate_similarity_pairs.filtered.parquet\"\n",
    "MAP_F  = OUT / \"clusters_token_map.parquet\"\n",
    "SUM_F  = OUT / \"clusters_summary.parquet\"\n",
    "\n",
    "REF_MAP_F = OUT / \"clusters_token_map.refined.parquet\"\n",
    "REF_SUM_F = OUT / \"clusters_summary.refined.parquet\"\n",
    "REF_EDGES_F = OUT / \"candidate_similarity_pairs.cluster0_refined.parquet\"\n",
    "\n",
    "# ---------- Config (tweak) ----------\n",
    "BASE_SIM_STRONG = 0.60\n",
    "BASE_SIM_WEAK   = 0.50\n",
    "MIN_SHARED_WORDS = 1\n",
    "SHORT_LEN_MAX = 3\n",
    "SHORT_RF_MIN = 0.70\n",
    "\n",
    "STOP_LANGS: List[str] = [\"en\"]  # e.g., [\"en\",\"es\",\"de\"]\n",
    "STOP_BACKEND: str = \"auto\"      # \"auto\"|\"iso\"|\"nltk\"|\"spacy\"|\"sklearn\"\n",
    "\n",
    "PRINT_EDGE_SAMPLES = 40\n",
    "TOP_CLUSTERS_PRINT = 20\n",
    "SUBCLUSTER_SAMPLE_TOKENS = 12\n",
    "\n",
    "# ---------- Imports ----------\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.dataset as ds\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Requires pyarrow. Install: pip install pyarrow\") from e\n",
    "\n",
    "try:\n",
    "    from rapidfuzz.fuzz import ratio as fuzz_ratio\n",
    "    HAS_RF = True\n",
    "except Exception:\n",
    "    HAS_RF = False\n",
    "\n",
    "# ---------- Printing ----------\n",
    "def p(x: str): print(x); sys.stdout.flush()\n",
    "def head(t: str):\n",
    "    sep = \"=\"*120\n",
    "    p(sep); p(f\"üîé {t}\"); p(sep)\n",
    "\n",
    "# ---------- Stopwords loader ----------\n",
    "_LANG_MAP_NLTK = {\n",
    "    \"ar\":\"arabic\",\"da\":\"danish\",\"nl\":\"dutch\",\"en\":\"english\",\"fi\":\"finnish\",\"fr\":\"french\",\"de\":\"german\",\n",
    "    \"hu\":\"hungarian\",\"it\":\"italian\",\"kk\":\"kazakh\",\"ne\":\"nepali\",\"no\":\"norwegian\",\"pt\":\"portuguese\",\n",
    "    \"ro\":\"romanian\",\"ru\":\"russian\",\"sl\":\"slovene\",\"es\":\"spanish\",\"sv\":\"swedish\",\"tr\":\"turkish\"\n",
    "}\n",
    "def load_stopwords(langs: List[str], backend: str = \"auto\") -> Tuple[Set[str], str]:\n",
    "    langs = [l.lower() for l in langs]\n",
    "    tried = []\n",
    "    # 1) stopwords-iso\n",
    "    if backend in (\"auto\",\"iso\"):\n",
    "        try:\n",
    "            from stopwordsiso import stopwords as sw_iso  # pip install stopwordsiso\n",
    "            sw: Set[str] = set()\n",
    "            for l in langs:\n",
    "                try:\n",
    "                    sw |= set(w.lower() for w in sw_iso(l))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if sw:\n",
    "                return sw, \"stopwords-iso\"\n",
    "            tried.append(\"stopwords-iso(empty)\")\n",
    "        except Exception:\n",
    "            tried.append(\"stopwords-iso(missing)\")\n",
    "            if backend == \"iso\":\n",
    "                raise RuntimeError(\"Install stopwords-iso: pip install stopwordsiso\")\n",
    "    # 2) NLTK\n",
    "    if backend in (\"auto\",\"nltk\"):\n",
    "        try:\n",
    "            import nltk\n",
    "            try:\n",
    "                from nltk.corpus import stopwords as nltk_sw\n",
    "                sw = set()\n",
    "                for l in langs:\n",
    "                    name = _LANG_MAP_NLTK.get(l, \"english\")\n",
    "                    try:\n",
    "                        sw |= set(w.lower() for w in nltk_sw.words(name))\n",
    "                    except LookupError:\n",
    "                        nltk.download(\"stopwords\", quiet=True)\n",
    "                        sw |= set(w.lower() for w in nltk_sw.words(name))\n",
    "                if sw:\n",
    "                    return sw, \"nltk\"\n",
    "                tried.append(\"nltk(empty)\")\n",
    "            except Exception:\n",
    "                tried.append(\"nltk(corpus err)\")\n",
    "        except Exception:\n",
    "            tried.append(\"nltk(missing)\")\n",
    "            if backend == \"nltk\":\n",
    "                raise RuntimeError(\"Install NLTK: pip install nltk\")\n",
    "    # 3) spaCy\n",
    "    if backend in (\"auto\",\"spacy\"):\n",
    "        try:\n",
    "            import spacy\n",
    "            sw = set()\n",
    "            for l in langs:\n",
    "                try:\n",
    "                    nlp = spacy.blank(l)\n",
    "                    sw |= set(w.lower() for w in nlp.Defaults.stop_words)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if sw:\n",
    "                return sw, \"spacy\"\n",
    "            tried.append(\"spacy(empty)\")\n",
    "        except Exception:\n",
    "            tried.append(\"spacy(missing)\")\n",
    "            if backend == \"spacy\":\n",
    "                raise RuntimeError(\"Install spaCy: pip install spacy\")\n",
    "    # 4) scikit-learn (English only)\n",
    "    if backend in (\"auto\",\"sklearn\"):\n",
    "        try:\n",
    "            from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "            return set(w.lower() for w in ENGLISH_STOP_WORDS), \"sklearn\"\n",
    "        except Exception:\n",
    "            tried.append(\"sklearn(missing)\")\n",
    "            if backend == \"sklearn\":\n",
    "                raise RuntimeError(\"Install scikit-learn: pip install scikit-learn\")\n",
    "    # 5) Minimal fallback\n",
    "    base = {\"a\",\"an\",\"the\",\"and\",\"or\",\"to\",\"of\",\"in\",\"on\",\"for\",\"with\",\"without\",\"by\",\"at\",\"as\",\"from\",\n",
    "            \"this\",\"that\",\"these\",\"those\",\"it\",\"its\",\"is\",\"are\",\"be\",\"was\",\"were\",\"been\",\"am\",\"i\",\"you\",\n",
    "            \"he\",\"she\",\"they\",\"we\",\"me\",\"him\",\"her\",\"them\",\"my\",\"your\",\"our\",\"their\",\"not\",\"no\",\"yes\",\n",
    "            \"into\",\"over\",\"under\",\"up\",\"down\",\"out\",\"more\",\"most\",\"less\",\"least\",\"very\"}\n",
    "    p(f\"‚ö†Ô∏è Using minimal fallback stopword list. Tried: {', '.join(tried)}\")\n",
    "    return base, \"fallback\"\n",
    "\n",
    "TOKEN_RE = re.compile(r\"[a-z]{2,}\")  # keep simple Latin 2+ letters\n",
    "\n",
    "def build_content_filter(langs: List[str], backend: str = \"auto\"):\n",
    "    sw, used = load_stopwords(langs, backend)\n",
    "    p(f\"‚úÖ Stopwords backend: {used} | langs={langs} | count={len(sw)}\")\n",
    "    sw = set(sw)  # ensure set\n",
    "    def content_words(s: str) -> Set[str]:\n",
    "        # why: reduce semantic chaining with content word overlap\n",
    "        words = TOKEN_RE.findall(s.lower())\n",
    "        return {w for w in words if w not in sw}\n",
    "    return content_words\n",
    "\n",
    "# ---------- Short guard ----------\n",
    "def short_guard(a: str, b: str) -> bool:\n",
    "    if min(len(a), len(b)) > SHORT_LEN_MAX:\n",
    "        return True\n",
    "    if not HAS_RF:\n",
    "        return True\n",
    "    return (fuzz_ratio(a, b) / 100.0) >= SHORT_RF_MIN\n",
    "\n",
    "# ---------- 0) Load inputs ----------\n",
    "head(\"INPUTS\")\n",
    "for f in [PAIR_F, MAP_F, SUM_F]:\n",
    "    p(f\"{f} | exists={f.exists()} | size={(f.stat().st_size/1024/1024):.2f} MB\" if f.exists() else f\"{f} | MISSING\")\n",
    "if not (PAIR_F.exists() and MAP_F.exists() and SUM_F.exists()):\n",
    "    raise FileNotFoundError(\"Required inputs missing. Run previous cells (filter + cluster).\")\n",
    "\n",
    "sum_df = pd.read_parquet(SUM_F)\n",
    "map_df = pd.read_parquet(MAP_F)\n",
    "p(f\"clusters: {len(sum_df):,} | tokens mapped: {len(map_df):,}\")\n",
    "\n",
    "# ---------- 1) Select the largest cluster ----------\n",
    "head(\"SELECT LARGEST CLUSTER\")\n",
    "top = sum_df.sort_values([\"size\",\"medoid_wdeg\"], ascending=[False, False]).iloc[0]\n",
    "giant_cid = int(top[\"cluster_id\"]); giant_size = int(top[\"size\"])\n",
    "p(f\"Picked cluster_id={giant_cid} | size={giant_size:,} | medoid='{top['medoid']}'\")\n",
    "giant_tokens = map_df.loc[map_df[\"cluster_id\"] == giant_cid, \"token\"].tolist()\n",
    "giant_set = set(giant_tokens)\n",
    "p(f\"Collected tokens for giant cluster: {len(giant_tokens):,}\")\n",
    "\n",
    "# ---------- 2) Stopwords-driven content words ----------\n",
    "head(\"LOAD STOPWORDS & BUILD CONTENT WORDS\")\n",
    "content_words = build_content_filter(STOP_LANGS, STOP_BACKEND)\n",
    "\n",
    "# ---------- 3) Precompute content-word sets ----------\n",
    "head(\"BUILD CONTENT-WORD SETS (GIANT CLUSTER)\")\n",
    "t0 = time.time()\n",
    "cw: Dict[str, Set[str]] = {}\n",
    "for s in giant_tokens:\n",
    "    cw[s] = content_words(s)\n",
    "p(f\"Built content sets: {len(cw):,} | time={time.time()-t0:.2f}s\")\n",
    "\n",
    "# ---------- 4) Stream edges and keep those that pass gates ----------\n",
    "head(\"STREAM & FILTER EDGES WITH LEXICAL OVERLAP\")\n",
    "dsobj = ds.dataset(PAIR_F.as_posix(), format=\"parquet\")\n",
    "scanner = dsobj.scanner(columns=[\"token_a\",\"token_b\",\"cosine_sim\",\"rank\"], batch_size=250_000)\n",
    "\n",
    "kept_edges: List[Tuple[str,str,float,int]] = []\n",
    "dropped = Counter()\n",
    "t1 = time.time()\n",
    "rows = 0\n",
    "for batch in scanner.to_batches():\n",
    "    df = batch.to_pandas()\n",
    "    m = df[\"token_a\"].isin(giant_set) & df[\"token_b\"].isin(giant_set)\n",
    "    if not m.any():\n",
    "        continue\n",
    "    sub = df[m]\n",
    "    for a, b, sim, r in zip(sub[\"token_a\"], sub[\"token_b\"], sub[\"cosine_sim\"], sub[\"rank\"]):\n",
    "        rows += 1\n",
    "        s = float(sim)\n",
    "        if not short_guard(a, b):\n",
    "            dropped[\"short_guard_fail\"] += 1\n",
    "            continue\n",
    "        if s >= BASE_SIM_STRONG:\n",
    "            kept_edges.append((a, b, s, int(r)))\n",
    "        elif s >= BASE_SIM_WEAK:\n",
    "            if len(cw[a] & cw[b]) >= MIN_SHARED_WORDS:\n",
    "                kept_edges.append((a, b, s, int(r)))\n",
    "            else:\n",
    "                dropped[\"no_word_overlap\"] += 1\n",
    "        else:\n",
    "            dropped[\"below_min_sim\"] += 1\n",
    "\n",
    "p(f\"Scanned in-giant edges: {rows:,} | kept={len(kept_edges):,} | time={time.time()-t1:.2f}s\")\n",
    "p(\"Drop reasons:\")\n",
    "for k,v in dropped.items():\n",
    "    p(f\"- {k}: {v:,}\")\n",
    "if not kept_edges:\n",
    "    raise RuntimeError(\"No edges retained; relax gates or check stopword config.\")\n",
    "\n",
    "# ---------- 5) Build subgraph + connected components ----------\n",
    "head(\"BUILD SUBGRAPH & SUBCLUSTERS\")\n",
    "adj: Dict[str, List[str]] = defaultdict(list)\n",
    "for a, b, s, r in kept_edges:\n",
    "    adj[a].append(b); adj[b].append(a)\n",
    "\n",
    "visited = set()\n",
    "sub_components: List[List[str]] = []\n",
    "for node in giant_tokens:\n",
    "    if node in visited: continue\n",
    "    if node not in adj:\n",
    "        visited.add(node); sub_components.append([node]); continue\n",
    "    q = deque([node]); comp = []\n",
    "    visited.add(node)\n",
    "    while q:\n",
    "        u = q.popleft(); comp.append(u)\n",
    "        for v in adj[u]:\n",
    "            if v not in visited:\n",
    "                visited.add(v); q.append(v)\n",
    "    sub_components.append(comp)\n",
    "\n",
    "sizes = sorted((len(c) for c in sub_components), reverse=True)\n",
    "p(f\"Subclusters={len(sub_components):,} | max={sizes[0]:,} | median={int(np.median(sizes))} | mean={np.mean(sizes):.2f}\")\n",
    "\n",
    "# ---------- 6) Medoids ----------\n",
    "deg = {t: len(adj.get(t, [])) for t in giant_tokens}\n",
    "sim_sum = defaultdict(float); sim_cnt = defaultdict(int)\n",
    "for a, b, s, _ in kept_edges:\n",
    "    sim_sum[a] += s; sim_cnt[a] += 1\n",
    "    sim_sum[b] += s; sim_cnt[b] += 1\n",
    "def medoid_of(nodes: List[str]) -> Tuple[str, int, float]:\n",
    "    best, best_score = None, (-1, -1.0)\n",
    "    for t in nodes:\n",
    "        d = deg.get(t, 0)\n",
    "        w = (sim_sum[t]/sim_cnt[t]) if sim_cnt[t] else 0.0\n",
    "        if (d, w) > best_score:\n",
    "            best_score = (d, w); best = t\n",
    "    return best, best_score[0], best_score[1]\n",
    "\n",
    "sub_infos = []\n",
    "for idx, nodes in enumerate(sub_components):\n",
    "    m_tok, m_deg, m_w = medoid_of(nodes)\n",
    "    sub_infos.append((idx, len(nodes), m_tok, m_deg, m_w))\n",
    "sub_sum_df = pd.DataFrame(sub_infos, columns=[\"sub_id\",\"size\",\"medoid\",\"medoid_deg\",\"medoid_mean_sim\"])\\\n",
    "                .sort_values([\"size\",\"medoid_deg\",\"medoid_mean_sim\"], ascending=[False, False, False])\n",
    "p(\"\\nTop 15 subclusters:\")\n",
    "p(sub_sum_df.head(15).to_string(index=False))\n",
    "\n",
    "# ---------- 7) Edge samples ----------\n",
    "head(\"EDGE SAMPLES FROM BIG SUBCLUSTERS\")\n",
    "need_sub = set(sub_sum_df.head(5)[\"sub_id\"].tolist())\n",
    "node_to_sub = {}\n",
    "for sub_id, nodes in enumerate(sub_components):\n",
    "    for t in nodes:\n",
    "        node_to_sub[t] = sub_id\n",
    "\n",
    "samples = []\n",
    "quota = Counter()\n",
    "for a, b, s, r in kept_edges:\n",
    "    sa = node_to_sub.get(a); sb = node_to_sub.get(b)\n",
    "    if sa is None or sa != sb or sa not in need_sub: continue\n",
    "    if quota[sa] >= PRINT_EDGE_SAMPLES: continue\n",
    "    samples.append((sa, a, b, float(s), int(r)))\n",
    "    quota[sa] += 1\n",
    "samples_df = pd.DataFrame(samples, columns=[\"sub_id\",\"token_a\",\"token_b\",\"cosine_sim\",\"rank\"])\n",
    "p(samples_df.head(60).to_string(index=False))\n",
    "\n",
    "# ---------- 8) Reassign cluster IDs (refine giant only) ----------\n",
    "head(\"REASSIGN CLUSTER IDS (REFINE GIANT ONLY)\")\n",
    "ref_map = map_df.copy()\n",
    "mask = ref_map[\"cluster_id\"] == giant_cid\n",
    "ref_map.loc[mask, \"cluster_id\"] = -1\n",
    "\n",
    "BASE = int(giant_cid) * 1_000_000\n",
    "token_to_new = {}\n",
    "for sub_id, nodes in enumerate(sub_components):\n",
    "    new_cid = BASE + sub_id\n",
    "    for t in nodes:\n",
    "        token_to_new[t] = new_cid\n",
    "ref_map.loc[mask, \"cluster_id\"] = ref_map.loc[mask, \"token\"].map(token_to_new).astype(np.int64)\n",
    "\n",
    "ref_sum = sum_df.copy()\n",
    "ref_sum = ref_sum[ref_sum[\"cluster_id\"] != giant_cid]\n",
    "rows = []\n",
    "for sub_id, nodes in enumerate(sub_components):\n",
    "    med, mdeg, mw = medoid_of(nodes)\n",
    "    rows.append({\"cluster_id\": BASE + sub_id, \"size\": len(nodes), \"medoid\": med,\n",
    "                 \"medoid_deg\": mdeg, \"medoid_wdeg\": mw,\n",
    "                 \"mean_deg\": float(np.mean([deg.get(t,0) for t in nodes])),\n",
    "                 \"mean_wdeg\": float(np.mean([(sim_sum[t]/sim_cnt[t]) if sim_cnt[t] else 0.0 for t in nodes])),\n",
    "                 \"short_rate\": float(np.mean([len(t)<=3 for t in nodes])),\n",
    "                 \"digit_rate\": float(np.mean([any(c.isdigit() for c in t) for t in nodes])),\n",
    "                 \"zz_rate\": float(np.mean([t.strip().lower().startswith('zz') for t in nodes]))})\n",
    "ref_sum = pd.concat([ref_sum, pd.DataFrame(rows)], ignore_index=True)\\\n",
    "             .sort_values([\"size\",\"medoid_wdeg\"], ascending=[False, False])\n",
    "\n",
    "p(f\"Refined clusters: {len(ref_sum):,} (giant ‚Üí {len(sub_components):,} subclusters)\")\n",
    "p(\"\\nRefined top 15 clusters:\")\n",
    "p(ref_sum.head(15).to_string(index=False))\n",
    "\n",
    "# ---------- 9) Save artifacts ----------\n",
    "head(\"SAVE ARTIFACTS\")\n",
    "ref_map.to_parquet(REF_MAP_F, index=False)\n",
    "ref_sum.to_parquet(REF_SUM_F, index=False)\n",
    "pd.DataFrame(kept_edges, columns=[\"token_a\",\"token_b\",\"cosine_sim\",\"rank\"]).to_parquet(REF_EDGES_F, index=False)\n",
    "p(f\"‚úÖ Saved refined token‚Üícluster map ‚Üí {REF_MAP_F}\")\n",
    "p(f\"‚úÖ Saved refined cluster summary ‚Üí {REF_SUM_F}\")\n",
    "p(f\"‚úÖ Saved refined giant-cluster edges ‚Üí {REF_EDGES_F}\")\n",
    "\n",
    "# ---------- 10) Next-step guidance ----------\n",
    "head(\"NEXT STEP GUIDANCE\")\n",
    "p(\"- If overlap is too strict, reduce MIN_SHARED_WORDS or lower BASE_SIM_STRONG (e.g., 0.58).\")\n",
    "p(\"- For multilingual data, set STOP_LANGS=['en','es','de',...] to broaden stopword removal.\")\n",
    "p(\"- Export canonical map (token ‚Üí medoid) and apply to shelves next.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RapidFuzz available\n",
      "========================================================================================================================\n",
      "üîé INPUTS\n",
      "========================================================================================================================\n",
      "romance-novel-nlp-research/src/eda_analysis/outputs/clusters_token_map.refined.parquet | exists=True | size=4.24 MB\n",
      "romance-novel-nlp-research/src/eda_analysis/outputs/clusters_summary.refined.parquet | exists=True | size=0.22 MB\n",
      "romance-novel-nlp-research/src/eda_analysis/outputs/candidate_similarity_pairs.cluster0_refined.parquet | exists=True | size=17.83 MB\n",
      "\n",
      "Giant cluster id (refined namespace): 0 | size=213,904\n",
      "Giant tokens mapped: 213,904\n",
      "Giant edges (refined file rows): 1,135,066\n",
      "========================================================================================================================\n",
      "üîé EDGE & TOKEN STATS (GIANT CLUSTER)\n",
      "========================================================================================================================\n",
      "count    1.135066e+06\n",
      "mean     8.118041e-01\n",
      "std      9.405289e-02\n",
      "min      5.005934e-01\n",
      "50%      8.194881e-01\n",
      "75%      8.861359e-01\n",
      "90%      9.316543e-01\n",
      "95%      9.531078e-01\n",
      "99%      9.795010e-01\n",
      "max      9.997207e-01\n",
      "\n",
      "Digit ratio (tokens): mean=0.032 | p90=0.12 | p99=0.50\n",
      "Top token shape prefixes:\n",
      "- 'AAAAAAA_AAAA': 14,429\n",
      "- 'AAAAAA_AAAAA': 12,950\n",
      "- 'AAAAAAAA_AAA': 10,149\n",
      "- 'AAAAA_AAAAAA': 9,805\n",
      "- 'AAAAAAAAA_AA': 8,009\n",
      "- 'AAAA_AAAAAAA': 6,688\n",
      "- 'AAAAAAAAAA_A': 5,007\n",
      "- 'AAAAAAAAAAAA': 4,746\n",
      "- 'AAAA_AAAA_AA': 3,779\n",
      "- 'AAA_AAAAAAAA': 3,052\n",
      "- 'AAAA_AA_AAAA': 3,037\n",
      "- 'AAAAA_AAAA_A': 2,929\n",
      "- 'AAAA_AAA_AAA': 2,613\n",
      "- 'AAAAA_AAA_AA': 2,417\n",
      "- 'AAAAAA': 2,329\n",
      "- 'AAAAAAA': 2,287\n",
      "- 'AAAAA_AAAAA_': 2,248\n",
      "- 'AAAAA_AA_AAA': 2,246\n",
      "- 'AAAA_AAAAA_A': 2,237\n",
      "- 'AA_AAAA_AAAA': 2,169\n",
      "========================================================================================================================\n",
      "üîé CONTENT WORDS + OVERLAP STATS\n",
      "========================================================================================================================\n",
      "‚úÖ stopwords: sklearn (318)\n",
      "Sampled edges: 250,000\n",
      "\n",
      "Shared-word counts (sample):\n",
      "count    250000.000000\n",
      "mean          1.205200\n",
      "std           0.796718\n",
      "min           0.000000\n",
      "50%           1.000000\n",
      "75%           2.000000\n",
      "90%           2.000000\n",
      "95%           3.000000\n",
      "99%           3.000000\n",
      "max           6.000000\n",
      "\n",
      "Jaccard (cw) (sample):\n",
      "count    250000.000000\n",
      "mean          0.449451\n",
      "std           0.318837\n",
      "min           0.000000\n",
      "50%           0.400000\n",
      "75%           0.666667\n",
      "90%           1.000000\n",
      "95%           1.000000\n",
      "99%           1.000000\n",
      "max           1.000000\n",
      "\n",
      "RapidFuzz char-sim (sample):\n",
      "count    250000.000000\n",
      "mean          0.654550\n",
      "std           0.166090\n",
      "min           0.000000\n",
      "50%           0.666667\n",
      "75%           0.782609\n",
      "90%           0.875000\n",
      "95%           0.923077\n",
      "99%           0.965517\n",
      "max           0.985507\n",
      "\n",
      "Jaccard histogram:\n",
      "[0.00,0.05): 44,308\n",
      "[0.05,0.10): 0\n",
      "[0.10,0.15): 491\n",
      "[0.15,0.20): 1,699\n",
      "[0.20,0.30): 26,013\n",
      "[0.30,0.40): 52,268\n",
      "[0.40,0.50): 5,634\n",
      "[0.50,1.01): 119,587\n",
      "\n",
      "Shared-word histogram:\n",
      "[0.00,1.00): 44,308\n",
      "[1.00,2.00): 123,735\n",
      "[2.00,3.00): 69,257\n",
      "[3.00,4.00): 11,796\n",
      "[4.00,5.00): 858\n",
      "[5.00,10.00): 46\n",
      "[10.00,1000.00): 0\n",
      "\n",
      "Low-char but high-sim examples (rf<0.5 & cos>=0.8) (up to 30):\n",
      "                 token_a                  token_b  cosine_sim  _shared    _jacc      _rf\n",
      "                2 triads       triad relationship    0.837414        0 0.000000 0.461538\n",
      "        i can t even dnf      dnf just not for me    0.861655        1 0.500000 0.400000\n",
      "     cocky charming hero               hero cocky    0.879902        2 0.666667 0.344828\n",
      "favs read at least twice read more than once favs    0.920150        2 0.666667 0.416667\n",
      "  multiple author series     series single author    0.862706        2 0.500000 0.476190\n",
      "                sex anal            anal play sex    0.870417        2 0.666667 0.380952\n",
      "   thriller and suspense    suspense or thrillers    0.942245        1 0.333333 0.428571\n",
      "            immoral hero  hero adulterous immoral    0.814330        2 0.666667 0.400000\n",
      "            ku book 2017        favorite ku books    0.841887        1 0.250000 0.482759\n",
      "         moning karen 5s             karen moning    0.817918        2 1.000000 0.444444\n",
      "             other lgbtq               lgbt other    0.919678        0 0.000000 0.476190\n",
      "         release in 2013         dec 2013 release    0.917862        1 0.500000 0.451613\n",
      "           possibly next               next maybe    0.912706        0 0.000000 0.347826\n",
      "                both pov           different povs    0.821236        0 0.000000 0.454545\n",
      "     lovers from friends       friends and lovers    0.892781        2 1.000000 0.378378\n",
      " firsts books new series   new series first books    0.908793        3 0.750000 0.488889\n",
      " awaiting next in series          series awaiting    0.877392        2 1.000000 0.421053\n",
      "               bdsm 2014     2014 bdsm april june    0.815368        1 0.333333 0.344828\n",
      "           hero criminal criminal hero or heroine    0.822009        2 0.666667 0.432432\n",
      "                    edgy          not edgy enough    0.841875        1 1.000000 0.421053\n",
      " 2013 tbr pile challenge    challenge 17 tbr pile    0.922234        3 1.000000 0.454545\n",
      "        review on amazon            amazon review    0.960240        2 1.000000 0.413793\n",
      "       hero effing crazy       crazy as shit hero    0.839207        2 0.500000 0.285714\n",
      " wish list mean priority        priority wishlist    0.913671        1 0.200000 0.400000\n",
      "           cherry popped          pop that cherry    0.842519        1 0.333333 0.428571\n",
      "     romance with menage       menage romance mmm    0.891203        2 0.666667 0.486486\n",
      "        amazon or kindle          kindle or ebook    0.870460        1 0.333333 0.387097\n",
      "   series ously romantic  romantic mystery series    0.830078        2 0.500000 0.363636\n",
      "        book 3 in series           series 3 books    0.937464        1 0.333333 0.400000\n",
      "1 on kindle not yet read      0 to read on kindle    0.858665        2 1.000000 0.465116\n",
      "========================================================================================================================\n",
      "üîé WHAT-IF EDGE RETENTION (SAMPLE)\n",
      "========================================================================================================================\n",
      "Total sample: 250,000\n",
      "- A_strict: keep 237,008 (94.8%)\n",
      "- B_jacc: keep 203,260 (81.3%)\n",
      "- C_tight: keep 217,357 (86.9%)\n",
      "========================================================================================================================\n",
      "üîé BRIDGING TOKEN CANDIDATES\n",
      "========================================================================================================================\n",
      "Scanned 121,005 candidates in 7.78s\n",
      "\n",
      "Top 30 bridge-like tokens:\n",
      "                             token  degree  neighbor_word_uniq  entropy  top5_word_hits\n",
      "         songbird by melissa pearl      16                  29 3.367296              10\n",
      "     ostavljeno na pola za kasnije      16                  24 3.163424              12\n",
      "love unexpected by delaney diamond      14                  24 3.131334              14\n",
      "                    skal ikke lses      20                  25 3.102811              24\n",
      "     leuk vervolg om te gaan lezen      14                  24 3.029649              28\n",
      "           moja polica za proitati      18                  22 3.024696              18\n",
      "                 kirja joka maasta      20                  21 3.004767              16\n",
      "        bound hearts by lora leigh      14                  21 2.997069              18\n",
      "                  by michelle reid      20                  20 2.995732              10\n",
      "               miejsce akcji zamek      14                  20 2.995732              10\n",
      "     polisiye macera gerilim korku      18                  20 2.954673              16\n",
      "             beli ulang baca ulang      18                  21 2.941617              18\n",
      "         la protagonista es odiosa      20                  21 2.941617              18\n",
      "                  udgivet ikke lst      20                  24 2.928516              32\n",
      "               by charlotte hughes      20                  19 2.926418              12\n",
      "          gekocht nog niet gelezen      20                  23 2.918680              30\n",
      "               by lauren gallagher      18                  19 2.912494              14\n",
      "                  bker jeg kan lne      20                  21 2.907329              28\n",
      "                   tiempos pasados      18                  20 2.903401              18\n",
      "                   by diana palmer      20                  19 2.902002              16\n",
      " expecting to fly by cathy hopkins      10                  18 2.890372              10\n",
      "                  song bird novels      20                  20 2.884863              22\n",
      "                      japoszczyzna      20                  18 2.857103              14\n",
      "           books by cassie edwards      20                  22 2.854732              26\n",
      "                    ska inte hitad      12                  18 2.846480              16\n",
      "                      har skumlest      16                  21 2.846324              22\n",
      "                   takanaga hinako      18                  17 2.833213              10\n",
      "                   hinako takanaga      16                  17 2.833213              10\n",
      "              fate by erika dunham      10                  17 2.833213              10\n",
      "             bcker jag har att lsa      16                  19 2.831544              24\n",
      "========================================================================================================================\n",
      "üîé TOKEN SHAPE / DIGIT OFFENDERS\n",
      "========================================================================================================================\n",
      "Tokens with digit_ratio>=0.50: 2,737 (show 30)\n",
      "     token  digit_ratio      shape\n",
      "      4564          1.0       DDDD\n",
      "       123          1.0        DDD\n",
      "    160324          1.0     DDDDDD\n",
      " 103036323          1.0  DDDDDDDDD\n",
      "  00000000          1.0   DDDDDDDD\n",
      "      9410          1.0       DDDD\n",
      "    011516          1.0     DDDDDD\n",
      "      7828          1.0       DDDD\n",
      "      1912          1.0       DDDD\n",
      "    201707          1.0     DDDDDD\n",
      "       100          1.0        DDD\n",
      "      1915          1.0       DDDD\n",
      "      2010          1.0       DDDD\n",
      "      2107          1.0       DDDD\n",
      "      1910          1.0       DDDD\n",
      "     24690          1.0      DDDDD\n",
      "      9089          1.0       DDDD\n",
      "0060519711          1.0 DDDDDDDDDD\n",
      "      1920          1.0       DDDD\n",
      "     02010          1.0      DDDDD\n",
      "      2002          1.0       DDDD\n",
      "     02016          1.0      DDDDD\n",
      "       004          1.0        DDD\n",
      "     54545          1.0      DDDDD\n",
      "      9281          1.0       DDDD\n",
      "      1011          1.0       DDDD\n",
      "     22015          1.0      DDDDD\n",
      "      9216          1.0       DDDD\n",
      "   2016666          1.0    DDDDDDD\n",
      "   0000000          1.0    DDDDDDD\n",
      "\n",
      "Top 20 shapes:\n",
      "shape\n",
      "AAAAAA             2329\n",
      "AAAAAAA            2287\n",
      "AAAAAAAA           2168\n",
      "AAAAA              2031\n",
      "AAAAA_AAAAA        1958\n",
      "AAAAAAAAA          1801\n",
      "AAAA               1799\n",
      "AAAAA_AAAAAA       1716\n",
      "AAAAAA_AAAAA       1695\n",
      "AAAA_AAAAA         1676\n",
      "AAAAA_AAAAAAA      1605\n",
      "AAAAAA_AAAAAAA     1580\n",
      "AAAAA_AAAA         1501\n",
      "AAAA_AAAAAA        1501\n",
      "AAAA_AAAAAAA       1477\n",
      "AAAAAA_AAAAAA      1476\n",
      "AAAAAAA_AAAAA      1473\n",
      "AAAA_AAAA          1461\n",
      "AAAAAAAAAA         1423\n",
      "AAAAAAA_AAAAAAA    1410\n",
      "========================================================================================================================\n",
      "üîé ACTIONABLE NEXT-STEP SUGGESTIONS (DERIVED)\n",
      "========================================================================================================================\n",
      "- ~17% of sampled edges have Jaccard<0.10; ~18% <0.20; ~32% have ‚â•2 shared words.\n",
      "- If many edges rely on zero/one shared word, require ‚â•2 shared words for sim<0.60.\n",
      "- If 'A_strict' retains ‚â§~60% of sample but clusters remain coherent, adopt it. Otherwise try 'C_tight'.\n",
      "- Add digit/shape guard: drop tokens with digit_ratio‚â•0.5 unless paired with ‚â•2 shared words.\n",
      "- Build a small stoplist from top bridge tokens (above) if they are meta/noise tags.\n",
      "- Re-run filter with: BASE_SIM_STRONG in [0.65,0.70], MIN_SHARED_WORDS in [2,3], plus digit/shape gates.\n"
     ]
    }
   ],
   "source": [
    "# romance-novel-nlp-research/src/eda_analysis/cell3e_explore_refined_outputs.py\n",
    "# Heavy-print audit of refined clustering to pick the next feature-engineering gates.\n",
    "\n",
    "from __future__ import annotations\n",
    "import sys, re, json, math, time\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Set, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------- Paths ----------------------------------\n",
    "OUT = Path(\"romance-novel-nlp-research/src/eda_analysis/outputs\")\n",
    "MAP_F  = OUT / \"clusters_token_map.refined.parquet\"         # from cell3d\n",
    "SUM_F  = OUT / \"clusters_summary.refined.parquet\"           # from cell3d\n",
    "EDGES_F = OUT / \"candidate_similarity_pairs.cluster0_refined.parquet\"  # giant-cluster edges\n",
    "\n",
    "# -------------------------------- Printing ---------------------------------\n",
    "def p(x: str) -> None:\n",
    "    print(x); sys.stdout.flush()\n",
    "\n",
    "def head(title: str) -> None:\n",
    "    sep = \"=\" * 120\n",
    "    p(sep); p(f\"üîé {title}\"); p(sep)\n",
    "\n",
    "# ------------------------------ Stopwords utils ----------------------------\n",
    "# Why: content-words reduce semantic chaining. Keep portable; no downloads required.\n",
    "def load_stopwords() -> Set[str]:\n",
    "    try:\n",
    "        from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "        sw = set(w.lower() for w in ENGLISH_STOP_WORDS)\n",
    "        p(f\"‚úÖ stopwords: sklearn ({len(sw)})\")\n",
    "        return sw\n",
    "    except Exception:\n",
    "        base = {\"a\",\"an\",\"the\",\"and\",\"or\",\"to\",\"of\",\"in\",\"on\",\"for\",\"with\",\"by\",\"as\",\"from\",\n",
    "                \"this\",\"that\",\"these\",\"those\",\"it\",\"its\",\"is\",\"are\",\"be\",\"was\",\"were\",\"been\",\n",
    "                \"i\",\"you\",\"we\",\"he\",\"she\",\"they\",\"me\",\"him\",\"her\",\"them\",\"my\",\"your\",\"our\",\"their\",\n",
    "                \"not\",\"no\",\"yes\",\"into\",\"over\",\"under\",\"up\",\"down\",\"out\",\"more\",\"most\",\"less\",\"very\"}\n",
    "        p(f\"‚ö†Ô∏è stopwords: fallback ({len(base)})\")\n",
    "        return base\n",
    "\n",
    "TOKEN_RE = re.compile(r\"[a-z]{2,}\")  # why: stable content tokens\n",
    "\n",
    "def content_words(s: str, sw: Set[str]) -> Set[str]:\n",
    "    return {w for w in TOKEN_RE.findall(s.lower()) if w not in sw}\n",
    "\n",
    "def digit_ratio(s: str) -> float:\n",
    "    if not s: return 0.0\n",
    "    digits = sum(c.isdigit() for c in s)\n",
    "    return digits / len(s)\n",
    "\n",
    "def token_shape(s: str) -> str:\n",
    "    # why: spot numeric/symbolic pseudo-tags\n",
    "    out = []\n",
    "    for c in s:\n",
    "        if c.isalpha(): out.append('A')\n",
    "        elif c.isdigit(): out.append('D')\n",
    "        elif c.isspace(): out.append('_')\n",
    "        else: out.append('#')\n",
    "    return ''.join(out)\n",
    "\n",
    "# ----------------------------- Optional RapidFuzz ---------------------------\n",
    "try:\n",
    "    from rapidfuzz.fuzz import ratio as fuzz_ratio\n",
    "    HAS_RF = True\n",
    "    p(\"‚úÖ RapidFuzz available\")\n",
    "except Exception:\n",
    "    HAS_RF = False\n",
    "    p(\"‚ÑπÔ∏è RapidFuzz not available (skip char-sim prints)\")\n",
    "\n",
    "# --------------------------------- Load ------------------------------------\n",
    "head(\"INPUTS\")\n",
    "for f in [MAP_F, SUM_F, EDGES_F]:\n",
    "    p(f\"{f} | exists={f.exists()} | size={(f.stat().st_size/1024/1024):.2f} MB\" if f.exists() else f\"{f} | MISSING\")\n",
    "if not (MAP_F.exists() and SUM_F.exists() and EDGES_F.exists()):\n",
    "    raise FileNotFoundError(\"Missing refined artifacts. Run cell3d_refine_giant_cluster.py first.\")\n",
    "\n",
    "map_df = pd.read_parquet(MAP_F)             # token, cluster_id, ...\n",
    "sum_df = pd.read_parquet(SUM_F)             # cluster summary\n",
    "edges = pd.read_parquet(EDGES_F)            # token_a, token_b, cosine_sim, rank\n",
    "\n",
    "# Identify giant cluster id base\n",
    "giant_row = sum_df.sort_values([\"size\",\"medoid_wdeg\"], ascending=[False, False]).iloc[0]\n",
    "giant_cid = int(giant_row[\"cluster_id\"])\n",
    "p(f\"\\nGiant cluster id (refined namespace): {giant_cid} | size={int(giant_row['size']):,}\")\n",
    "\n",
    "giant_tokens = set(map_df.loc[map_df[\"cluster_id\"] == giant_cid, \"token\"].tolist())\n",
    "p(f\"Giant tokens mapped: {len(giant_tokens):,}\")\n",
    "p(f\"Giant edges (refined file rows): {len(edges):,}\")\n",
    "\n",
    "# --------------------------------- Stats 1 ---------------------------------\n",
    "head(\"EDGE & TOKEN STATS (GIANT CLUSTER)\")\n",
    "p(edges[\"cosine_sim\"].describe(percentiles=[.5,.75,.9,.95,.99]).to_string())\n",
    "\n",
    "# token shapes / digit share\n",
    "toks = list(giant_tokens)\n",
    "dr = np.array([digit_ratio(t) for t in toks], dtype=np.float32)\n",
    "p(f\"\\nDigit ratio (tokens): mean={dr.mean():.3f} | p90={np.quantile(dr,0.90):.2f} | p99={np.quantile(dr,0.99):.2f}\")\n",
    "shape_counts = Counter(token_shape(t)[:12] for t in toks)  # prefix to compact\n",
    "p(\"Top token shape prefixes:\")\n",
    "for shp, cnt in shape_counts.most_common(20):\n",
    "    p(f\"- {shp!r}: {cnt:,}\")\n",
    "\n",
    "# --------------------------------- Stats 2 ---------------------------------\n",
    "head(\"CONTENT WORDS + OVERLAP STATS\")\n",
    "SW = load_stopwords()\n",
    "# Precompute cw for all tokens appearing in edges (reduce memory)\n",
    "nodes_in_edges = set(edges[\"token_a\"]).union(set(edges[\"token_b\"]))\n",
    "cw: Dict[str, Set[str]] = {}\n",
    "for s in nodes_in_edges:\n",
    "    cw[s] = content_words(s, SW)\n",
    "\n",
    "# Compute overlap, Jaccard, and (optional) RapidFuzz on a large sample\n",
    "sample_n = min(250_000, len(edges))\n",
    "sample = edges.sample(n=sample_n, random_state=7).copy()\n",
    "sample[\"_cw_a\"] = [cw[a] for a in sample[\"token_a\"]]\n",
    "sample[\"_cw_b\"] = [cw[b] for b in sample[\"token_b\"]]\n",
    "sample[\"_shared\"] = [len(a & b) for a,b in zip(sample[\"_cw_a\"], sample[\"_cw_b\"])]\n",
    "sample[\"_union\"]  = [max(1, len(a | b)) for a,b in zip(sample[\"_cw_a\"], sample[\"_cw_b\"])]\n",
    "sample[\"_jacc\"]   = sample[\"_shared\"] / sample[\"_union\"]\n",
    "if HAS_RF:\n",
    "    sample[\"_rf\"] = [fuzz_ratio(a,b)/100.0 for a,b in zip(sample[\"token_a\"], sample[\"token_b\"])]\n",
    "\n",
    "p(f\"Sampled edges: {sample_n:,}\")\n",
    "p(\"\\nShared-word counts (sample):\")\n",
    "p(sample[\"_shared\"].describe(percentiles=[.5,.75,.9,.95,.99]).to_string())\n",
    "p(\"\\nJaccard (cw) (sample):\")\n",
    "p(sample[\"_jacc\"].describe(percentiles=[.5,.75,.9,.95,.99]).to_string())\n",
    "if HAS_RF:\n",
    "    p(\"\\nRapidFuzz char-sim (sample):\")\n",
    "    p(sample[\"_rf\"].describe(percentiles=[.5,.75,.9,.95,.99]).to_string())\n",
    "\n",
    "# Buckets\n",
    "def hist(series: pd.Series, bins: List[float]) -> List[Tuple[str,int]]:\n",
    "    arr = series.to_numpy(np.float32, copy=False)\n",
    "    out = []\n",
    "    for i in range(len(bins)-1):\n",
    "        lo, hi = bins[i], bins[i+1]\n",
    "        cnt = int(((arr >= lo) & (arr < hi)).sum())\n",
    "        out.append((f\"[{lo:.2f},{hi:.2f})\", cnt))\n",
    "    return out\n",
    "\n",
    "p(\"\\nJaccard histogram:\")\n",
    "for rng, cnt in hist(sample[\"_jacc\"], [0, .05, .10, .15, .20, .30, .40, .50, 1.01]):\n",
    "    p(f\"{rng}: {cnt:,}\")\n",
    "\n",
    "p(\"\\nShared-word histogram:\")\n",
    "for rng, cnt in hist(sample[\"_shared\"].astype(np.float32), [0,1,2,3,4,5,10,1000]):\n",
    "    p(f\"{rng}: {cnt:,}\")\n",
    "\n",
    "if HAS_RF:\n",
    "    p(\"\\nLow-char but high-sim examples (rf<0.5 & cos>=0.8) (up to 30):\")\n",
    "    zz = sample[(sample[\"_rf\"]<0.5) & (sample[\"cosine_sim\"]>=0.8)].head(30)\n",
    "    p(zz[[\"token_a\",\"token_b\",\"cosine_sim\",\"_shared\",\"_jacc\",\"_rf\"]].to_string(index=False))\n",
    "\n",
    "# -------------------------------- What-if gates ----------------------------\n",
    "head(\"WHAT-IF EDGE RETENTION (SAMPLE)\")\n",
    "def keep_edge(sim: float, shared: int, jacc: float) -> Dict[str,bool]:\n",
    "    return {\n",
    "        \"A_strict\": (sim >= 0.65) or (sim >= 0.55 and shared >= 2) or (sim >= 0.50 and shared >= 3),\n",
    "        \"B_jacc\":   (jacc >= 0.20) and (sim >= 0.55),\n",
    "        \"C_tight\":  (sim >= 0.70) or (sim >= 0.60 and shared >= 2)\n",
    "    }\n",
    "\n",
    "ret = {\"A_strict\":0, \"B_jacc\":0, \"C_tight\":0}\n",
    "for sim, sh, j in zip(sample[\"cosine_sim\"], sample[\"_shared\"], sample[\"_jacc\"]):\n",
    "    d = keep_edge(float(sim), int(sh), float(j))\n",
    "    for k in ret: ret[k] += int(d[k])\n",
    "\n",
    "p(f\"Total sample: {sample_n:,}\")\n",
    "for k,v in ret.items():\n",
    "    p(f\"- {k}: keep {v:,} ({v/sample_n*100:.1f}%)\")\n",
    "\n",
    "# -------------------------------- Bridges ----------------------------------\n",
    "head(\"BRIDGING TOKEN CANDIDATES\")\n",
    "# Why: identify nodes that connect many disparate word themes (likely generic/noisy).\n",
    "adj = defaultdict(list)\n",
    "for a,b,sim in zip(edges[\"token_a\"], edges[\"token_b\"], edges[\"cosine_sim\"]):\n",
    "    adj[a].append(b); adj[b].append(a)\n",
    "\n",
    "def entropy(counts: Counter) -> float:\n",
    "    total = sum(counts.values()) or 1\n",
    "    H = 0.0\n",
    "    for c in counts.values():\n",
    "        p = c/total\n",
    "        H -= p*math.log(p+1e-12)\n",
    "    return H\n",
    "\n",
    "bridge_rows = []\n",
    "t0 = time.time()\n",
    "for t, neighs in adj.items():\n",
    "    if len(neighs) < 10:  # focus on sufficiently connected\n",
    "        continue\n",
    "    wc = Counter()\n",
    "    for n in neighs[:200]:  # cap for speed\n",
    "        wc.update(cw.get(n) or content_words(n, SW))\n",
    "    H = entropy(wc)\n",
    "    uniq = len(wc)\n",
    "    topk = sum(c for _, c in wc.most_common(5))\n",
    "    bridge_rows.append((t, len(neighs), uniq, H, topk))\n",
    "p(f\"Scanned {len(bridge_rows):,} candidates in {time.time()-t0:.2f}s\")\n",
    "\n",
    "bridge_df = pd.DataFrame(bridge_rows, columns=[\"token\",\"degree\",\"neighbor_word_uniq\",\"entropy\",\"top5_word_hits\"])\\\n",
    "             .sort_values([\"entropy\",\"neighbor_word_uniq\",\"degree\"], ascending=[False, False, False])\n",
    "p(\"\\nTop 30 bridge-like tokens:\")\n",
    "p(bridge_df.head(30).to_string(index=False))\n",
    "\n",
    "# --------------------------- Offender token shapes -------------------------\n",
    "head(\"TOKEN SHAPE / DIGIT OFFENDERS\")\n",
    "tok_stats = []\n",
    "for t in list(giant_tokens)[:300000]:\n",
    "    dr = digit_ratio(t)\n",
    "    shp = token_shape(t)\n",
    "    tok_stats.append((t, dr, shp))\n",
    "tok_df = pd.DataFrame(tok_stats, columns=[\"token\",\"digit_ratio\",\"shape\"])\n",
    "off = tok_df[tok_df[\"digit_ratio\"] >= 0.50].sort_values(\"digit_ratio\", ascending=False)\n",
    "p(f\"Tokens with digit_ratio>=0.50: {len(off):,} (show 30)\")\n",
    "p(off.head(30).to_string(index=False))\n",
    "\n",
    "shape_top = tok_df[\"shape\"].value_counts().head(20)\n",
    "p(\"\\nTop 20 shapes:\")\n",
    "p(shape_top.to_string())\n",
    "\n",
    "# --------------------------------- Guidance --------------------------------\n",
    "head(\"ACTIONABLE NEXT-STEP SUGGESTIONS (DERIVED)\")\n",
    "# Numbers for quick decision\n",
    "j_low = int((sample[\"_jacc\"] < 0.10).mean()*100)\n",
    "j_mid = int((sample[\"_jacc\"] < 0.20).mean()*100)\n",
    "sw_ge2 = int((sample[\"_shared\"] >= 2).mean()*100)\n",
    "p(f\"- ~{j_low}% of sampled edges have Jaccard<0.10; ~{j_mid}% <0.20; ~{sw_ge2}% have ‚â•2 shared words.\")\n",
    "p(\"- If many edges rely on zero/one shared word, require ‚â•2 shared words for sim<0.60.\")\n",
    "p(\"- If 'A_strict' retains ‚â§~60% of sample but clusters remain coherent, adopt it. Otherwise try 'C_tight'.\")\n",
    "p(\"- Add digit/shape guard: drop tokens with digit_ratio‚â•0.5 unless paired with ‚â•2 shared words.\")\n",
    "p(\"- Build a small stoplist from top bridge tokens (above) if they are meta/noise tags.\")\n",
    "p(\"- Re-run filter with: BASE_SIM_STRONG in [0.65,0.70], MIN_SHARED_WORDS in [2,3], plus digit/shape gates.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîé SOURCES  (refined)\n",
      "============================================================\n",
      "token map : romance-novel-nlp-research/src/eda_analysis/outputs/clusters_token_map.refined.parquet | exists=True\n",
      "summary   : romance-novel-nlp-research/src/eda_analysis/outputs/clusters_summary.refined.parquet | exists=True\n",
      "\n",
      "Rows: token_map=232,918 | clusters=7,019\n",
      "\n",
      "============================================================\n",
      "üíæ SAVING CANONICAL MAP\n",
      "============================================================\n",
      "‚úÖ Parquet ‚Üí romance-novel-nlp-research/src/eda_analysis/outputs/token_canonical_map.parquet\n",
      "‚úÖ CSV     ‚Üí romance-novel-nlp-research/src/eda_analysis/outputs/token_canonical_map.csv\n",
      "\n",
      "============================================================\n",
      "üìà SUMMARY\n",
      "============================================================\n",
      "Tokens mapped      : 232,918\n",
      "Clusters covered   : 4,223\n",
      "Canonical labels   : 7,019\n",
      "Identity mappings  : 7,019 (3.01%)\n",
      "\n",
      "Top 10 canonical labels by cluster size:\n",
      " cluster_id                         medoid   size\n",
      "          0    manga graphic novels comics 213904\n",
      "          1               150 to 200 pages    328\n",
      "          2      meet n greet 2015 dec jan    140\n",
      "          3           erotica bdsm romance     96\n",
      "          4              01 june utc bonus     77\n",
      "          6 2015 read a thon challenge jan     60\n",
      "          5                      1 on deck     60\n",
      "          7           00 feb ult challenge     59\n",
      "          8 pride and prejudice variations     53\n",
      "          9                  wattpad books     51\n",
      "\n",
      "============================================================\n",
      "üîç SAMPLE REWRITES (200)  ‚Äî token ‚Üí canonical_label\n",
      "============================================================\n",
      "                           token              canonical_label  cluster_id  degree   wdegree\n",
      "            1 night stand series  manga graphic novels comics           0       8  6.807698\n",
      "                           1920s  manga graphic novels comics           0      20 16.446217\n",
      "                    2 read again  manga graphic novels comics           0      10  8.818759\n",
      "      2000 2009 publication date  manga graphic novels comics           0       6  5.029310\n",
      "                 2015 book shelf  manga graphic novels comics           0      14 12.385276\n",
      "   2015 dangerous hero challenge  manga graphic novels comics           0      20 18.233218\n",
      "                         6 1800s  manga graphic novels comics           0       6  5.108589\n",
      "             a pleasant surprise  manga graphic novels comics           0      16 12.163233\n",
      "                         a queue  manga graphic novels comics           0      20 16.703262\n",
      "                   a s a p books  manga graphic novels comics           0       8  6.167840\n",
      "                      alex award  manga graphic novels comics           0       6  4.219562\n",
      "                     alexandra i  manga graphic novels comics           0      20 15.730658\n",
      "             alternative reality  manga graphic novels comics           0      20 16.463977\n",
      "                     and up next  manga graphic novels comics           0       4  3.126255\n",
      "                   annika martin  manga graphic novels comics           0      10  7.917597\n",
      "                    annoying h h  manga graphic novels comics           0      20 16.973248\n",
      "                      arc direct  manga graphic novels comics           0       4  3.163494\n",
      "    audiobooks next to listen to  manga graphic novels comics           0       6  5.342594\n",
      "               author bonnie dee  manga graphic novels comics           0       6  4.500257\n",
      "              available in ebook  manga graphic novels comics           0       6  5.384606\n",
      "                  bad ass alphas  manga graphic novels comics           0       4  3.659132\n",
      "             bad boys undercover  manga graphic novels comics           0       6  4.746801\n",
      "                 bcl unavailable  manga graphic novels comics           0       6  4.197561\n",
      "                  beautiful soul  manga graphic novels comics           0       6  4.066855\n",
      "                    best avoided  manga graphic novels comics           0      12  8.669420\n",
      "                      blame game  manga graphic novels comics           0      12  7.360366\n",
      "                books i can loan  manga graphic novels comics           0      16 14.279672\n",
      "          books in local library  manga graphic novels comics           0       6  5.275109\n",
      "                    books to buy  manga graphic novels comics           0      18 16.818147\n",
      "            brutal raw macho man  manga graphic novels comics           0       6  3.864594\n",
      "            cancer mental health  manga graphic novels comics           0       4  2.860900\n",
      "     challenge 2014 rlftci group  manga graphic novels comics           0      16 14.873547\n",
      " challenge 2016 q1 read your tbr  manga graphic novels comics           0      16 15.200959\n",
      "                          cheese  manga graphic novels comics           0      20 15.403502\n",
      "                   chosen for me  manga graphic novels comics           0      16 12.019978\n",
      "                    clair boston  manga graphic novels comics           0      10  6.735434\n",
      "                       classroom  manga graphic novels comics           0      10  8.030941\n",
      "            coming out in august  manga graphic novels comics           0       8  6.718580\n",
      "        could see the sparks fly  manga graphic novels comics           0       6  4.120134\n",
      "                   crazy awesome  manga graphic novels comics           0      10  7.754251\n",
      "         cute adorable character  manga graphic novels comics           0      16 13.090938\n",
      "                          cutesy  manga graphic novels comics           0      12  8.992637\n",
      "                     dare valley  manga graphic novels comics           0      10  7.908662\n",
      "                      day in bed  manga graphic novels comics           0      12  7.907707\n",
      "                     demon blood  manga graphic novels comics           0       4  3.285321\n",
      "           deserves better cover  manga graphic novels comics           0       6  4.258242\n",
      "                        devotion  manga graphic novels comics           0      14 10.811644\n",
      "                 disliked hero s  manga graphic novels comics           0       6  5.033091\n",
      " drama mystery suspense thriller  manga graphic novels comics           0       4  3.810538\n",
      "                      e d walker  manga graphic novels comics           0      12  8.178932\n",
      "            element dark intense  manga graphic novels comics           0       6  4.937184\n",
      "               excellent reading  manga graphic novels comics           0      10  8.735052\n",
      "        family secrets mysteries  manga graphic novels comics           0      14 11.266952\n",
      "                   februari 2016  manga graphic novels comics           0      12 10.299644\n",
      "           firemen smoke jumpers  manga graphic novels comics           0       8  6.539864\n",
      "            for the austen lover  manga graphic novels comics           0       4  3.373310\n",
      "                     fox calista  manga graphic novels comics           0       4  2.540684\n",
      "          fractured lives series  manga graphic novels comics           0       8  5.427677\n",
      "           frank dorothea benton  manga graphic novels comics           0      10  7.968838\n",
      "                   futur release  manga graphic novels comics           0       2  1.445675\n",
      "                   genrethriller  manga graphic novels comics           0       4  2.847240\n",
      "            gilded age 1870 1898  manga graphic novels comics           0      18 13.588912\n",
      "                    gina whitney  manga graphic novels comics           0      16 11.986857\n",
      "      got sample but did not buy  manga graphic novels comics           0      12  9.277211\n",
      "            graphic comic photos  manga graphic novels comics           0       6  5.255855\n",
      "                      gritty lit  manga graphic novels comics           0       6  4.929159\n",
      "           has illness addiction  manga graphic novels comics           0       8  6.766400\n",
      "                havent been read  manga graphic novels comics           0      16 14.444753\n",
      "                    heartstrings  manga graphic novels comics           0      18 14.566180\n",
      "          hero bad past marriage  manga graphic novels comics           0       4  3.407866\n",
      "      hero has aggression issues  manga graphic novels comics           0      14 10.674389\n",
      "                  hero long hair  manga graphic novels comics           0      18 15.516268\n",
      "                     hero tycoon  manga graphic novels comics           0      18 15.056002\n",
      "          heroes that make me go  manga graphic novels comics           0       8  5.679168\n",
      "heroine plain ugly not beautiful  manga graphic novels comics           0       8  6.528772\n",
      "                hodder stoughton  manga graphic novels comics           0      10  7.195935\n",
      "                 holiday newyear  manga graphic novels comics           0       4  3.404157\n",
      "                       hot alfas  manga graphic novels comics           0       8  5.538575\n",
      "                         hp read  manga graphic novels comics           0      16 13.151186\n",
      "                humiliation kink  manga graphic novels comics           0      12  9.930243\n",
      "   i cannot camembear the cheese  manga graphic novels comics           0       6  3.753613\n",
      "              i love wealthy men  manga graphic novels comics           0       6  4.567454\n",
      "               interesting plots  manga graphic novels comics           0      20 15.629577\n",
      "                    issue health  manga graphic novels comics           0      12  9.163804\n",
      "        it could have been great  manga graphic novels comics           0      14 11.417019\n",
      "                   jennifer foor  manga graphic novels comics           0      12  9.543245\n",
      "                    job activist  manga graphic novels comics           0       6  4.200069\n",
      "             just not feeling it  manga graphic novels comics           0      18 13.448513\n",
      "                           keysi  manga graphic novels comics           0       8  5.506844\n",
      "                      kiera kass  manga graphic novels comics           0      14  9.716621\n",
      "            kindle and ibook own  manga graphic novels comics           0      16 13.933298\n",
      "                   kink pet play  manga graphic novels comics           0      14 10.869270\n",
      "                     late 1800 s  manga graphic novels comics           0      16 13.469633\n",
      "                     lawful good  manga graphic novels comics           0       6  4.240469\n",
      "                   life of women  manga graphic novels comics           0      20 15.580945\n",
      "                       lol reads  manga graphic novels comics           0       6  4.446624\n",
      "                lora leigh nauti  manga graphic novels comics           0      20 16.187324\n",
      "                 maggie robinson  manga graphic novels comics           0      20 15.169397\n",
      "              may decide to read  manga graphic novels comics           0       2  1.660159\n",
      "                     maybe 1 day  manga graphic novels comics           0      16 12.845293\n",
      "                           mob c  manga graphic novels comics           0       6  4.658726\n",
      "                     montana sky  manga graphic novels comics           0      16 13.338631\n",
      "                  movie industry  manga graphic novels comics           0       8  6.838699\n",
      "                must buy to read  manga graphic novels comics           0      10  9.039891\n",
      "               my most favorites  manga graphic novels comics           0      18 15.602995\n",
      "      nb genderfluid genderqueer  manga graphic novels comics           0      12  9.281683\n",
      "       neurodivergent characters  manga graphic novels comics           0      10  6.826620\n",
      "                  new books 2012  manga graphic novels comics           0       4  3.599035\n",
      "             new books 2015 june  manga graphic novels comics           0      16 14.630379\n",
      "                        no insta  manga graphic novels comics           0      16 12.508975\n",
      "                      non scries  manga graphic novels comics           0      12  7.696244\n",
      "                 not going there  manga graphic novels comics           0      16 11.565904\n",
      "                   once a series  manga graphic novels comics           0       2  1.581518\n",
      "             out of wedlock baby  manga graphic novels comics           0       6  4.269121\n",
      "                      p g county  manga graphic novels comics           0      12  7.379106\n",
      "            p s i love you death  manga graphic novels comics           0       2  1.443678\n",
      "                    paperback hb  manga graphic novels comics           0      10  8.104328\n",
      "                    pc 1000 1999  manga graphic novels comics           0      16 11.215562\n",
      "                   piece of shit  manga graphic novels comics           0      10  6.861497\n",
      "                  plus size love  manga graphic novels comics           0      18 13.737492\n",
      "police special agents undercover  manga graphic novels comics           0      10  8.674541\n",
      "                       raceberry  manga graphic novels comics           0       8  5.295777\n",
      "                 rachel blaufeld  manga graphic novels comics           0       8  5.831013\n",
      "               read the season o  manga graphic novels comics           0       2  1.445586\n",
      "               read when younger  manga graphic novels comics           0      18 14.995263\n",
      "                   reads 4 stars  manga graphic novels comics           0      20 18.447602\n",
      "               reread eventually  manga graphic novels comics           0      10  8.547175\n",
      "                review in amazon  manga graphic novels comics           0      20 18.559865\n",
      "                      reynolds a  manga graphic novels comics           0      12  9.111856\n",
      "                      romancegay  manga graphic novels comics           0      16 13.554451\n",
      "      royality nobilty character  manga graphic novels comics           0       2  1.461940\n",
      "              royals and romance  manga graphic novels comics           0       2  1.522741\n",
      "                        s gender  manga graphic novels comics           0      10  7.652009\n",
      "                seasons holidays  manga graphic novels comics           0      14 12.436667\n",
      "sequels that i cant wait to read  manga graphic novels comics           0      18 16.523011\n",
      "                    series scars  manga graphic novels comics           0       4  3.045199\n",
      "                   set in alaska  manga graphic novels comics           0       6  4.665232\n",
      "                  shifters lions  manga graphic novels comics           0      14 11.781158\n",
      "                       shonen ai  manga graphic novels comics           0       6  4.912851\n",
      "        short stories n novellas  manga graphic novels comics           0       6  5.683338\n",
      "        shouldn t forget to read  manga graphic novels comics           0       8  6.202925\n",
      "                  smutty romance  manga graphic novels comics           0      18 16.182968\n",
      "       stand wait trilogy series  manga graphic novels comics           0       4  3.290654\n",
      "  started to read never finished  manga graphic novels comics           0       8  7.210571\n",
      "  steam some like it kinda kinky  manga graphic novels comics           0       2  1.575457\n",
      "                      strohmeyer  manga graphic novels comics           0       8  5.056216\n",
      "                 tbr in december  manga graphic novels comics           0      16 13.862175\n",
      "                      teen abuse  manga graphic novels comics           0       4  2.997650\n",
      "                      teen lover  manga graphic novels comics           0       8  6.257568\n",
      "                       territory  manga graphic novels comics           0      10  6.396388\n",
      "                  the cover lies  manga graphic novels comics           0      12  9.514563\n",
      "           the never ending list  manga graphic novels comics           0       4  3.068429\n",
      "                the subcontinent  manga graphic novels comics           0       4  2.953816\n",
      "theme abuse cruelty violence now  manga graphic novels comics           0       8  6.264391\n",
      "  theyre tattoos not ink or tats  manga graphic novels comics           0       4  3.130291\n",
      "                 titlefood drink  manga graphic novels comics           0       4  2.371216\n",
      "          to infinity and beyond  manga graphic novels comics           0       8  6.100837\n",
      "              to read f auth new  manga graphic novels comics           0      14  8.824498\n",
      "           to read maybe someday  manga graphic novels comics           0      16 14.180920\n",
      "            too stupid to finish  manga graphic novels comics           0       8  6.526606\n",
      "                    tried failed  manga graphic novels comics           0      18 14.890753\n",
      "                      ugh boring  manga graphic novels comics           0       8  6.649838\n",
      "            uitgekomen nog kopen  manga graphic novels comics           0      10  6.659287\n",
      "     ultimate challenge jun 2017  manga graphic novels comics           0      12 11.037277\n",
      "             unread want to read  manga graphic novels comics           0      12 10.140533\n",
      "                     upper class  manga graphic novels comics           0      14 10.254432\n",
      "              urban epic fantasy  manga graphic novels comics           0       2  1.770965\n",
      "                 war battle book  manga graphic novels comics           0      10  8.147501\n",
      "            wine trash book club  manga graphic novels comics           0       4  3.051628\n",
      "                        with you  manga graphic novels comics           0      10  7.024515\n",
      "                wizards and such  manga graphic novels comics           0       8  6.596093\n",
      "                  wounded heroes  manga graphic novels comics           0      20 16.806826\n",
      "                        xcurrent  manga graphic novels comics           0      14  9.065189\n",
      "                        ya ya ya  manga graphic novels comics           0      14 10.770293\n",
      "                        z2ckfilz  manga graphic novels comics           0       6  3.900480\n",
      "           zamour wild about you  manga graphic novels comics           0       2  1.347700\n",
      "                 zcase a shelf 2  manga graphic novels comics           0      10  8.922655\n",
      "                bdsm and erotica                      0 onmc1           3      16 14.962798\n",
      "                    mm kink bdsm                      0 onmc1           3      16 14.850443\n",
      "             mills and boon plus                    0 chimica          17      20 17.976845\n",
      "              2016 rita finalist          2014 rita finalists          19      12 11.275105\n",
      "            plagiarism sucks ass                   plagiarism          50      14 10.482326\n",
      "          z student teacher boss                       intern         121       2  1.510756\n",
      "               gay romance 20 99     gay romance 20 199 pages         148       4  3.168185\n",
      "               give away entered           nope also nopenope         434       6  5.345608\n",
      "               y photojournalist    the club by ren monterrey         505       6  4.416921\n",
      "                2b releazed 201x are they ever going to do it         528       6  3.519776\n",
      "                     sports golf                 alpha or dom         574       6  5.071951\n",
      "                book club of two                nous possdons         614       6  5.545221\n",
      "            older title wishlist                    beware of         751       6  5.289763\n",
      "          bewitching book covers         bewitching book tour        1023       2  1.478112\n",
      "                must first short                   must short        1221       2  1.728421\n",
      "                pinpoint editing               editing credit        1318       2  1.032610\n",
      "                      overall no                  overall meh        1464       2  1.424563\n",
      "     type short stories novellas                          fun        1555       4  3.887120\n",
      "                 all2013 release                jaine diamond        1937       2  1.574647\n",
      "         historical tapestry2013               just for kicks        1961       2  1.070673\n",
      "                      my perf ya      books i read in the 10s        2233       2  1.114101\n",
      "                     zoet sappig          ctrope wealthy hero        2525       4  2.377316\n",
      "   talkative boyish sunny heroes                   sunny hero        3966       2  1.399652\n"
     ]
    }
   ],
   "source": [
    "# romance-novel-nlp-research/src/eda_analysis/cell3f_export_canonical_map.py\n",
    "from __future__ import annotations\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "OUT = Path(\"romance-novel-nlp-research/src/eda_analysis/outputs\")\n",
    "# Prefer refined artifacts\n",
    "MAP_REFINED  = OUT / \"clusters_token_map.refined.parquet\"\n",
    "SUM_REFINED  = OUT / \"clusters_summary.refined.parquet\"\n",
    "# Fallback (pre-refine)\n",
    "MAP_BASE     = OUT / \"clusters_token_map.parquet\"\n",
    "SUM_BASE     = OUT / \"clusters_summary.parquet\"\n",
    "\n",
    "# Outputs\n",
    "CANON_PARQUET = OUT / \"token_canonical_map.parquet\"\n",
    "CANON_CSV     = OUT / \"token_canonical_map.csv\"\n",
    "\n",
    "def p(x: str) -> None:\n",
    "    print(x); sys.stdout.flush()\n",
    "\n",
    "def pick_sources() -> tuple[Path, Path, str]:\n",
    "    \"\"\"Why: use refined if present; else fallback and warn.\"\"\"\n",
    "    if MAP_REFINED.exists() and SUM_REFINED.exists():\n",
    "        return MAP_REFINED, SUM_REFINED, \"refined\"\n",
    "    if MAP_BASE.exists() and SUM_BASE.exists():\n",
    "        p(\"‚ö†Ô∏è Refined artifacts not found. Falling back to base clustering outputs.\")\n",
    "        return MAP_BASE, SUM_BASE, \"base\"\n",
    "    raise FileNotFoundError(\"Neither refined nor base cluster artifacts are available.\")\n",
    "\n",
    "def compute_fallback_medoids(map_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Why: ensure medoid for all clusters; choose token with max (degree, wdegree).\"\"\"\n",
    "    need_cols = {\"cluster_id\",\"token\"}\n",
    "    if \"degree\" in map_df.columns: need_cols.add(\"degree\")\n",
    "    else: map_df[\"degree\"] = 0\n",
    "    if \"wdegree\" in map_df.columns: need_cols.add(\"wdegree\")\n",
    "    else: map_df[\"wdegree\"] = 0.0\n",
    "    g = map_df[list(need_cols)].copy()\n",
    "    # rank: higher degree, then higher wdegree\n",
    "    g[\"_rk\"] = g.groupby(\"cluster_id\").apply(\n",
    "        lambda d: (-d[\"degree\"].to_numpy(), -d[\"wdegree\"].to_numpy())\n",
    "    ).reset_index(level=0, drop=True)\n",
    "    # Pandas can't sort by tuple directly across groups; do argsort per group\n",
    "    def top1(df: pd.DataFrame) -> pd.Series:\n",
    "        idx = np.lexsort(( -df[\"wdegree\"].to_numpy(), -df[\"degree\"].to_numpy() ))\n",
    "        # np.lexsort sorts by last key first; we negated to make it descending.\n",
    "        # Take first index of sorted order\n",
    "        return df.iloc[idx[0]][[\"cluster_id\",\"token\",\"degree\",\"wdegree\"]]\n",
    "    top = g.groupby(\"cluster_id\", sort=False).apply(top1).reset_index(drop=True)\n",
    "    top.rename(columns={\"token\":\"medoid\"}, inplace=True)\n",
    "    return top[[\"cluster_id\",\"medoid\"]]\n",
    "\n",
    "def main() -> None:\n",
    "    map_p, sum_p, mode = pick_sources()\n",
    "    p(\"============================================================\")\n",
    "    p(f\"üîé SOURCES  ({mode})\")\n",
    "    p(\"============================================================\")\n",
    "    p(f\"token map : {map_p} | exists={map_p.exists()}\")\n",
    "    p(f\"summary   : {sum_p} | exists={sum_p.exists()}\")\n",
    "\n",
    "    map_df = pd.read_parquet(map_p)  # expected: token, cluster_id, (degree,wdegree,flags‚Ä¶)\n",
    "    sum_df = pd.read_parquet(sum_p)  # expected: cluster_id, size, medoid, ‚Ä¶\n",
    "\n",
    "    p(f\"\\nRows: token_map={len(map_df):,} | clusters={len(sum_df):,}\")\n",
    "    missing_medoid = sum_df[\"medoid\"].isna().sum() if \"medoid\" in sum_df.columns else len(sum_df)\n",
    "    if missing_medoid:\n",
    "        p(f\"‚ö†Ô∏è {missing_medoid:,} clusters missing medoid in summary ‚Üí computing fallbacks.\")\n",
    "        medoids_fallback = compute_fallback_medoids(map_df)\n",
    "        sum_df = sum_df.merge(medoids_fallback, on=\"cluster_id\", how=\"left\", suffixes=(\"\",\"_fallback\"))\n",
    "        sum_df[\"medoid\"] = sum_df[\"medoid\"].fillna(sum_df[\"medoid_fallback\"])\n",
    "        sum_df.drop(columns=[c for c in sum_df.columns if c.endswith(\"_fallback\")], inplace=True)\n",
    "\n",
    "    medoids = sum_df[[\"cluster_id\",\"medoid\"]].dropna().copy()\n",
    "    medoids[\"cluster_id\"] = medoids[\"cluster_id\"].astype(\"int64\")\n",
    "\n",
    "    # Build canonical map\n",
    "    keep_cols = [\"token\",\"cluster_id\"]\n",
    "    extras = [c for c in [\"degree\",\"wdegree\",\"is_short\",\"has_digit\",\"starts_zz\"] if c in map_df.columns]\n",
    "    df = map_df[keep_cols + extras].copy()\n",
    "    df[\"cluster_id\"] = df[\"cluster_id\"].astype(\"int64\")\n",
    "    df = df.merge(medoids, on=\"cluster_id\", how=\"left\")\n",
    "    df.rename(columns={\"medoid\":\"canonical_label\"}, inplace=True)\n",
    "\n",
    "    # Sanity\n",
    "    no_cano = int(df[\"canonical_label\"].isna().sum())\n",
    "    if no_cano:\n",
    "        p(f\"‚ö†Ô∏è {no_cano:,} tokens missing canonical_label after join (will drop).\")\n",
    "        df = df.dropna(subset=[\"canonical_label\"]).reset_index(drop=True)\n",
    "\n",
    "    # Save artifacts\n",
    "    p(\"\\n============================================================\")\n",
    "    p(\"üíæ SAVING CANONICAL MAP\")\n",
    "    p(\"============================================================\")\n",
    "    df_out = df[[\"token\",\"cluster_id\",\"canonical_label\"] + extras].copy()\n",
    "    df_out.to_parquet(CANON_PARQUET, index=False)\n",
    "    df_out.to_csv(CANON_CSV, index=False)\n",
    "    p(f\"‚úÖ Parquet ‚Üí {CANON_PARQUET}\")\n",
    "    p(f\"‚úÖ CSV     ‚Üí {CANON_CSV}\")\n",
    "\n",
    "    # Coverage & quick stats\n",
    "    n_tokens = df_out[\"token\"].nunique()\n",
    "    n_clusters = df_out[\"cluster_id\"].nunique()\n",
    "    n_cano = df_out[\"canonical_label\"].nunique()\n",
    "    self_maps = int((df_out[\"token\"] == df_out[\"canonical_label\"]).sum())\n",
    "    p(\"\\n============================================================\")\n",
    "    p(\"üìà SUMMARY\")\n",
    "    p(\"============================================================\")\n",
    "    p(f\"Tokens mapped      : {n_tokens:,}\")\n",
    "    p(f\"Clusters covered   : {n_clusters:,}\")\n",
    "    p(f\"Canonical labels   : {n_cano:,}\")\n",
    "    p(f\"Identity mappings  : {self_maps:,} ({self_maps/max(1,n_tokens):.2%})\")\n",
    "\n",
    "    # Top canonical labels by cluster size (from summary if available)\n",
    "    if \"size\" in sum_df.columns:\n",
    "        top = sum_df.sort_values([\"size\",\"medoid_wdeg\" if \"medoid_wdeg\" in sum_df.columns else \"size\"], ascending=[False, False]).head(10)\n",
    "        p(\"\\nTop 10 canonical labels by cluster size:\")\n",
    "        p(top[[\"cluster_id\",\"medoid\",\"size\"]].to_string(index=False))\n",
    "\n",
    "    # 200 random rewrites where token != canonical_label\n",
    "    p(\"\\n============================================================\")\n",
    "    p(\"üîç SAMPLE REWRITES (200)  ‚Äî token ‚Üí canonical_label\")\n",
    "    p(\"============================================================\")\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    diff = df_out[df_out[\"token\"] != df_out[\"canonical_label\"]]\n",
    "    sample_n = min(200, len(diff)) if len(diff) else min(200, len(df_out))\n",
    "    sample = diff.sample(sample_n, random_state=42) if len(diff) else df_out.sample(sample_n, random_state=42)\n",
    "    show_cols = [\"token\",\"canonical_label\",\"cluster_id\"] + [c for c in [\"degree\",\"wdegree\"] if c in sample.columns]\n",
    "    # Order for readability\n",
    "    sample = sample[show_cols].sort_values([\"cluster_id\",\"canonical_label\",\"token\"]).reset_index(drop=True)\n",
    "    print(sample.to_string(index=False, max_colwidth=80))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîé INPUTS\n",
      "============================================================\n",
      "giant cluster_id=0 | size=213,904\n",
      "giant edges file: romance-novel-nlp-research/src/eda_analysis/outputs/candidate_similarity_pairs.cluster0_refined.parquet | rows=1,135,066\n",
      "Content-word sets: 216,300 | time=2.93s\n",
      "============================================================\n",
      "üîé EDGE FILTER SUMMARY (GIANT)\n",
      "============================================================\n",
      "Kept edges: 1,076,118 / 1,135,066  | time=6.55s\n",
      "- gate_fail: 58,948\n",
      "Graph nodes=215,776 | edges=538,059\n",
      "============================================================\n",
      "üîé COMMUNITY DETECTION\n",
      "============================================================\n",
      "Communities: 35,766 | time=77.81s\n",
      "Size histogram:\n",
      "1-1: 5,261\n",
      "2-2: 5,672\n",
      "3-4: 8,019\n",
      "5-9: 9,417\n",
      "10-19: 6,385\n",
      "20-49: 1,012\n",
      "50-99: 0\n",
      "100-199: 0\n",
      "200-499: 0\n",
      "500-999: 0\n",
      "1000-999998: 0\n",
      "max=49 | median=4 | mean=6.03\n",
      "============================================================\n",
      "üíæ SAVED\n",
      "============================================================\n",
      "Edges used            : romance-novel-nlp-research/src/eda_analysis/outputs/giant_comm_edges_used.parquet\n",
      "Token map (community) : romance-novel-nlp-research/src/eda_analysis/outputs/clusters_token_map.community.parquet\n",
      "Summary (community)   : romance-novel-nlp-research/src/eda_analysis/outputs/clusters_summary.community.parquet\n",
      "Canonical map         : romance-novel-nlp-research/src/eda_analysis/outputs/token_canonical_map.community.parquet\n",
      "Canonical CSV         : romance-novel-nlp-research/src/eda_analysis/outputs/token_canonical_map.community.csv\n",
      "============================================================\n",
      "üîç SAMPLE REWRITES (200) ‚Äî token ‚Üí canonical_label\n",
      "============================================================\n",
      "                              token  cluster_id                     canonical_label\n",
      "                   05 may utc bonus           4                   01 june utc bonus\n",
      "               ebooks wattpad books           9                         queen betsy\n",
      "   fairytale and classic retellings          12                  pleasant surprises\n",
      "                                1 9          13                       own book read\n",
      "            potential sci fi series          18                       series sci fi\n",
      "                        rita awards          19                   cover jacket ehhh\n",
      "                 skim read at times          28                                skim\n",
      "         nuevo libros que yo queiro          63                    libros que tengo\n",
      "                      read wishlist          79                            medievel\n",
      "                                 4s          88                                 4 s\n",
      "            previewed not continued         101                                 135\n",
      "                         owned isbn         116                  it s electric baby\n",
      "        canaries conquer tbr s 2016         145                                yard\n",
      "            03 1 historical romance         167                  tr first in series\n",
      "                    character buffy         178                                 4 1\n",
      "             2015 new to me authors         203              new to me authors 2014\n",
      "                         red kindle         234                        library owns\n",
      "       books to remove from shelves         252                             classic\n",
      "                  ilsa madden mills         277                3rd quarter book log\n",
      "                     geoff laughton         350          zmental illnessagoraphobia\n",
      "                src summer2015 read         445                       alles mgliche\n",
      "             haveinlibraryyettoread         450                      alpha romero h\n",
      "                 2nd qtr challenges         525                      arag con abdct\n",
      "        you got me with that ending         527 architect carpenter designr flipper\n",
      "               a standalone perhaps         540                          standalone\n",
      "                artis dan selebriti         548                           meta 2017\n",
      "                         a roamance         564                          at cuyhoga\n",
      "             alpha with a capital a         577         alpha with a capital a baby\n",
      "                    in love with it         583                          auth a lib\n",
      "                           pbs sent         597                 summer sun and sand\n",
      "               jane austen inspired         615                     austen inspired\n",
      "romance werewolf lykae shapeshifter         677            shapeshifters werewolves\n",
      "                                 bd         691           recommended by bookpusher\n",
      "                     mafia romances         726                       kindle worlds\n",
      "                pub beachwalk press         741                    pub vivant press\n",
      "               tissues at the ready         746                         to read mmm\n",
      "                         l over 250         779                          l over 300\n",
      "                        boncompagni         805                          bonkbuster\n",
      "     to be reviewed at a later date         805                          bonkbuster\n",
      "               unlikeable character         823                                 boy\n",
      "      books i love the most want it        1109                   occhi a cuoricino\n",
      "                       tanya holmes        1147                      while deployed\n",
      "the girlfriend s guide to boys seri        1252                    don t get fooled\n",
      "                      contemp stuff        1263  dontbothertheintrovertwhilereading\n",
      "                 rock star musician        1266       h too horny to think straight\n",
      "                  worst ending ever        1289                      hell s hunters\n",
      "               bukake facials baths        1309                   sc bukake facials\n",
      "                            spaaace        1358                            spaaaace\n",
      "       hero smexin hot drool worthy        1386                                yoga\n",
      "                 interacial romance        1494                interspecies romance\n",
      "                    at library book        1527                       in my library\n",
      "        romance erotic contemporary        1537                            soilders\n",
      "              pagehabit friends tbr        1540             sports baseball retired\n",
      "                   chocolate lovers        1586                      chocolate love\n",
      "          dnf might return to later        1705                            stricken\n",
      "   paranormal romance books to read        1773                          horrendous\n",
      "                 1 reviewed audible        1822                            518 home\n",
      "                         imblushing        1860                      traumatic past\n",
      "  weak boring annoying dumb heroine        1903                      boring heroine\n",
      "             home sweet home series        1922                nicolabooksiwontread\n",
      "     galaxy alien mail order brides        1960           2016 de freebie challenge\n",
      "                       mclain molly        1966                      oncoverreveals\n",
      "              series lords of satyr        2088                   tracey richardson\n",
      "                         pine hills        2147                               hills\n",
      "                         matefinder        2153                    kaitlyn o connor\n",
      "                          the taken        2202                                take\n",
      "                           good men        2216                             trajedy\n",
      "                     need right now        2285                             bruised\n",
      "                       cover i like        2348                         omegasphere\n",
      "                    lesbian to read        2385                           pygmalion\n",
      "                1 want to read soon        2462                    contemporary hea\n",
      " my favorite irish scottish romance        2478            cooper posey blood stone\n",
      "                           no avoid        2519                          crime pays\n",
      "                      a 2 read next        2521                   crossover fantasy\n",
      "                romance geeky nerdy        2525                 ctrope wealthy hero\n",
      "                   goodread authors        2584             goodreads authors books\n",
      "                        pgs 101 150        2607                         pgs 300 399\n",
      "                        i like smut        2653                      ebooks english\n",
      "                  too much thinking        2680                      thinking about\n",
      "                      apples valley        2786                           ffinished\n",
      "                     karmas a bitch        2882                               karma\n",
      "          1st in a series challenge        2889                          geen serie\n",
      "                     worth 2nd read        2985                     worth a re read\n",
      "                  first read winner        3005                     first read wins\n",
      "     felt really bad for the heroin        3015 hero my romancelandia husband harem\n",
      "                       via bookmate        3131                            bookmate\n",
      "                         nj walters        3233                       walters ednah\n",
      "                    wolf s princess        3239             big bad wolf books 2013\n",
      "      to read novella or short book        3446                 novella short reads\n",
      "                       summer of 30        3511              30 days of summer 2016\n",
      "   female character w troubled past        3616     paranormal romance vamps tramps\n",
      "                 plus size heroines        3634                       physical 2016\n",
      "                          owning it        3669                   priority owned mf\n",
      "         reserach for queen project        3766              research for the queen\n",
      "          author kathleen woodiwiss        3779                   rlci 2013 bk goal\n",
      "                 angel demon reaper        3811                    sa book wishlist\n",
      "       seduced and coerced to marry        3818                             seducer\n",
      "                    beckett sparrow        3886                 shifters wereanimal\n",
      "                   11a next to read        3930                   11 must read soon\n",
      "          i had higher expectations        3942            didn t meet expectations\n",
      "                               good        3960                           good good\n",
      "    x bdsm erotica etc want to read        4123                    i got pissed off\n",
      "     heroine disgraced fallen woman        4159                    won but not read\n",
      "                            tbrsoon        4298                             tbrhave\n",
      "                       freebie read        5192                  freebie to be read\n",
      "                       school story        5342                        kids in plot\n",
      "                             candis        5478                               terry\n",
      "                   paranormal world        6140               paranormal otherworld\n",
      "                   april 2012 reads        6299                    march 2012 reads\n",
      "                       100 top must        6708                             top 100\n",
      "                      local library        6819                local public library\n",
      "     hilarious secondary characters        6844          great secondary characters\n",
      "                    married couples        7033                             married\n",
      "                 the ivy chronicles        7297                             the ivy\n",
      "                          bought 99        7341                     02 purchased 99\n",
      "                      fab male lead        7362                      fave male lead\n",
      "                  burn the doormats        7391              wipe it on the doormat\n",
      "                        shirl henke        7646                               shiek\n",
      "                    vicki gen books        7760                       vicki s books\n",
      "                       ficstbr soon        7906                              fic cb\n",
      "                       shirley karr        7993                             shirley\n",
      "                 character musician        8155                      music musician\n",
      "                    must read manga        8267                         read mangas\n",
      "             series dark protectors        8636                     dark protectors\n",
      "                            kortney        8944                                koll\n",
      "                       char fighter        9052                   character fighter\n",
      "                        harlem lily        9133                                lily\n",
      "                     written review        9642                     reviews written\n",
      "                         its smokin       10467                          smokin hot\n",
      "                 down under setting       10761                          down under\n",
      "                     melissa andrea       11377                             melissa\n",
      "                 read grey jeanette       11520                         andrew grey\n",
      "                          emo witty       12598                            emo hero\n",
      "              can t wait for sequal       12603                      to read sequal\n",
      "                2017 tcf books read       12712                      tcf 2013 reads\n",
      "                      tcf fwys 2017       12787                            tcf 2016\n",
      "                          storage 1       12901                           storage 2\n",
      "       started of well then fizzled       13489            started well ended badly\n",
      "             c tortured broken hero       13502               tortured damaged hero\n",
      "     content dubious or non consent       13866                     dubious content\n",
      "                               r rm       14101                               af am\n",
      "               herione was annoying       14206                             herione\n",
      "                      may june 2015       14386                            may 2015\n",
      "                        char badass       14445          badass character kicks ass\n",
      "                     must read book       14802           to read have book already\n",
      "                  darker the better       14876                   darker and darker\n",
      "              2017 to read to print       15244                     2016 read print\n",
      "             need to add to calibre       15307                      add to calibre\n",
      "                  smutty smutsville       15416                     wheres the smut\n",
      "                          2 wish mm       15485                       wish maybe mm\n",
      "                  police department       15915                        police force\n",
      "                     m animal magic       16062                     job animal care\n",
      "            favourite holiday reads       16078                  holiday books read\n",
      "              ending needs epilogue       16329                   needs an epilogue\n",
      "                 zzz ebay purchases       16829                                ebay\n",
      "               read fiction romance       17719       fiction romance books to read\n",
      "                    ylva publishing       17763           publisher ylva publishing\n",
      "                        maybe shelf       17841                     the maybe shelf\n",
      "                              z ind       17937                                z kn\n",
      "       amazon prime lending library       18049                   amazon prime lend\n",
      "                zzz cover body part       18131                    zzz cover weapon\n",
      "                           sp alien       18526                             alien s\n",
      "                      p celebrities       18567                         celebrities\n",
      "                     what is a plot       18873                   where is the plot\n",
      "                        prioritrios       18891                               prior\n",
      "                                2 r       19116                               r 2 r\n",
      "               child ren from an ex       19219                   adopted child ren\n",
      "                         to be gone       19923                            vanished\n",
      "                      has me hooked       20564                       got me hooked\n",
      "                royal agents series       20717         series immortal brotherhood\n",
      "                            gaslamp       20792                     gaslamp fantasy\n",
      "         strong female kicking butt       20964              kick ass female strong\n",
      "                 tragedy love story       20971                         tragic love\n",
      "                               z bb       20985                              z beta\n",
      "              manga light novel own       21001                  manga light novels\n",
      "                               shag       21315                             sharmel\n",
      "                           jindejak       21704                               jinal\n",
      "                         scary past       22122                       horrible past\n",
      "                       sexual magic       22319                           gay magic\n",
      "                 vaguely interested       22789                 slightly interested\n",
      "                lucy kincaid series       22959                        lucy kincaid\n",
      "             search and rescue dogs       23226                   search and rescue\n",
      "                 publisher red rose       23437                 red rose publishing\n",
      "              written in 1st person       23659                written first person\n",
      "              recommended overdrive       23796            overdrive recommendation\n",
      "            books i ve commissioned       24082                 books i ve designed\n",
      "                      veterans ptsd       24380              military war hero ptsd\n",
      "             lesbian romantic story       25390                     lesbian novella\n",
      "                  cheating fuckwits       26055                    cheating fuckers\n",
      "                 adventure survival       26420              survival and adventure\n",
      "                title king or queen       26590                          queen king\n",
      "                   bad life at home       26829                     tough home life\n",
      "              end of summer reading       27372                  read during summer\n",
      "                   all of the angst       27743                       all the angst\n",
      "        teen problems relationships       27965             teen real life problems\n",
      "   books by people who review books       28024           review books from authors\n",
      "   sociology and psychology romance       28628               psychological romance\n",
      "                     s mmf and more       29256                         s mm or mmf\n",
      "                     cover beverage       29337                   beverage on cover\n",
      "heroine is grieving loss of a spous       30468                    grieving heroine\n"
     ]
    }
   ],
   "source": [
    "# romance-novel-nlp-research/src/eda_analysis/cell3g_split_giant_community_detection.py\n",
    "\"\"\"\n",
    "Split the giant cluster via community detection on a filtered, weighted subgraph.\n",
    "Re-export canonical labels (medoids) and print 200 random rewrites.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import sys, re, time, math\n",
    "from pathlib import Path\n",
    "from typing import Dict, Set, List, Tuple\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- Paths ----------------\n",
    "OUT = Path(\"romance-novel-nlp-research/src/eda_analysis/outputs\")\n",
    "MAP_REF   = OUT / \"clusters_token_map.refined.parquet\"\n",
    "SUM_REF   = OUT / \"clusters_summary.refined.parquet\"\n",
    "EDGES_G0  = OUT / \"candidate_similarity_pairs.cluster0_refined.parquet\"  # edges within giant cluster\n",
    "\n",
    "# Outputs\n",
    "MAP_COM   = OUT / \"clusters_token_map.community.parquet\"\n",
    "SUM_COM   = OUT / \"clusters_summary.community.parquet\"\n",
    "CANON_PQ  = OUT / \"token_canonical_map.community.parquet\"\n",
    "CANON_CSV = OUT / \"token_canonical_map.community.csv\"\n",
    "EDGES_USED= OUT / \"giant_comm_edges_used.parquet\"\n",
    "\n",
    "# --------------- Config ----------------\n",
    "SIM_STRONG: float = 0.65\n",
    "SIM_WEAK: float   = 0.55\n",
    "MIN_SHARED_WEAK: int = 2\n",
    "WEIGHT_ALPHA: float = 0.5          # weight = sim * (1 + alpha * jaccard)\n",
    "SHORT_LEN_MAX: int = 3\n",
    "SHORT_RF_MIN: float = 0.70\n",
    "PRINT_SUBCOMM: int = 15\n",
    "REWRITE_SAMPLE_N: int = 200\n",
    "\n",
    "# --------------- Imports ---------------\n",
    "try:\n",
    "    import networkx as nx\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Requires networkx. Install: pip install networkx\") from e\n",
    "\n",
    "try:\n",
    "    from rapidfuzz.fuzz import ratio as fuzz_ratio\n",
    "    HAS_RF = True\n",
    "except Exception:\n",
    "    HAS_RF = False\n",
    "\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "    STOP = set(w.lower() for w in ENGLISH_STOP_WORDS)\n",
    "except Exception as e:\n",
    "    STOP = {\"a\",\"an\",\"the\",\"and\",\"or\",\"to\",\"of\",\"in\",\"on\",\"for\",\"with\",\"by\",\"as\",\"from\",\n",
    "            \"this\",\"that\",\"these\",\"those\",\"it\",\"its\",\"is\",\"are\",\"be\",\"was\",\"were\",\"been\",\n",
    "            \"i\",\"you\",\"we\",\"he\",\"she\",\"they\",\"me\",\"him\",\"her\",\"them\",\"my\",\"your\",\"our\",\"their\",\n",
    "            \"not\",\"no\",\"yes\",\"into\",\"over\",\"under\",\"up\",\"down\",\"out\",\"more\",\"most\",\"less\",\"very\"}\n",
    "\n",
    "TOKEN_RE = re.compile(r\"[a-z]{2,}\")\n",
    "\n",
    "# ------------- Helpers -----------------\n",
    "def p(x: str) -> None:\n",
    "    print(x); sys.stdout.flush()\n",
    "\n",
    "def content_words(s: str) -> Set[str]:\n",
    "    return {w for w in TOKEN_RE.findall(s.lower()) if w not in STOP}\n",
    "\n",
    "def short_guard(a: str, b: str) -> bool:\n",
    "    if min(len(a), len(b)) > SHORT_LEN_MAX:\n",
    "        return True\n",
    "    if not HAS_RF:\n",
    "        return True\n",
    "    return (fuzz_ratio(a, b) / 100.0) >= SHORT_RF_MIN\n",
    "\n",
    "def medoid_of(nodes: List[str], adj: Dict[str, List[Tuple[str,float]]]) -> Tuple[str, int, float]:\n",
    "    best, best_score = None, (-1, -1.0)\n",
    "    for t in nodes:\n",
    "        nbrs = adj.get(t, [])\n",
    "        d = len(nbrs)\n",
    "        w = (sum(s for _, s in nbrs) / d) if d else 0.0\n",
    "        if (d, w) > best_score:\n",
    "            best_score = (d, w); best = t\n",
    "    return best, best_score[0], best_score[1]\n",
    "\n",
    "# --------------- Main ------------------\n",
    "def main() -> None:\n",
    "    # Load inputs\n",
    "    if not (MAP_REF.exists() and SUM_REF.exists() and EDGES_G0.exists()):\n",
    "        raise FileNotFoundError(\"Missing refined artifacts. Ensure previous cells finished.\")\n",
    "    map_df = pd.read_parquet(MAP_REF)    # token, cluster_id, degree, wdegree, flags\n",
    "    sum_df = pd.read_parquet(SUM_REF)    # cluster_id, size, medoid, ...\n",
    "    edges0 = pd.read_parquet(EDGES_G0)   # token_a, token_b, cosine_sim, rank\n",
    "\n",
    "    # Identify giant cluster id (largest row in refined summary)\n",
    "    giant_row = sum_df.sort_values([\"size\",\"medoid_wdeg\" if \"medoid_wdeg\" in sum_df.columns else \"size\"],\n",
    "                                   ascending=[False, False]).iloc[0]\n",
    "    giant_cid = int(giant_row[\"cluster_id\"])\n",
    "    g_tokens = set(map_df.loc[map_df[\"cluster_id\"] == giant_cid, \"token\"])\n",
    "    p(\"============================================================\")\n",
    "    p(\"üîé INPUTS\")\n",
    "    p(\"============================================================\")\n",
    "    p(f\"giant cluster_id={giant_cid} | size={len(g_tokens):,}\")\n",
    "    p(f\"giant edges file: {EDGES_G0} | rows={len(edges0):,}\")\n",
    "\n",
    "    # Precompute content words\n",
    "    t0 = time.time()\n",
    "    all_nodes = set(edges0[\"token_a\"]).union(set(edges0[\"token_b\"]))\n",
    "    cw = {s: content_words(s) for s in all_nodes}\n",
    "    p(f\"Content-word sets: {len(cw):,} | time={time.time()-t0:.2f}s\")\n",
    "\n",
    "    # Filter edges + compute weights\n",
    "    kept = []\n",
    "    dropped = Counter()\n",
    "    t1 = time.time()\n",
    "    for a, b, sim in zip(edges0[\"token_a\"], edges0[\"token_b\"], edges0[\"cosine_sim\"]):\n",
    "        if not short_guard(a, b):\n",
    "            dropped[\"short_guard_fail\"] += 1\n",
    "            continue\n",
    "        A, B = cw[a], cw[b]\n",
    "        shared = len(A & B)\n",
    "        union = max(1, len(A | B))\n",
    "        jacc = shared / union\n",
    "        s = float(sim)\n",
    "        keep = (s >= SIM_STRONG) or (s >= SIM_WEAK and shared >= MIN_SHARED_WEAK)\n",
    "        if not keep:\n",
    "            dropped[\"gate_fail\"] += 1\n",
    "            continue\n",
    "        w = s * (1.0 + WEIGHT_ALPHA * jacc)\n",
    "        kept.append((a, b, s, shared, jacc, w))\n",
    "    p(\"============================================================\")\n",
    "    p(\"üîé EDGE FILTER SUMMARY (GIANT)\")\n",
    "    p(\"============================================================\")\n",
    "    p(f\"Kept edges: {len(kept):,} / {len(edges0):,}  | time={time.time()-t1:.2f}s\")\n",
    "    for k,v in dropped.items():\n",
    "        p(f\"- {k}: {v:,}\")\n",
    "\n",
    "    if not kept:\n",
    "        raise RuntimeError(\"No edges kept; relax thresholds.\")\n",
    "\n",
    "    # Build weighted graph\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(g_tokens)\n",
    "    for a, b, s, shared, jacc, w in kept:\n",
    "        G.add_edge(a, b, weight=w, sim=s, shared=shared, jacc=jacc)\n",
    "    p(f\"Graph nodes={G.number_of_nodes():,} | edges={G.number_of_edges():,}\")\n",
    "\n",
    "    # Community detection (asynchronous label propagation, weighted)\n",
    "    t2 = time.time()\n",
    "    comms = list(nx.algorithms.community.asyn_lpa_communities(G, weight=\"weight\", seed=42))\n",
    "    p(\"============================================================\")\n",
    "    p(\"üîé COMMUNITY DETECTION\")\n",
    "    p(\"============================================================\")\n",
    "    p(f\"Communities: {len(comms):,} | time={time.time()-t2:.2f}s\")\n",
    "\n",
    "    sizes = sorted([len(c) for c in comms], reverse=True)\n",
    "    p(\"Size histogram:\")\n",
    "    bins = [1,2,3,5,10,20,50,100,200,500,1000,999999]\n",
    "    for i in range(len(bins)-1):\n",
    "        lo, hi = bins[i], bins[i+1]-1\n",
    "        cnt = sum(1 for s in sizes if lo <= s <= hi)\n",
    "        p(f\"{lo}-{hi}: {cnt:,}\")\n",
    "    p(f\"max={sizes[0]:,} | median={int(np.median(sizes))} | mean={np.mean(sizes):.2f}\")\n",
    "\n",
    "    # Build adjacency for medoids\n",
    "    adj: Dict[str, List[Tuple[str,float]]] = defaultdict(list)\n",
    "    for a, b, s, shared, jacc, w in kept:\n",
    "        adj[a].append((b, s)); adj[b].append((a, s))\n",
    "\n",
    "    # New IDs for giant subcommunities (stable)\n",
    "    BASE = giant_cid * 1_000_000\n",
    "    rows_sum = []\n",
    "    token_to_new = {}\n",
    "    for sub_id, nodes in enumerate(comms):\n",
    "        nodes_list = list(nodes)\n",
    "        med, mdeg, mw = medoid_of(nodes_list, adj)\n",
    "        new_cid = BASE + sub_id\n",
    "        for t in nodes_list:\n",
    "            token_to_new[t] = new_cid\n",
    "        rows_sum.append({\n",
    "            \"cluster_id\": new_cid, \"size\": len(nodes_list), \"medoid\": med,\n",
    "            \"medoid_deg\": int(mdeg), \"medoid_wdeg\": float(mw),\n",
    "            \"mean_deg\": float(np.mean([len(adj.get(t, [])) for t in nodes_list])),\n",
    "            \"mean_wdeg\": float(np.mean([(sum(s for _, s in adj.get(t, []))/max(1,len(adj.get(t, []))))\n",
    "                                        for t in nodes_list])),\n",
    "            \"short_rate\": float(np.mean([len(t)<=3 for t in nodes_list])),\n",
    "            \"digit_rate\": float(np.mean([any(c.isdigit() for c in t) for t in nodes_list])),\n",
    "            \"zz_rate\": float(np.mean([t.strip().lower().startswith('zz') for t in nodes_list]))\n",
    "        })\n",
    "\n",
    "    # Merge with non-giant clusters\n",
    "    map_com = map_df.copy()\n",
    "    mask = map_com[\"cluster_id\"] == giant_cid\n",
    "    map_com.loc[mask, \"cluster_id\"] = map_com.loc[mask, \"token\"].map(token_to_new).astype(\"int64\")\n",
    "    # Summary\n",
    "    sum_non = sum_df[sum_df[\"cluster_id\"] != giant_cid].copy()\n",
    "    sum_giant_new = pd.DataFrame(rows_sum)\n",
    "    sum_com = pd.concat([sum_non, sum_giant_new], ignore_index=True)\\\n",
    "                 .sort_values([\"size\",\"medoid_wdeg\"], ascending=[False, False])\n",
    "\n",
    "    # Canonical map (token ‚Üí medoid)\n",
    "    medoids = sum_com[[\"cluster_id\",\"medoid\"]].copy()\n",
    "    canon = map_com[[\"token\",\"cluster_id\"]].merge(medoids, on=\"cluster_id\", how=\"left\")\n",
    "    canon.rename(columns={\"medoid\":\"canonical_label\"}, inplace=True)\n",
    "\n",
    "    # Save artifacts\n",
    "    pd.DataFrame(kept, columns=[\"token_a\",\"token_b\",\"cosine_sim\",\"shared\",\"jaccard\",\"weight\"]).to_parquet(EDGES_USED, index=False)\n",
    "    map_com.to_parquet(MAP_COM, index=False)\n",
    "    sum_com.to_parquet(SUM_COM, index=False)\n",
    "    canon.to_parquet(CANON_PQ, index=False)\n",
    "    canon.to_csv(CANON_CSV, index=False)\n",
    "\n",
    "    # Prints\n",
    "    p(\"============================================================\")\n",
    "    p(\"üíæ SAVED\")\n",
    "    p(\"============================================================\")\n",
    "    p(f\"Edges used            : {EDGES_USED}\")\n",
    "    p(f\"Token map (community) : {MAP_COM}\")\n",
    "    p(f\"Summary (community)   : {SUM_COM}\")\n",
    "    p(f\"Canonical map         : {CANON_PQ}\")\n",
    "    p(f\"Canonical CSV         : {CANON_CSV}\")\n",
    "\n",
    "    # Spot-check 200 rewrites (token != canonical)\n",
    "    p(\"============================================================\")\n",
    "    p(\"üîç SAMPLE REWRITES (200) ‚Äî token ‚Üí canonical_label\")\n",
    "    p(\"============================================================\")\n",
    "    diff = canon[canon[\"token\"] != canon[\"canonical_label\"]]\n",
    "    sample = diff.sample(min(REWRITE_SAMPLE_N, len(diff)), random_state=42) if len(diff) else canon.sample(min(REWRITE_SAMPLE_N, len(canon)), random_state=42)\n",
    "    print(sample.sort_values([\"cluster_id\",\"canonical_label\",\"token\"]).to_string(index=False, max_colwidth=80))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîé INPUTS\n",
      "============================================================\n",
      "romance-novel-nlp-research/src/eda_analysis/outputs/clusters_token_map.community.parquet | exists=True | size=4.68 MB\n",
      "romance-novel-nlp-research/src/eda_analysis/outputs/clusters_summary.community.parquet | exists=True | size=1.48 MB\n",
      "romance-novel-nlp-research/src/eda_analysis/outputs/giant_comm_edges_used.parquet | exists=True\n",
      "clusters=42,784 | tokens=232,918\n",
      "============================================================\n",
      "üîß BUILD STRUCTURAL STATS FROM EDGES (giant portion)\n",
      "============================================================\n",
      "============================================================\n",
      "üöÄ COMPUTE NEW CANONICAL LABELS (quality re-score + synthetic labels)\n",
      "============================================================\n",
      "[1000/42784] clusters processed...\n",
      "[2000/42784] clusters processed...\n",
      "[3000/42784] clusters processed...\n",
      "[4000/42784] clusters processed...\n",
      "[5000/42784] clusters processed...\n",
      "[6000/42784] clusters processed...\n",
      "[7000/42784] clusters processed...\n",
      "[8000/42784] clusters processed...\n",
      "[9000/42784] clusters processed...\n",
      "[10000/42784] clusters processed...\n",
      "[11000/42784] clusters processed...\n",
      "[12000/42784] clusters processed...\n",
      "[13000/42784] clusters processed...\n",
      "[14000/42784] clusters processed...\n",
      "[15000/42784] clusters processed...\n",
      "[16000/42784] clusters processed...\n",
      "[17000/42784] clusters processed...\n",
      "[18000/42784] clusters processed...\n",
      "[19000/42784] clusters processed...\n",
      "[20000/42784] clusters processed...\n",
      "[21000/42784] clusters processed...\n",
      "[22000/42784] clusters processed...\n",
      "[23000/42784] clusters processed...\n",
      "[24000/42784] clusters processed...\n",
      "[25000/42784] clusters processed...\n",
      "[26000/42784] clusters processed...\n",
      "[27000/42784] clusters processed...\n",
      "[28000/42784] clusters processed...\n",
      "[29000/42784] clusters processed...\n",
      "[30000/42784] clusters processed...\n",
      "[31000/42784] clusters processed...\n",
      "[32000/42784] clusters processed...\n",
      "[33000/42784] clusters processed...\n",
      "[34000/42784] clusters processed...\n",
      "[35000/42784] clusters processed...\n",
      "Done. Clusters processed: 42,784 | time=119.83s\n",
      "============================================================\n",
      "üíæ SAVING (v2 canonicalization)\n",
      "============================================================\n",
      "‚úÖ Summary (v2): romance-novel-nlp-research/src/eda_analysis/outputs/clusters_summary.community.v2.parquet\n",
      "‚úÖ Canon map v2 (parquet): romance-novel-nlp-research/src/eda_analysis/outputs/token_canonical_map.community.v2.parquet\n",
      "‚úÖ Canon map v2 (csv):     romance-novel-nlp-research/src/eda_analysis/outputs/token_canonical_map.community.v2.csv\n",
      "============================================================\n",
      "üìà SUMMARY\n",
      "============================================================\n",
      "Clusters: 35,115 | Tokens: 232,918 | Unique canonical labels: 35,041\n",
      "Top 15 largest clusters (new labels):\n",
      " cluster_id  size                          new_label label_source               best_token_candidate  best_token_score\n",
      "          1   334                less than 250 pages        token                less than 250 pages          0.779154\n",
      "          2   144     rrrc meet and greet selections        token     rrrc meet and greet selections          1.107401\n",
      "          3   100  contemporary romance erotica bdsm        token  contemporary romance erotica bdsm          1.146877\n",
      "          4    83                     july utc bonus        token                     july utc bonus          0.962656\n",
      "         15    71                            samhain        token                            samhain          1.050000\n",
      "         22    70               summer books to read        token               summer books to read          1.068620\n",
      "          7    68           00 feb mar ult challenge        token           00 feb mar ult challenge          1.011363\n",
      "         10    67               new author challenge        token               new author challenge          1.095024\n",
      "         12    66              fairy tale retellings        token              fairy tale retellings          1.102381\n",
      "          8    66         pride prejudice variations        token         pride prejudice variations          1.101350\n",
      "          6    64 2015 read a thon jul aug challenge        token 2015 read a thon jul aug challenge          0.964484\n",
      "          5    64                            on deck        token                            on deck          0.974052\n",
      "          9    63            published wattpad books        token            published wattpad books          1.102740\n",
      "         13    58               completely abandoned        token               completely abandoned          1.052343\n",
      "        746    58                     tissues needed        token                     tissues needed          1.064286\n",
      "\n",
      "Changed labels: 186,590 / 341,295 (54.7%)\n",
      "\n",
      "============================================================\n",
      "üîç SAMPLE REWRITES (200) ‚Äî token ‚Üí old_label  //  new_label\n",
      "============================================================\n",
      "                              token                           old_label                          new_label  cluster_id\n",
      "                     will read soon                                  99                     will read soon          14\n",
      "                            zboxs12                              zboxb1                          bookstore          16\n",
      "       genre dystopian scifi series                    gardella vampire             science fiction series          18\n",
      "                    botb june bonus                         alyson noel              2015 botb april bonus          23\n",
      "                              bea13                            bea 2013                              frost          27\n",
      "                 yaoi ellora s cave                        elloras cave                   pub elloras cave          42\n",
      "       mm short story under 100 pgs                           pre order                pre ordered ordered          46\n",
      "                   przeczytane 2017                    przeczytane 2017          currently reading fantasy          65\n",
      "               przeczytane maj 2017                want to read fantasy          currently reading fantasy          65\n",
      "                    shelf x authors                      in the shadows                    shelf s authors          66\n",
      "          2017 audio book challenge                            break up                audiobook challenge          73\n",
      "                     read wish list                            medievel              wishlist want to read          79\n",
      "              wishlist want to read                            medievel              wishlist want to read          79\n",
      "            a z reading challenge 1          a z reading challenge 2017              a z reading challenge          85\n",
      "            challenge complete 2017                           gladiator                gladiator challenge          87\n",
      "                2014 rrrc challenge                   11 nahrungsmittel                     rrrc challenge          97\n",
      "             instalove or instalust                  holy smokes batman                instalust instalove         105\n",
      "                  fairytale romance                                 rpf      romance based on a fairy tale         114\n",
      "                     favourite hero                  fairy tale romance      romance based on a fairy tale         114\n",
      "                  books with bodies                  dnf did not finish  dnf did not finish do not recomme         134\n",
      "           lovelyorplayfulalphamale                      crazyalphamale                            en casa         150\n",
      "             mysteries and chicklit                chick lit paranormal                     trilogy series         158\n",
      "           supense thriller mystery                         bucket list          mystery thriller suspence         172\n",
      "               kindle purchased tbr                     2r0 kindred s8x                   nextbookinseries         175\n",
      "                   spitfire heroine                    spitfire heroine                 a z book challenge         206\n",
      "    historical romance not so clean            historical romance clean                   mm cops military         240\n",
      "                               mayo                                mayo                           changing         273\n",
      "                      single mother seducir a mi ayudante one night sta                     single mom dad         290\n",
      "                       hot sexy men                   blackberry island                       sexy hot men         299\n",
      "                   challenge august                  0 august challenge               august m m challenge         343\n",
      "                  read billionaires         2016 dhasg yearly challenge                 billionaire novels         364\n",
      "                     surprise child                           uummazing                      baby surprise         385\n",
      "                 endearing narrator                             morocco                sweet and endearing         409\n",
      "                sweet and endearing                       alas notavail                sweet and endearing         409\n",
      "                           rosemoor                            rosemoor                          novella s         424\n",
      "               do you need a cuddle                           promocave           sweet cuddle hug snuggle         433\n",
      "                   series no boxset                      read on anobii                     series box set         493\n",
      "    next book in series not out yet                  highly antecipated       series waiting for next book         507\n",
      "                     standalone yes                          standalone                    want standalone         540\n",
      "                         f f modern                        ae via books                     m f historical         562\n",
      "         job bartender or bar owner                 bartender bar owner      character bar owner bartender         576\n",
      "                             draggy                    more from auther                            dragged         582\n",
      "              kindle unlimited read                            bbc 2017              kindle unlimited read         606\n",
      "          young adult intergalactic              maailman ympri kirjoin                     inter galactic         619\n",
      "                                b2s            irish counties challenge               digitally bootlegged         623\n",
      "                                dgt                                  dg                           code red         664\n",
      "             another virgin heroine                              bandas                     virgin heroine         665\n",
      "                     heroine virgin                heroine not a virgin                     virgin heroine         665\n",
      "                          dnfs bbcs           bankrupt or frozen assets                            czytada         667\n",
      "                    m m read online              have read sample maybe     have read sample read for sure         737\n",
      "     2017 around the year challenge      2017 around the year challenge                       wink authors         764\n",
      "                  to read in summer                  wine about it bpub                     summer to read         768\n",
      "                        faith books                           bodygyard                    religious books         795\n",
      "      why spellapalooza winter 2017             bogged down maybe later      why spellapalooza winter 2017         799\n",
      "      02b1 order independent series      excuse me but wtf r u you doin               02b2 order dependent         820\n",
      "                   book discussions                    1 fantasy sci fi              talking volumes books         830\n",
      "        bisexual book award winners                  r oppositesattract                              raven         886\n",
      "                           4 star e                      4 star regular                            painful         904\n",
      "               10 oct ult challange     attacking my kindle free list 2                        ult oct nov         975\n",
      "                  uber jealous hero                        chgind2017ir              the most jealous hero         981\n",
      "                       browyn scott                   bookbub mysteries                        talyn scott        1046\n",
      "                     containsm preg                      containsm preg                new adult mature ya        1064\n",
      "             adventure is out there                                keen                           rbm keen        1073\n",
      "            character shapeshifters            characters shapeshifters       shapeshifters characters cox        1089\n",
      "                     jerk but sweet                          laaaaadies                               jerk        1092\n",
      "                     criminal twist                                 d b                 reviews by heather        1133\n",
      "     hero doesnt know he has a baby                          l b dunbar       hero has a child or children        1138\n",
      "                stephanie evanovich                            deferred               janet evanovich read        1177\n",
      "                     sad depressing                         dugeon play            depressing and just sad        1184\n",
      "                              q fic                               fic o                             am fic        1196\n",
      "                   must first short                          must short                           olympics        1221\n",
      "                     psychic vision                         to giveaway                 dreams and visions        1231\n",
      "          yes series getting better                  friendship centric                 book not available        1240\n",
      "               z2 freebie nonkindle                    zz 2013 freebies                          z freebie        1262\n",
      "                        joelle knox                             stutter                        hadley knox        1269\n",
      "                  sentimental go to                         sentimental                  sentimental value        1334\n",
      "            paranormal shifters any                 paranormal shifters        fantasy paranormal shifters        1351\n",
      "               made by for in india            books for 2016 challenge          book challenge books 2016        1399\n",
      "                    sarah s to read                      felicitas ivey                       sarahs books        1445\n",
      "                         romanticky                                infj                     usa california        1514\n",
      "                         lezrom fic                        placeholdeer                             lezrom        1518\n",
      "                     books i love 2                     books i enjoyed      books i ve thoroughly enjoyed        1546\n",
      "                           year2015                            year2017    year getbackunderbridge midyear        1604\n",
      "                      botb nov 2014                       botb feb 2014                       botb jan feb        1612\n",
      "                  mm oh shit im gay                       htts lesson 6                     mm gay for you        1744\n",
      "  41 books you might ve missed 2017  2017 books i shouldn t have missed                 harlequin intrigue        1752\n",
      "                    quirkyblinddate                            tonedark                  quirky blind date        1769\n",
      "                          smashword                         2small town                          smashword        1781\n",
      "                   5 stars for real                i m in like with you             myths legends folklore        1817\n",
      "ancient greece rome egypt etc roman                          iabbaddict                       greek greece        1826\n",
      "                    heiress heroine                             labeled            heroine heiress wealthy        1840\n",
      "                      period future         abused by bf gf husband etc                   a period in time        1890\n",
      "                    wow at synopsis                        alskdfjasdlk      synopsis grabbed my attention        1951\n",
      "               kindle lending books                                  ay     kindle books available to lend        2076\n",
      "                     woo sex scenes                   bad guy rich dude                      sexual scenes        2091\n",
      "                      better ideals            may thor strike you dead                             brooke        2158\n",
      "                 austen fan fiction                      meange thruple                jane austen fiction        2169\n",
      "                   placeholder book               th mehrere jahrzehnte                               take        2202\n",
      "                      diane mannino                  whydoireadthiscrap                     sandra bricker        2257\n",
      "                        c last name                                  ni                        a last name        2265\n",
      "                      bpl wish list                        what a ninny                        ow wants om        2269\n",
      "                    auclair colette                       c 16rc random                            colette        2303\n",
      "                    rrrc 02 feb rtm             canadian erotic fiction                    rrrc rts summer        2313\n",
      "                   author nm silber             cat crime investigation                       silber stuff        2323\n",
      "                 jen owns can trade                                 s d                           jen owns        2387\n",
      "      contemporay christian fiction          christian fiction suspence                 botb oct christian        2390\n",
      "       young adult books for adults                         unn investm       young adult books for adults        2439\n",
      "                          courtlife                              straya                           bd manga        2497\n",
      "                                sdd                                  sd                    possessive hero        2534\n",
      "         7 price watch not lendable                      syma hynnydeuh                      price watcher        2582\n",
      "                            spartan                disappointing sequel                       ashlyn chase        2594\n",
      "              deadly secrets series                              sporom                     secrets series        2595\n",
      "                        201 250 pgs                         pgs 300 399                     pgs mushing pg        2607\n",
      "               genre m m paranormal                              suvena             m m paranormal fantasy        2615\n",
      "              wizards witches magic                  edit wary vs weary              witches wizards magic        2663\n",
      "                            sploosh                               emosh                     physical owned        2677\n",
      "         its the drama in the llama                                  ug                        drama llama        2688\n",
      "                           equality                         underthesea                    ruptura amorosa        2693\n",
      "                   best sex scene s                    explicit romance               best damn sex scenes        2722\n",
      "                    fairytale esque          what are you trying to say                    vi esque trying        2739\n",
      "      carpenter contractor handyman                            handyman      carpenter contractor handyman        2783\n",
      "                       m m ive read                         fluffy smut                             read m        2811\n",
      "                        sugar daddy     fmale lead is an absolute bitch                         sugarcreek        2813\n",
      "                 awesome love story                             fragile                 a great love story        2835\n",
      "                            zzz dnf                              zz dnf            z dnf just not my thing        2929\n",
      "                        never never                            gunnison                   never ever never        2955\n",
      "               abused or past abuse                         abused past                  past abuse trauma        3147\n",
      "                    family series s                       family series              family related series        3202\n",
      "                       the g series                must read mjune 2017                           series g        3463\n",
      "                     mount tbr 2016                      mount tbr 2017                          mount tbr        3499\n",
      "       plot running from the past h           otra travelling book club               hh running from past        3586\n",
      "                  powertools series                          powertools                 fiction books read        3661\n",
      "                           hated it                      ptntl srs seqs                absolutely hated it        3685\n",
      "                   worthremembering                  rating dua bintang                     worthrereading        3710\n",
      "                 yay i think im gay                                  rb                             gay ya        3712\n",
      "                    kathleen brooks                   rlci 2013 bk goal                    kathleen brooks        3779\n",
      "                      hm ward books                            h m ward                            ward hm        3911\n",
      "           challenge mm smexin 2010                        sorbet books                       mm challenge        3921\n",
      "             single book favourites                t sport past current             single book favourites        3977\n",
      "                  don t own yet tbr                 thank you netgalley                       dont own yet        4009\n",
      "                        mm whiny mc                               mm mc                       broken mc mm        4036\n",
      "                 read 2017 november     waiting for 5 complete series 5                     november reads        4117\n",
      "                     janelle mowery                   wooden shelf left                       paula mowery        4161\n",
      "                  yay for libraries                          ya library                         sea breeze        4190\n",
      "                               1 mj                         fredrick mj                       fredrick m j        4905\n",
      "                      caitlin daire                       caitlin crews                      caitlin ricci        4946\n",
      "                        g steampunk                           steampunk                   steamy steampunk        5421\n",
      "                    multiple copies                            copies 1                    multiple copies        5620\n",
      "         given by author for review             from authors for review         given by author for review        5845\n",
      "           want to read the sequels              sequels i want to read               need to read sequels        6340\n",
      "            veiled seduction series        secrets and seduction series            veiled seduction series        6630\n",
      "                   book made me cry              books that make me cry   books that sometimes make me cry        7097\n",
      "                maybe buy maybe not                           maybe buy                     maybe purchase        7892\n",
      "            royalty romance to read                       royal romance         contemporary royal romance        8292\n",
      "                 best of 2014 lists                           2015 list                    list best lists        8308\n",
      "                   both experienced                both h h experienced                   both experienced        8367\n",
      "                    fire police spy                      police firemen            law enforcement firemen        8501\n",
      "                g opposites attract                   opposites attract               th opposites attract        8992\n",
      "   weirdest craziest shit i ve read                  all kinds of crazy                       crazy people        9778\n",
      "                      0 menage poly                         menage poly                   list menage poly        9985\n",
      "                             return                           return to                       return later       10084\n",
      "                        cover by me                         cover by me                   tempted by cover       10259\n",
      "         favorite christian fiction              best christian fiction         favorite christian fiction       10596\n",
      "                            pn gods                            pnr gods                      pnr mythology       10679\n",
      "                  now i read fanfic                      fanfic to read                 fanfic online read       10951\n",
      "    genre mystery thriller suspense           mystery thriller suspense   suspense action thriller mystery       11357\n",
      "                     grief and loss                          grief loss                   death loss grief       11389\n",
      "                         grief loss                          grief loss                   death loss grief       11389\n",
      "               places south america                       south america              setting south america       11676\n",
      "        2016 botb 09 september dare                2016 botb march dare       may 2016 botb dare challenge       11755\n",
      "                         donna hill                               donna                       donna alward       11881\n",
      "                         trope food                 trope food industry                      food industry       11958\n",
      "                        added y2013                               y2011                        y2012 added       12203\n",
      "                        to read mfm                         content m f                        content mff       12261\n",
      "               could ve done better      could have been so much better               could ve been better       12768\n",
      "           mystery within a romance           mystery with some romance    mystery with a touch of romance       13158\n",
      "                   tag he got money                           has money                someone s got money       15028\n",
      "             western nativeamerican                      western native             western nativeamerican       15176\n",
      "                   next follow auth                               autho                        new authour       15853\n",
      "         tortured dark hero heroine               tortured hero heroine      scarred tortured hero heroine       16076\n",
      "                             cheeky                              cheeky                             tongue       16434\n",
      "                gay sci fi dystopic                        gay dystopia            gay sci fi or dystopian       17487\n",
      "           finished everything done                       finished with                           finished       17885\n",
      "           check for series release               series to be released                new series releases       18675\n",
      "                        hawt heroes                                hawt                     hawt hawt hawt       18941\n",
      "                        blair kerry                           dar kerry                              kerry       19089\n",
      "                    reviewed for rt                       review for rt                         rt reviews       19466\n",
      "                   bdsm want 2 read                           bdsm read                  bdsm want to read       19668\n",
      "                        davis a tac                         davis a tac          a tac series by dee davis       19984\n",
      "                  100 books by 2015                   100 books in 2014                   books reads read       20180\n",
      "                    dangerous dudes                      dangerous guys                  bad dangerous men       20767\n",
      "                kompletirana serija                 kompletirana serija                            serijal       20826\n",
      "                   2017 august haul                           haul 2016                  haul march august       21867\n",
      "                also in a boxed set                           boxed set                         boxed sets       22637\n",
      "                               ukya                           ukya 2016                               ukya       24002\n",
      "              series one eyed jacks               one eyed jacks series              series one eyed jacks       24499\n",
      "                 4 projects in 2016                  2 projects in 2017 3 projects for the next year or so       25177\n",
      "                        reread hihi                                hihi                            hilburn       25620\n",
      "                   reviewed in 2011                       reviewed 2012                 year 2012 reviewed       26842\n",
      "                              noc14                            noc 2017                           love noc       31477\n",
      "\n",
      "============================================================\n",
      "NEXT ACTION HINTS\n",
      "============================================================\n",
      "- If labels still look noisy, raise penalties (DIGIT_PENALTY, ZZ_PENALTY) or require ALPHA_RATIO_MIN_FOR_TOKEN_LABEL‚âà0.75.\n",
      "- Increase SYNTH_MIN_WORDS to 3 to force more descriptive synthetic labels on tiny communities.\n",
      "- Optional next: merge communities by new-label embeddings (thr‚âà0.85) to reduce 35k ‚Üí fewer macro-topics.\n"
     ]
    }
   ],
   "source": [
    "# romance-novel-nlp-research/src/eda_analysis/cell3h_canonicalize_merge_labels.py\n",
    "\"\"\"\n",
    "Re-score canonical labels for community clusters with quality heuristics and synthetic labels.\n",
    "Outputs new canonical map + summary and prints 200 before/after rewrites for QA.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import re, sys, math, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Set\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------- Paths -----------------------------\n",
    "OUT = Path(\"romance-novel-nlp-research/src/eda_analysis/outputs\")\n",
    "MAP_COM   = OUT / \"clusters_token_map.community.parquet\"\n",
    "SUM_COM   = OUT / \"clusters_summary.community.parquet\"\n",
    "EDGES_USED= OUT / \"giant_comm_edges_used.parquet\"   # edges from giant-cluster refinement\n",
    "\n",
    "# Outputs (v2 canonicalization)\n",
    "SUM_V2    = OUT / \"clusters_summary.community.v2.parquet\"\n",
    "CANON_V2_PQ = OUT / \"token_canonical_map.community.v2.parquet\"\n",
    "CANON_V2_CSV= OUT / \"token_canonical_map.community.v2.csv\"\n",
    "\n",
    "# --------------------------- Config knobs ------------------------\n",
    "SYNTH_TOP_K_WORDS: int = 3\n",
    "SYNTH_MIN_WORDS: int = 2\n",
    "\n",
    "# Quality scoring configuration\n",
    "class QUALITY:\n",
    "    # weights to balance structure + string quality\n",
    "    DEG_W: float = 0.60\n",
    "    MEAN_SIM_W: float = 0.40\n",
    "    CW_BONUS_W: float = 0.20\n",
    "    # penalties\n",
    "    DIGIT_PENALTY: float = 0.70   # scaled by digit_ratio\n",
    "    SHORT_PENALTY: float = 0.60   # len<4\n",
    "    ZZ_PENALTY: float = 0.80      # startswith 'zz'\n",
    "    NONALPHA_PENALTY: float = 0.50 # 1 - alpha_ratio\n",
    "\n",
    "# selection rules\n",
    "PREFER_SYNTH_IF_LOW_QUALITY: bool = True\n",
    "LOW_QUALITY_SCORE_CUTOFF: float = 0.30\n",
    "ALPHA_RATIO_MIN_FOR_TOKEN_LABEL: float = 0.65\n",
    "MAX_LABEL_LEN: int = 80\n",
    "\n",
    "# ----------------------------- Utils -----------------------------\n",
    "def p(x: str) -> None:\n",
    "    print(x); sys.stdout.flush()\n",
    "\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "    STOP = set(w.lower() for w in ENGLISH_STOP_WORDS)\n",
    "except Exception:\n",
    "    STOP = {\"a\",\"an\",\"the\",\"and\",\"or\",\"to\",\"of\",\"in\",\"on\",\"for\",\"with\",\"by\",\"as\",\"from\",\n",
    "            \"this\",\"that\",\"these\",\"those\",\"it\",\"its\",\"is\",\"are\",\"be\",\"was\",\"were\",\"been\",\n",
    "            \"i\",\"you\",\"we\",\"he\",\"she\",\"they\",\"me\",\"him\",\"her\",\"them\",\"my\",\"your\",\"our\",\"their\",\n",
    "            \"not\",\"no\",\"yes\",\"into\",\"over\",\"under\",\"up\",\"down\",\"out\",\"more\",\"most\",\"less\",\"very\"}\n",
    "\n",
    "TOKEN_RE = re.compile(r\"[a-z]{2,}\")\n",
    "\n",
    "def content_words(s: str) -> List[str]:\n",
    "    return [w for w in TOKEN_RE.findall(s.lower()) if w not in STOP]\n",
    "\n",
    "def digit_ratio(s: str) -> float:\n",
    "    if not s: return 0.0\n",
    "    d = sum(c.isdigit() for c in s); return d/len(s)\n",
    "\n",
    "def alpha_ratio(s: str) -> float:\n",
    "    if not s: return 0.0\n",
    "    a = sum(c.isalpha() for c in s); return a/len(s)\n",
    "\n",
    "def is_bad_prefix(s: str) -> bool:\n",
    "    x = s.strip().lower()\n",
    "    return x.startswith(\"zz\") or x.startswith(\"0 \") or x in {\"good\",\"bad\",\"ok\",\"okay\"}\n",
    "\n",
    "def clamp(x: float, a: float, b: float) -> float:\n",
    "    return max(a, min(b, x))\n",
    "\n",
    "# ------------------------ Load artifacts -------------------------\n",
    "p(\"============================================================\")\n",
    "p(\"üîé INPUTS\")\n",
    "p(\"============================================================\")\n",
    "for f in [MAP_COM, SUM_COM]:\n",
    "    p(f\"{f} | exists={f.exists()} | size={(f.stat().st_size/1024/1024):.2f} MB\" if f.exists() else f\"{f} | MISSING\")\n",
    "if not (MAP_COM.exists() and SUM_COM.exists()):\n",
    "    raise FileNotFoundError(\"Missing community artifacts. Run cell3g first.\")\n",
    "\n",
    "map_df = pd.read_parquet(MAP_COM)    # token, cluster_id, possibly degree/wdegree from earlier\n",
    "sum_df = pd.read_parquet(SUM_COM)    # cluster_id, size, medoid, medoid_deg, medoid_wdeg, ...\n",
    "\n",
    "HAS_EDGES = EDGES_USED.exists()\n",
    "p(f\"{EDGES_USED} | exists={HAS_EDGES}\")\n",
    "edges = pd.read_parquet(EDGES_USED) if HAS_EDGES else None\n",
    "\n",
    "p(f\"clusters={len(sum_df):,} | tokens={len(map_df):,}\")\n",
    "\n",
    "# ----------------- Token structural stats (giant only) -----------------\n",
    "# Recompute degree & mean_sim from edges for nodes present (covers giant-portion).\n",
    "p(\"============================================================\")\n",
    "p(\"üîß BUILD STRUCTURAL STATS FROM EDGES (giant portion)\")\n",
    "p(\"============================================================\")\n",
    "deg = defaultdict(int)\n",
    "sim_sum = defaultdict(float)\n",
    "\n",
    "if HAS_EDGES:\n",
    "    for a, b, s in zip(edges[\"token_a\"], edges[\"token_b\"], edges[\"cosine_sim\"]):\n",
    "        s = float(s)\n",
    "        deg[a] += 1; sim_sum[a] += s\n",
    "        deg[b] += 1; sim_sum[b] += s\n",
    "\n",
    "# Merge structural stats into map_df; keep previous columns as fallback\n",
    "if \"degree\" not in map_df.columns: map_df[\"degree\"] = 0\n",
    "if \"wdegree\" not in map_df.columns: map_df[\"wdegree\"] = 0.0\n",
    "\n",
    "map_df[\"_deg2\"] = map_df[\"token\"].map(lambda t: deg.get(t, 0)).astype(np.int32)\n",
    "map_df[\"_msim2\"] = map_df[\"token\"].map(lambda t: (sim_sum.get(t, 0.0) / max(1, deg.get(t, 0)))).astype(np.float32)\n",
    "\n",
    "# Combine: prefer recomputed if available\n",
    "map_df[\"_deg\"]  = map_df[[\"_deg2\",\"degree\"]].max(axis=1)\n",
    "map_df[\"_msim\"] = map_df[[\"_msim2\",\"wdegree\"]].max(axis=1)\n",
    "\n",
    "# Normalize per cluster later\n",
    "# --------------------- Quality score function -------------------------\n",
    "def token_quality_score(token: str, deg_norm: float, mean_sim_norm: float) -> float:\n",
    "    \"\"\"Combine structure + string quality into a single score in [0,1+].\"\"\"\n",
    "    cw = content_words(token)\n",
    "    cw_bonus = QUALITY.CW_BONUS_W * clamp(len(cw)/4.0, 0.0, 1.0)  # up to +0.20 for ‚â•4 content words\n",
    "    # penalties\n",
    "    pen = 0.0\n",
    "    pen += QUALITY.DIGIT_PENALTY * digit_ratio(token)\n",
    "    pen += QUALITY.SHORT_PENALTY * (1.0 if len(token) < 4 else 0.0)\n",
    "    pen += QUALITY.ZZ_PENALTY * (1.0 if is_bad_prefix(token) else 0.0)\n",
    "    pen += QUALITY.NONALPHA_PENALTY * (1.0 - alpha_ratio(token))\n",
    "    base = QUALITY.DEG_W * deg_norm + QUALITY.MEAN_SIM_W * mean_sim_norm\n",
    "    return clamp(base + cw_bonus - pen, -1.0, 2.0)\n",
    "\n",
    "# --------------------- Per-cluster rescoring --------------------------\n",
    "p(\"============================================================\")\n",
    "p(\"üöÄ COMPUTE NEW CANONICAL LABELS (quality re-score + synthetic labels)\")\n",
    "p(\"============================================================\")\n",
    "rows_sum = []\n",
    "token_rows = []\n",
    "\n",
    "# Pre-group to speed up\n",
    "grp = map_df.groupby(\"cluster_id\", sort=False)\n",
    "N = len(sum_df)\n",
    "\n",
    "t0 = time.time()\n",
    "for idx, (cid, g) in enumerate(grp, start=1):\n",
    "    # Normalize deg/msim inside the cluster for fairness\n",
    "    d = g[\"_deg\"].to_numpy(dtype=np.float32, copy=False)\n",
    "    m = g[\"_msim\"].to_numpy(dtype=np.float32, copy=False)\n",
    "    d_norm = (d - d.min()) / (d.max() - d.min() + 1e-9)\n",
    "    m_norm = (m - m.min()) / (m.max() - m.min() + 1e-9)\n",
    "\n",
    "    # Score tokens\n",
    "    scores = []\n",
    "    for tok, dn, mn in zip(g[\"token\"].tolist(), d_norm.tolist(), m_norm.tolist()):\n",
    "        scores.append((tok, token_quality_score(tok, dn, mn)))\n",
    "    # Best token candidate\n",
    "    best_tok, best_score = max(scores, key=lambda x: x[1])\n",
    "\n",
    "    # Synthetic label: top content words across cluster\n",
    "    cw_counts = Counter()\n",
    "    for tok in g[\"token\"].tolist():\n",
    "        cw_counts.update(content_words(tok))\n",
    "    synth_words = [w for w,_ in cw_counts.most_common(SYNTH_TOP_K_WORDS)]\n",
    "    synth_words = [w for w in synth_words if len(w) >= 2][:SYNTH_TOP_K_WORDS]\n",
    "    synth_label = \" \".join(synth_words[:SYNTH_TOP_K_WORDS])[:MAX_LABEL_LEN]\n",
    "    use_synth = False\n",
    "\n",
    "    # Decide final label\n",
    "    if PREFER_SYNTH_IF_LOW_QUALITY:\n",
    "        # If best token looks low-quality, prefer synthetic if it has enough words\n",
    "        if (best_score < LOW_QUALITY_SCORE_CUTOFF or alpha_ratio(best_tok) < ALPHA_RATIO_MIN_FOR_TOKEN_LABEL) and len(synth_words) >= SYNTH_MIN_WORDS:\n",
    "            use_synth = True\n",
    "\n",
    "    final_label = synth_label if use_synth and synth_label else best_tok\n",
    "    label_source = \"synthetic\" if (final_label == synth_label and final_label) else \"token\"\n",
    "\n",
    "    # Build summary row\n",
    "    prev_row = sum_df.loc[sum_df[\"cluster_id\"] == cid]\n",
    "    prev_medoid = prev_row[\"medoid\"].iloc[0] if len(prev_row) else None\n",
    "    rows_sum.append({\n",
    "        \"cluster_id\": int(cid),\n",
    "        \"size\": int(len(g)),\n",
    "        \"prev_medoid\": prev_medoid,\n",
    "        \"new_label\": final_label,\n",
    "        \"label_source\": label_source,\n",
    "        \"best_token_candidate\": best_tok,\n",
    "        \"best_token_score\": float(best_score),\n",
    "        \"uniq_cw\": int(len(cw_counts)),\n",
    "    })\n",
    "\n",
    "    # Assign to all tokens in cluster\n",
    "    tok_df = g[[\"token\",\"cluster_id\"]].copy()\n",
    "    tok_df[\"canonical_label\"] = final_label\n",
    "    token_rows.append(tok_df)\n",
    "\n",
    "    if idx % 1000 == 0:\n",
    "        p(f\"[{idx}/{N}] clusters processed...\")\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "p(f\"Done. Clusters processed: {N:,} | time={elapsed:.2f}s\")\n",
    "\n",
    "sum_v2 = pd.DataFrame(rows_sum).sort_values([\"size\",\"new_label\"], ascending=[False, True])\n",
    "canon_v2 = pd.concat(token_rows, ignore_index=True)\n",
    "\n",
    "# ------------------------ Save artifacts -------------------------\n",
    "p(\"============================================================\")\n",
    "p(\"üíæ SAVING (v2 canonicalization)\")\n",
    "p(\"============================================================\")\n",
    "sum_v2.to_parquet(SUM_V2, index=False)\n",
    "canon_v2.to_parquet(CANON_V2_PQ, index=False)\n",
    "canon_v2.to_csv(CANON_V2_CSV, index=False)\n",
    "p(f\"‚úÖ Summary (v2): {SUM_V2}\")\n",
    "p(f\"‚úÖ Canon map v2 (parquet): {CANON_V2_PQ}\")\n",
    "p(f\"‚úÖ Canon map v2 (csv):     {CANON_V2_CSV}\")\n",
    "\n",
    "# ------------------------ Prints & QA ----------------------------\n",
    "p(\"============================================================\")\n",
    "p(\"üìà SUMMARY\")\n",
    "p(\"============================================================\")\n",
    "n_clusters = sum_v2[\"cluster_id\"].nunique()\n",
    "n_tokens = canon_v2[\"token\"].nunique()\n",
    "n_labels = sum_v2[\"new_label\"].nunique()\n",
    "p(f\"Clusters: {n_clusters:,} | Tokens: {n_tokens:,} | Unique canonical labels: {n_labels:,}\")\n",
    "p(\"Top 15 largest clusters (new labels):\")\n",
    "p(sum_v2.head(15)[[\"cluster_id\",\"size\",\"new_label\",\"label_source\",\"best_token_candidate\",\"best_token_score\"]].to_string(index=False, max_colwidth=60))\n",
    "\n",
    "# Compare with previous canonical map (if exists)\n",
    "prev_map_pq = OUT / \"token_canonical_map.community.parquet\"\n",
    "if prev_map_pq.exists():\n",
    "    prev = pd.read_parquet(prev_map_pq)[[\"token\",\"cluster_id\",\"canonical_label\"]].rename(columns={\"canonical_label\":\"old_label\"})\n",
    "    merged = canon_v2.merge(prev, on=[\"token\",\"cluster_id\"], how=\"left\")\n",
    "    changed = merged[merged[\"canonical_label\"] != merged[\"old_label\"]]\n",
    "    p(f\"\\nChanged labels: {len(changed):,} / {len(merged):,} ({len(changed)/max(1,len(merged)):.1%})\")\n",
    "\n",
    "    # Show 200 random before/after rewrites\n",
    "    p(\"\\n============================================================\")\n",
    "    p(\"üîç SAMPLE REWRITES (200) ‚Äî token ‚Üí old_label  //  new_label\")\n",
    "    p(\"============================================================\")\n",
    "    sample_n = min(200, len(changed)) if len(changed) else min(200, len(merged))\n",
    "    samp = (changed if len(changed) else merged).sample(sample_n, random_state=42)\n",
    "    out = samp[[\"token\",\"old_label\",\"canonical_label\",\"cluster_id\"]].rename(columns={\"canonical_label\":\"new_label\"})\n",
    "    out = out.sort_values([\"cluster_id\",\"new_label\",\"token\"]).reset_index(drop=True)\n",
    "    print(out.to_string(index=False, max_colwidth=80))\n",
    "else:\n",
    "    # No previous map; show straight 200 rewrites (token -> label)\n",
    "    p(\"\\n============================================================\")\n",
    "    p(\"üîç SAMPLE REWRITES (200) ‚Äî token ‚Üí new_label\")\n",
    "    p(\"============================================================\")\n",
    "    sample_n = min(200, len(canon_v2))\n",
    "    samp = canon_v2.sample(sample_n, random_state=42).sort_values([\"cluster_id\",\"canonical_label\",\"token\"])\n",
    "    print(samp.to_string(index=False, max_colwidth=80))\n",
    "\n",
    "p(\"\\n============================================================\")\n",
    "p(\"NEXT ACTION HINTS\")\n",
    "p(\"============================================================\")\n",
    "p(\"- If labels still look noisy, raise penalties (DIGIT_PENALTY, ZZ_PENALTY) or require ALPHA_RATIO_MIN_FOR_TOKEN_LABEL‚âà0.75.\")\n",
    "p(\"- Increase SYNTH_MIN_WORDS to 3 to force more descriptive synthetic labels on tiny communities.\")\n",
    "p(\"- Optional next: merge communities by new-label embeddings (thr‚âà0.85) to reduce 35k ‚Üí fewer macro-topics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîé INPUTS\n",
      "============================================================\n",
      "clusters=35,115 | tokens=232,918\n",
      "normalize_mode=year_blind | keep_numeric_topics=True\n",
      "‚ö†Ô∏è Empty merge keys: 154 (will preserve as separate clusters)\n",
      "Candidate groups: 33,746 | multi-merge groups: 637 | time=4.20s\n",
      "============================================================\n",
      "üíæ SAVED\n",
      "============================================================\n",
      "Token map (v3): romance-novel-nlp-research/src/eda_analysis/outputs/clusters_token_map.community.v3.parquet\n",
      "Summary   (v3): romance-novel-nlp-research/src/eda_analysis/outputs/clusters_summary.community.v3.parquet\n",
      "Canon map (v3): romance-novel-nlp-research/src/eda_analysis/outputs/token_canonical_map.community.v3.parquet\n",
      "Canon csv (v3): romance-novel-nlp-research/src/eda_analysis/outputs/token_canonical_map.community.v3.csv\n",
      "============================================================\n",
      "üìà MERGE SUMMARY\n",
      "============================================================\n",
      "Before communities : 35,115\n",
      "After communities  : 33,899\n",
      "Merging groups     : 638\n",
      "Clusters collapsed : 1,369\n",
      "\n",
      "Top 20 merge keys (size, key -> sample labels):\n",
      "-  26 | read -> ['read m', 'read c', 'read read', '2 read read', 'g read']\n",
      "-  23 | series -> ['series13', 'series g', 'b a d series', 'b series', 's a s s series']\n",
      "-  20 | romance -> ['h romance', '7 romance', 'a romance', 'l b romance', 'm f m romance']\n",
      "-  19 | books -> ['f books', 't books', 'e books', 'k books', 'g books']\n",
      "-  17 | kindle -> ['02 kindle', 'kindle', 'y kindle', 'a kindle', '6 kindle']\n",
      "-  15 | authors -> ['z authors', 'authors m', 'a authors b', 'authors w', 'authors h']\n",
      "-  14 | read to -> ['to read r', 'to read p s', 'to read', 'to read j', 'e to read']\n",
      "-  13 | own -> ['own 2014', 'own k', 'own a', 'own b3', 'own b13']\n",
      "-  12 | book challenge -> ['50 book challenge', 'a z book challenge', 'book challenge', 'book challenge 2018', 'book challenge 2015']\n",
      "-  12 | in -> ['150 in 2015', '26 in 2017', '16000 in 2016', '75 in 2016', 'in 2013']\n",
      "-  12 | title -> ['title b', 'title u', 'title n', 'v title', 'a z title']\n",
      "-  11 | character -> ['character z', 'character m', 'character c', 'character j', 'character o']\n",
      "-  11 | historical -> ['historical 1800s', 'm f historical', 'g historical', 'r historical', 'z historical']\n",
      "-  11 | paranormal -> ['z paranormal', 'paranormal', '03 paranormal', '2011 paranormal', '3 paranormal']\n",
      "-  10 | challenge reading -> ['challenge reading', 'a z reading challenge', 'reading challenge 2014', 'c2014 reading challenge', 'reading challenge']\n",
      "-  10 | fiction -> ['3 fiction', 'f fiction', '00 a fiction', 'm m fiction', 'z fiction']\n",
      "-  10 | in read -> ['x read in 2014', 'read in 201601', 'read in 2015', 'read in 201108', 'read in 2014']\n",
      "-  10 | to -> ['a to z 2015', '2017 a to z', 'a to l', '2004 to 2009', '30 to 40 k']\n",
      "-   9 | books read -> ['e books read', 'read books', 'books read', 'read books', 'read books']\n",
      "-   9 | next read -> ['9 read next', 'read next 3', '7 read next', '03 next read', 'read next next next']\n",
      "\n",
      "Examples merged by year-blind (up to 30):\n",
      "* ala  ::  ['ala 13', 'ala 2013']\n",
      "* and older  ::  ['2012 and older', '16 and older']\n",
      "* animal  ::  ['animal', 'animal 2017']\n",
      "* anticipated highly  ::  ['highly anticipated 2017', 'highly anticipated']\n",
      "* ao  ::  ['ao', 'ao 2013']\n",
      "* april  ::  ['04 april 2017', 'april 4']\n",
      "* arc netgalley  ::  ['netgalley arc', 'netgalley arc 2017', 'netgalley e arc']\n",
      "* audible available  ::  ['2017 audible available', 'audible available']\n",
      "* audiobooks  ::  ['audiobooks 2017', '010 audiobooks']\n",
      "* aug  ::  ['aug 16', '2014 aug', '23 aug 2011']\n",
      "* august release  ::  ['2015 august release', '08 august release', '2017 august release']\n",
      "* august reviews  ::  ['reviews august 2015', 'reviews august 2013']\n",
      "* august september  ::  ['august september 17', 'august september 2017', 'august september']\n",
      "* aussie great reads  ::  ['great aussie reads', 'great aussie reads 2013']\n",
      "* autumn  ::  ['autumn', 'autumn 2016']\n",
      "* before read  ::  ['read before 2017', 'read before', 'read before 2015']\n",
      "* best releases  ::  ['best releases', '2015 best releases']\n",
      "* biker  ::  ['m biker', 'biker', '2017 biker']\n",
      "* book books challenge  ::  ['challenge book books', 'book challenge books 2016', 'challenge book books', 'challenge book books', 'book challenge books']\n",
      "* book boyfriend  ::  ['book boyfriend', 'boyfriend book', '2017 book boyfriend']\n",
      "* book challenge  ::  ['50 book challenge', 'a z book challenge', 'book challenge', 'book challenge 2018', 'book challenge 2015', '20 book challenge']\n",
      "* book challenge reading  ::  ['challenge reading book', 'book reading challenge 2015', '52 book reading challenge 2k16']\n",
      "* bookchallenge  ::  ['bookchallenge', 'bookchallenge 2014']\n",
      "* books  ::  ['f books', 't books', 'e books', 'k books', 'g books', 'j books']\n",
      "* books bought in  ::  ['books bought in 2014', 'books bought in 2016', 'bought books in 2017']\n",
      "* books challenge  ::  ['challenge books 2014', '52 books challenge', '2015 challenge books']\n",
      "* books challenge read  ::  ['challenge books read 2015', 'challenge books read 2016']\n",
      "* books favorite of  ::  ['favorite books of 2016', 'favorite books of 2017', 'favorite books of 2014', 'favorite books of 2012']\n",
      "* books finished  ::  ['finished books', '2014 books finished']\n",
      "* books five star  ::  ['five star books', '2015 five star books']\n",
      "\n",
      "============================================================\n",
      "üîç SAMPLE REWRITES (200) ‚Äî token ‚Üí old_label  //  new_label\n",
      "============================================================\n",
      "                             token                       old_label              new_label          cluster_id\n",
      "                  7 p2p fanfiction                      p2p fanfic                                         90\n",
      "                         blitz day                        a kindle                                        180\n",
      "    2014 reading challenge to read     2015 reading challenge read                                        538\n",
      "                  on kindle 2 read         on kindle ready to read                                        544\n",
      "            2012 75 book challenge            challenge book books                                        570\n",
      "     kindle unlimited want to read           kindle unlimited read                                        606\n",
      "                      first quater                         c grade                                        642\n",
      "                         on hoopla                          hoopla                                        883\n",
      "            to read contemp modern                         contemp                                       1263\n",
      "                    17uc may bonus                17uc april bonus                                       1561\n",
      "                               add                           added                                       1590\n",
      "    really wanted to but nevermind              really really want                                       1694\n",
      "                 ebook kindle only                   ebooks kindle                                       1799\n",
      "                      series novel                     book series                                       1853\n",
      "                  brooding bad boy                           paper                                       2280\n",
      "            career hero lead snger                    03 book recs                                       2320\n",
      "                read 2017 february           read in february 2017                                       2381\n",
      "                              raft         romance erotica romance                                       2389\n",
      "                       early 1900s                 timeframe 1900s                                       2456\n",
      "                      01 h romance                      01 romance                                       2511\n",
      "                        03 romance                      01 romance                                       2511\n",
      "                    have this book                     i have book                                       2635\n",
      "                    m m wanna read                          read m                                       2811\n",
      "          historical romance ecopy            books i didnt finish                                       3044\n",
      "                     march october                         october                                       3216\n",
      "                    z find romance               z fantasy romance                                       3447\n",
      "                 purchased e books                      buy e book                                       3726\n",
      "          1920s historical fiction 20th century historical fiction                                       3760\n",
      "                          bdsm hom                            bdsm                                       3901\n",
      "               youthful exuberance          young adult young love                                       4166\n",
      "                         7 romance                       7 romance                                       4197\n",
      "            2nd to read future tbr                        read tbr                                       4444\n",
      "                       gay erotica                     gay erotica                                       4612\n",
      "             published before 2012                2012 publication                                       4853\n",
      "                       na suspense                      suspense r                                       5077\n",
      "                      0 mf fantasy                       m fantasy                                       5266\n",
      "                           on fire                            fire                                       5395\n",
      "                          a sci fi                          sci fi                                       5754\n",
      "                          apr 2017                         apr tbr                                       5891\n",
      "                i need to get this                            need                                       5904\n",
      "     to read authors i really like                    read authors                                       6030\n",
      "                    bingo round 15                  bingo round 15                                       6431\n",
      "                  2017 read kindle               kindle read books                                       6549\n",
      "                ebooks kindle 2016               kindle read books                                       6549\n",
      "               reviews august 2013             reviews august 2013                                       6733\n",
      "            2015 monthly challenge     2013 june monthly challenge                                       6859\n",
      "                   read books only                      read books                                       6958\n",
      "              sub g shape shifters                    m f shifters                                       7012\n",
      "                         24 ebooks                        1 ebooks                                       7128\n",
      "                     tstl hero ine                            tstl                                       7275\n",
      "                          5 rating                        5 rating                                       7478\n",
      "                        younger me                   older younger                                       7604\n",
      "                         adult gfy                         g adult                                       7662\n",
      "                 2k17 reading plan              read reads reading                                       7694\n",
      "                         take over                            over                                       7816\n",
      "                    nope nope nope                  nope nope nope                                       7819\n",
      "i wanna start this series wishlist                wish list series                                       7985\n",
      "          wish list part of series                wish list series                                       7985\n",
      "                       theme sweet                           theme                                       8336\n",
      "                 theme that s life                           theme                                       8336\n",
      "        g characters i wish i knew                    characters g                                       8411\n",
      "                      gb to choose                         gb pick                                       8533\n",
      "                   1 next novellas                          book 3                                       8660\n",
      "                          ala 2014                        ala 2013                                       8896\n",
      "                 top reads of 2011                      reads read                                       8964\n",
      "                         summer 09              2013 spring summer                                       9096\n",
      "                          pub 2005                        pub 2010                                       9109\n",
      "                          pub 2007                        pub 2010                                       9109\n",
      "                 2015 gang bang q1                   gang bang qtr                                       9530\n",
      "              have it but not read                 have it to read                                       9545\n",
      "                         3 8 stars                         9 stars                                       9607\n",
      "                          2012 jan                jan february feb                                       9757\n",
      "                              list                            list                                      10001\n",
      "                      2018 to read              read reads reading                                      10002\n",
      "                             uk ya                          europe                                      10088\n",
      "                 arc via netgalley                   netgalley arc                                      10493\n",
      "                       true alphas                     alpha alpha                                      10500\n",
      "                     12 2011bought                    8 2012bought                                      10620\n",
      "                        g mystical                         g magic                                      10819\n",
      "                     series are us                        series u                                      10980\n",
      "                         3 unknown                         unknown                                      11238\n",
      "                    out in unknown                         unknown                                      11238\n",
      "               author first name m                       authors m                                      11262\n",
      "                short stories 2017              short stories 2016                                      11684\n",
      "                          a z 2017                     2017 a to z                                      12151\n",
      "                          y07 2012                     y2012 added                                      12203\n",
      "                        05 words s                        words sb                                      12210\n",
      "           2014 challenges general                 2013 challenges                                      12322\n",
      "                     2015 freebies                   freebies 2014                                      12350\n",
      "   that one 2015 reading challenge            2015 challenge reads                                      12662\n",
      "               dark darker darkest                  dark dark dark                                      12716\n",
      "                cheating is not ok                        cheating                                      13878\n",
      "                         its magic                           magic                                      14156\n",
      "                           x magic                           magic                                      14156\n",
      "                     m time travel                 m m time travel                                      14342\n",
      "                        b2 fantasy                       1 fantasy                                      14412\n",
      "                      menage m m f                    menage m f m                                      15194\n",
      "  obstacles in love forbidden love                  forbidden love                                      15332\n",
      "                           utc feb                    01 march utc                                      15466\n",
      "                            over 3                          over 5                                      15597\n",
      "                      to read 2015                    read in 2015                                      16442\n",
      "                           like 50                       under 100                                      16522\n",
      "                             8 aug                          aug 16                                      16822\n",
      "                 release date 2017           release releases date                                      17457\n",
      "                       fiction all                         fiction                                      17635\n",
      "                      audio listen                           audio                                      17894\n",
      "                       junior high                   middle school                                      18788\n",
      "                         hawt damn                  hawt hawt hawt                                      18941\n",
      "           1f contemporary romance        m f contemporary romance                                      19246\n",
      "                m m series to read              m m series to read                                      19371\n",
      "       book reading challenge 2015     book reading challenge 2015                                      20212\n",
      "            reading challenge 2016          reading challenge 2014                                      20420\n",
      "                           3 magic                         3 magic                                      20677\n",
      "                    read in 201601                  read in 201601                                      20845\n",
      "                       2176 series                series completed                                      21285\n",
      "                    2016 have read                    read ve year                                      21405\n",
      "                        short shit                         short 1                                      22567\n",
      "                       e new adult                 f f young adult                                      22873\n",
      "                 november december                        november                                      23918\n",
      "                  to be read owned                   to read owned                                      24039\n",
      "                      new 14 10 16                        new 5 16                                      25469\n",
      "              reading challenge 17            reading challenge 16                                      25771\n",
      "                          what dog                          animal                                      26342\n",
      "                        x sporty h                      x sporty h                                      28210\n",
      "                    a series books                    series books                                      28727\n",
      "                     adventure f f           f f adventure fantasy                                      29982\n",
      "                    s fff and more                            s nf                                      31511\n",
      "                          j k rock                        j k rock                                      33034\n",
      "    2015 challenge completed reads                                        challenge reads  153588549715677555\n",
      "             ebooks i own and read                                            ebooks read  232181514415999421\n",
      "                            sh j 6                                                     sh  303128576425781363\n",
      "                    love forbidden                                         forbidden love  346539719970284031\n",
      "              loving the forbidden                                         forbidden love  346539719970284031\n",
      "                     2015 february                                       feb february jan  531000597985268108\n",
      "                         2b series                                                 series  761095678166340588\n",
      "                short multi series                                                 series  761095678166340588\n",
      "                         01freebie                                                   free  942875077966656467\n",
      "                       2 free soon                                                   free  942875077966656467\n",
      "                      dark darkish                                                   dark  978930503932190492\n",
      "                            9 sept                                                   sept 1307224590787915260\n",
      "                           short s                                                  short 1756914682159862481\n",
      "                      finish books                                         books finished 2248235381729862944\n",
      "                  y mental illness                                              in series 2315591817374730085\n",
      "                 two for 1 or more                                                    for 2371658415451963965\n",
      "     summer 2015 reading challenge                                 challenge read reading 2597623576612602841\n",
      "                read prior to 2014                                             prior read 2613512214380705251\n",
      "               02 bub erotica dark                                                erotica 2861852220311322536\n",
      "                       2014erotica                                                erotica 2861852220311322536\n",
      "                    like harlequin                                              harlequin 2977540227854863476\n",
      "                               q r                                                    and 3412981375060820771\n",
      "                    9 i entered it                                                    won 3572248695495570168\n",
      "           gay abdl age play taboo                                                  taboo 4028672985674389146\n",
      "                      00 read 2007                                                   read 4121683039699363535\n",
      "                            read b                                                   read 4121683039699363535\n",
      "   20th century historical romance                                     century romance th 4229768533834451576\n",
      "               read authors i know                                           authors read 4239739811752948208\n",
      "                      2003 to 2006                                                     to 4687486395828818928\n",
      "                          a z 2015                                                     to 4687486395828818928\n",
      "                         summer 16                                          spring summer 4746683339533019176\n",
      "                       summer 2010                                          spring summer 4746683339533019176\n",
      "                          month 12                                                  month 4787230031952950244\n",
      "                        2k14 reads                                     read reading reads 4820741794704724571\n",
      "                not today not ever                                              not today 5018089584573226822\n",
      "                         s 99 deal                                                bargain 5204092908550003217\n",
      "                   story slow burn                                              burn slow 5274862554355991118\n",
      "                       favorites f                                              favorites 5335990411034506349\n",
      "           2 stars nothing special                                            really want 5467035846842968564\n",
      "                      2 wanna read                                              read want 5833673648664424264\n",
      "            summer reads blog 2014                                              read want 5833673648664424264\n",
      "                      love polygon                                               business 5970509188948228076\n",
      "                      2011 06 june                                               jan june 6008859105080602611\n",
      "                       zombies yay                                                zombies 6014810870195664999\n",
      "                   might read next                                               may read 6096676633179625557\n",
      "                best books of 2016                                      books favorite of 6145081099011884276\n",
      "                 fav books of 2017                                      books favorite of 6145081099011884276\n",
      "            favorite books of 2014                                      books favorite of 6145081099011884276\n",
      "               grown up love story                                       adult love young 6518170737148457590\n",
      "                   to young for me                                       adult love young 6518170737148457590\n",
      "                    read on scribd                                            read scribd 6778249771372541749\n",
      "                       8 new adult                                              adult new 6941152639577690511\n",
      "                             pg 13                                               pg rated 7279216241166948364\n",
      "               the shepherds heart                                               pg rated 7279216241166948364\n",
      "                          2013 may                                                   june 7310374614536384420\n",
      "             2014 s summer reading                                         reading summer 7357287868879957167\n",
      "                         epub have                                                   epub 7398754118816162763\n",
      "                6 kindle unlimited                                       kindle unlimited 7443976017111468339\n",
      "                      on nook read                                           nook on read 7665452950892120382\n",
      "                             qubec                                                  grade 7834199118191070916\n",
      "                     five and four                                                 of out 7956006458691752133\n",
      "                    mm em services                                                  email 7989154340064788109\n",
      "                    to be readneed                                                read to 8106713242349437898\n",
      "                               rom                                                    rom 8224108737977785683\n",
      "                  2016 01 jan botb                                       botb jan january 8324783708702316668\n",
      "                    bingo round 16                                            bingo round 8450280110557635851\n",
      "                        new others                                                    new 8621036432737751772\n",
      "           great aussie reads 2013                                     aussie great reads 8777561612330383212\n",
      "                      arc netgally                                          arc netgalley 8977784779125772117\n",
      "                 arc off netgalley                                          arc netgalley 8977784779125772117\n",
      "              owned books ive read                                            books owned 9015498210512107534\n",
      "             g f thriller suspense                                      suspense thriller 9097981949798624317\n"
     ]
    }
   ],
   "source": [
    "# romance-novel-nlp-research/src/eda_analysis/cell3i_year_blind_merge_labels.py\n",
    "\"\"\"\n",
    "Year-blind community merge:\n",
    "- Merge communities whose labels become equal after normalization.\n",
    "- Keep numeric semantics when digits are meaningful (pages, ordinals, \"top 100\", etc).\n",
    "- Rebuild token‚Üícanonical map; print before/after stats and 200 rewrites.\n",
    "\n",
    "Why: unify ephemeral-year shelves (\"to read 2013/2014\") without wrecking numeric topics.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import re, sys, time, hashlib\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "OUT = Path(\"romance-novel-nlp-research/src/eda_analysis/outputs\")\n",
    "SUM_V2    = OUT / \"clusters_summary.community.v2.parquet\"\n",
    "CANON_V2  = OUT / \"token_canonical_map.community.v2.parquet\"\n",
    "MAP_COMM  = OUT / \"clusters_token_map.community.parquet\"  # for tokens‚Üíold cluster ids\n",
    "\n",
    "# Outputs\n",
    "MAP_V3    = OUT / \"clusters_token_map.community.v3.parquet\"\n",
    "SUM_V3    = OUT / \"clusters_summary.community.v3.parquet\"\n",
    "CANON_V3P = OUT / \"token_canonical_map.community.v3.parquet\"\n",
    "CANON_V3C = OUT / \"token_canonical_map.community.v3.csv\"\n",
    "\n",
    "# ----------------------- Config knobs -----------------------\n",
    "NORMALIZE_MODE = \"year_blind\"     # \"year_blind\" | \"strip_all_digits\"\n",
    "KEEP_NUMERIC_TOPICS = True        # don't strip digits if label mentions pages/pgs/ordinals/top lists, etc.\n",
    "SYNTH_LABEL_TOP_K = 3             # top content words to form synthetic label\n",
    "REWRITE_SAMPLE_N = 200\n",
    "\n",
    "YEAR_RE   = re.compile(r\"\\b(?:19|20)\\d{2}\\b\")\n",
    "DIGIT_RE  = re.compile(r\"\\d+\")\n",
    "TOKEN_RE  = re.compile(r\"[a-z]{2,}\")\n",
    "SPC_RE    = re.compile(r\"\\s{2,}\")\n",
    "\n",
    "# patterns where numbers matter -> preserve\n",
    "MEANINGFUL_NUM_PATTERNS = [\n",
    "    r\"\\bpage(s)?\\b\", r\"\\bpgs?\\b\", r\"\\bword(s)?\\b\", r\"\\bchapter(s)?\\b\",\n",
    "    r\"\\bbook(s)?\\b\\s*\\d+\", r\"\\bvol(ume)?\\b\\s*\\d+\", r\"\\bpart\\s*\\d+\",\n",
    "    r\"\\btop\\s*\\d+\\b\", r\"\\b\\d+\\s*(to|-)\\s*\\d+\\b\",  # ranges\n",
    "    r\"\\b\\d{1,3}%\\b\", r\"\\bstar(s)?\\b\", r\"\\bseries\\s*\\d+\\b\"\n",
    "]\n",
    "MEANINGFUL_NUM_RE = re.compile(\"|\".join(MEANINGFUL_NUM_PATTERNS), re.IGNORECASE)\n",
    "\n",
    "def p(x: str) -> None:\n",
    "    print(x); sys.stdout.flush()\n",
    "\n",
    "def content_words(s: str) -> List[str]:\n",
    "    return TOKEN_RE.findall(s.lower())\n",
    "\n",
    "def normalize_label(text: str, mode: str) -> str:\n",
    "    \"\"\"Remove only years by default; optional full digit strip with guards.\"\"\"\n",
    "    x = text.strip()\n",
    "    # If digits are meaningful, return early when KEEP_NUMERIC_TOPICS\n",
    "    if KEEP_NUMERIC_TOPICS and MEANINGFUL_NUM_RE.search(x or \"\"):\n",
    "        y = YEAR_RE.sub(\"\", x)\n",
    "        y = SPC_RE.sub(\" \", y).strip()\n",
    "        return y.lower()\n",
    "    if mode == \"year_blind\":\n",
    "        y = YEAR_RE.sub(\"\", x)\n",
    "    elif mode == \"strip_all_digits\":\n",
    "        y = DIGIT_RE.sub(\"\", x)\n",
    "    else:\n",
    "        y = x\n",
    "    y = SPC_RE.sub(\" \", y).strip()\n",
    "    return y.lower()\n",
    "\n",
    "def key_from_label(text: str, mode: str) -> str:\n",
    "    \"\"\"Key is sorted content-words after normalization; stable for exact grouping.\"\"\"\n",
    "    norm = normalize_label(text, mode)\n",
    "    toks = [t for t in content_words(norm) if t]  # already lower\n",
    "    if not toks:\n",
    "        return \"\"  # avoid merging empties\n",
    "    toks = sorted(set(toks))\n",
    "    return \" \".join(toks)\n",
    "\n",
    "def synth_label_from_key(key: str, k: int) -> str:\n",
    "    if not key: return \"\"\n",
    "    toks = key.split()\n",
    "    # prefer frequent-ish words first: here key is set; keep lexicographic for stability\n",
    "    return \" \".join(toks[:k])\n",
    "\n",
    "def stable_new_cluster_id(key: str) -> int:\n",
    "    \"\"\"Deterministic 64-bit id from key (avoid collisions across runs).\"\"\"\n",
    "    h = hashlib.blake2b(key.encode(\"utf-8\"), digest_size=8).hexdigest()\n",
    "    return int(h, 16)\n",
    "\n",
    "def main() -> None:\n",
    "    # ---------- Load ----------\n",
    "    for f in (SUM_V2, CANON_V2, MAP_COMM):\n",
    "        if not f.exists():\n",
    "            raise FileNotFoundError(f\"Missing input: {f}\")\n",
    "    sum_v2   = pd.read_parquet(SUM_V2)      # cluster_id, size, new_label, ...\n",
    "    canon_v2 = pd.read_parquet(CANON_V2)    # token, cluster_id, canonical_label\n",
    "    map_comm = pd.read_parquet(MAP_COMM)    # token, cluster_id (pre-v2, but ids match v2 clusters)\n",
    "\n",
    "    # ---------- Prep ----------\n",
    "    p(\"============================================================\")\n",
    "    p(\"üîé INPUTS\")\n",
    "    p(\"============================================================\")\n",
    "    p(f\"clusters={sum_v2['cluster_id'].nunique():,} | tokens={canon_v2['token'].nunique():,}\")\n",
    "    p(f\"normalize_mode={NORMALIZE_MODE} | keep_numeric_topics={KEEP_NUMERIC_TOPICS}\")\n",
    "\n",
    "    # Build merge keys from v2 labels\n",
    "    t0 = time.time()\n",
    "    sum_v2 = sum_v2.copy()\n",
    "    sum_v2[\"merge_key\"] = sum_v2[\"new_label\"].astype(str).map(lambda s: key_from_label(s, NORMALIZE_MODE))\n",
    "    # Guard: don't merge empty keys\n",
    "    sum_v2[\"merge_key\"] = sum_v2[\"merge_key\"].fillna(\"\").astype(str)\n",
    "    empty_keys = int((sum_v2[\"merge_key\"] == \"\").sum())\n",
    "    if empty_keys:\n",
    "        p(f\"‚ö†Ô∏è Empty merge keys: {empty_keys:,} (will preserve as separate clusters)\")\n",
    "    # Group clusters by key\n",
    "    grp = sum_v2.groupby(\"merge_key\", dropna=False)\n",
    "    groups = {k: g[\"cluster_id\"].tolist() for k, g in grp}\n",
    "    # Stats\n",
    "    multi = {k:v for k,v in groups.items() if k and len(v) > 1}\n",
    "    p(f\"Candidate groups: {len(groups):,} | multi-merge groups: {len(multi):,} | time={time.time()-t0:.2f}s\")\n",
    "\n",
    "    # ---------- Build old‚Üínew cluster id map ----------\n",
    "    old_to_new: Dict[int, int] = {}\n",
    "    key_to_label: Dict[str, str] = {}\n",
    "    rows_sum = []\n",
    "\n",
    "    for key, cids in groups.items():\n",
    "        # new id: stable per key; for empty key, keep original ids\n",
    "        if not key or len(cids) == 1:\n",
    "            # identity mapping for singles/empties\n",
    "            for cid in cids:\n",
    "                old_to_new[int(cid)] = int(cid)\n",
    "            # label = existing\n",
    "            lab = sum_v2.loc[sum_v2[\"cluster_id\"] == cids[0], \"new_label\"].iloc[0]\n",
    "            key_to_label[key] = lab\n",
    "            continue\n",
    "\n",
    "        new_cid = stable_new_cluster_id(key)\n",
    "        for cid in cids:\n",
    "            old_to_new[int(cid)] = int(new_cid)\n",
    "        # synthetic label from key\n",
    "        synth = synth_label_from_key(key, SYNTH_LABEL_TOP_K)\n",
    "        # fallback: most frequent existing label in the group\n",
    "        if not synth:\n",
    "            counts = Counter(sum_v2.loc[sum_v2[\"cluster_id\"].isin(cids), \"new_label\"].astype(str))\n",
    "            synth = counts.most_common(1)[0][0]\n",
    "        key_to_label[key] = synth\n",
    "\n",
    "    # ---------- Reassign token map ----------\n",
    "    t1 = time.time()\n",
    "    map_v3 = map_comm[[\"token\",\"cluster_id\"]].copy()\n",
    "    map_v3[\"cluster_id_old\"] = map_v3[\"cluster_id\"].astype(\"int64\")\n",
    "    map_v3[\"cluster_id\"] = map_v3[\"cluster_id_old\"].map(lambda c: old_to_new.get(int(c), int(c))).astype(\"int64\")\n",
    "\n",
    "    # ---------- Build v3 summary ----------\n",
    "    # Size per new cluster\n",
    "    sz = map_v3.groupby(\"cluster_id\").size().rename(\"size\").reset_index()\n",
    "    # Derive label per new cluster\n",
    "    # Map new cluster_id back to key to get label\n",
    "    # Build key‚Üínew_id (skip empties handled by identity)\n",
    "    key_to_newid = {k: stable_new_cluster_id(k) for k in groups.keys() if k and len(groups[k]) > 1}\n",
    "    # For clusters not merged: keep original label from v2\n",
    "    # First, build cid‚Üílabel from v2 (new_label)\n",
    "    cid2label_v2 = dict(zip(sum_v2[\"cluster_id\"].astype(\"int64\"), sum_v2[\"new_label\"].astype(str)))\n",
    "    labels = []\n",
    "    for cid in sz[\"cluster_id\"].astype(\"int64\"):\n",
    "        # if this cid is one of the merged new ids, find its key label\n",
    "        key = None\n",
    "        for k, nid in key_to_newid.items():\n",
    "            if nid == cid:\n",
    "                key = k; break\n",
    "        if key is not None:\n",
    "            labels.append(key_to_label.get(key, synth_label_from_key(key, SYNTH_LABEL_TOP_K)))\n",
    "        else:\n",
    "            labels.append(cid2label_v2.get(int(cid), \"\"))\n",
    "\n",
    "    sum_v3 = sz.copy()\n",
    "    sum_v3[\"label\"] = labels\n",
    "\n",
    "    # ---------- Build canonical map v3 ----------\n",
    "    # token -> label via cluster_id\n",
    "    cid2label_v3 = dict(zip(sum_v3[\"cluster_id\"].astype(\"int64\"), sum_v3[\"label\"].astype(str)))\n",
    "    canon_v3 = map_v3[[\"token\",\"cluster_id\"]].copy()\n",
    "    canon_v3[\"canonical_label\"] = canon_v3[\"cluster_id\"].map(lambda c: cid2label_v3.get(int(c), \"\"))\n",
    "\n",
    "    # ---------- Save ----------\n",
    "    map_v3.drop(columns=[\"cluster_id_old\"], inplace=True)\n",
    "    map_v3.to_parquet(MAP_V3, index=False)\n",
    "    sum_v3.to_parquet(SUM_V3, index=False)\n",
    "    canon_v3.to_parquet(CANON_V3P, index=False)\n",
    "    canon_v3.to_csv(CANON_V3C, index=False)\n",
    "\n",
    "    # ---------- Prints ----------\n",
    "    p(\"============================================================\")\n",
    "    p(\"üíæ SAVED\")\n",
    "    p(\"============================================================\")\n",
    "    p(f\"Token map (v3): {MAP_V3}\")\n",
    "    p(f\"Summary   (v3): {SUM_V3}\")\n",
    "    p(f\"Canon map (v3): {CANON_V3P}\")\n",
    "    p(f\"Canon csv (v3): {CANON_V3C}\")\n",
    "\n",
    "    # Before/after stats\n",
    "    n_before = sum_v2[\"cluster_id\"].nunique()\n",
    "    n_after  = sum_v3[\"cluster_id\"].nunique()\n",
    "    merged_groups = sum(1 for v in groups.values() if len(v) > 1)\n",
    "    total_merged_clusters = sum(len(v)-1 for v in groups.values() if len(v) > 1)\n",
    "    p(\"============================================================\")\n",
    "    p(\"üìà MERGE SUMMARY\")\n",
    "    p(\"============================================================\")\n",
    "    p(f\"Before communities : {n_before:,}\")\n",
    "    p(f\"After communities  : {n_after:,}\")\n",
    "    p(f\"Merging groups     : {merged_groups:,}\")\n",
    "    p(f\"Clusters collapsed : {total_merged_clusters:,}\")\n",
    "\n",
    "    # Show top 20 merged keys\n",
    "    merged_key_sizes = [(k, len(v)) for k,v in groups.items() if k and len(v) > 1]\n",
    "    merged_key_sizes.sort(key=lambda x: x[1], reverse=True)\n",
    "    p(\"\\nTop 20 merge keys (size, key -> sample labels):\")\n",
    "    for k, szk in merged_key_sizes[:20]:\n",
    "        labs = sum_v2.loc[sum_v2[\"merge_key\"] == k, \"new_label\"].head(5).tolist()\n",
    "        p(f\"- {szk:>3} | {k} -> {labs}\")\n",
    "\n",
    "    # Year-only examples\n",
    "    p(\"\\nExamples merged by year-blind (up to 30):\")\n",
    "    examples = []\n",
    "    for k, cids in groups.items():\n",
    "        if not k or len(cids) <= 1: continue\n",
    "        labs = sum_v2.loc[sum_v2[\"cluster_id\"].isin(cids), \"new_label\"].astype(str).tolist()\n",
    "        if any(YEAR_RE.search(l) for l in labs):\n",
    "            examples.append((k, labs[:6]))\n",
    "        if len(examples) >= 30: break\n",
    "    for k, labs in examples:\n",
    "        p(f\"* {k}  ::  {labs}\")\n",
    "\n",
    "    # 200 rewrites: token ‚Üí old_label // new_label\n",
    "    p(\"\\n============================================================\")\n",
    "    p(\"üîç SAMPLE REWRITES (200) ‚Äî token ‚Üí old_label  //  new_label\")\n",
    "    p(\"============================================================\")\n",
    "    old = pd.read_parquet(CANON_V2)[[\"token\",\"cluster_id\",\"canonical_label\"]].rename(columns={\"canonical_label\":\"old_label\"})\n",
    "    new = canon_v3[[\"token\",\"cluster_id\",\"canonical_label\"]].rename(columns={\"canonical_label\":\"new_label\"})\n",
    "    merged = old.merge(new, on=[\"token\",\"cluster_id\"], how=\"outer\", indicator=True).fillna({\"old_label\":\"\",\"new_label\":\"\"})\n",
    "    changed = merged[merged[\"old_label\"] != merged[\"new_label\"]]\n",
    "    samp = (changed if len(changed) else merged).sample(min(REWRITE_SAMPLE_N, len(merged)), random_state=42)\n",
    "    out = samp[[\"token\",\"old_label\",\"new_label\",\"cluster_id\"]].sort_values([\"cluster_id\",\"new_label\",\"token\"])\n",
    "    print(out.to_string(index=False, max_colwidth=80))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
